\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{mathtools}
\usepackage{mdframed}
\usepackage{cancel}
\usepackage{import, xifthen, pdfpages, transparent}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{float}
\usepackage{tikz, pgfplots}
\usetikzlibrary{positioning}
\pgfplotsset{compat=1.18}
\geometry{a4paper, margin=2cm}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    inputencoding=utf8,
    extendedchars=true,
    literate={à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {È}{{\`E}}1 {é}{{\'e}}1,
}
% Definizione colori stile MATLAB
\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{mBack}{rgb}{0.95,0.95,0.92} % Colore sfondo opzionale

% Stile per il CODICE
\lstdefinestyle{matlabCode}{
    language=Matlab,
    backgroundcolor=\color{white},   % Sfondo bianco
    commentstyle=\color{mGreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\ttfamily\footnotesize, % Font monospaziato
    breakatwhitespace=false,
    breaklines=true,                 % A capo automatico
    captionpos=b,
    keepspaces=true,
    numbers=left,                    % Numeri di riga a sinistra
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single                     % Cornice attorno al codice
}

% Stile per l'Output (senza colori, sfondo grigio chiaro)
\lstdefinestyle{matlabOutput}{
    language={},                     % Nessun linguaggio
    backgroundcolor=\color{gray!10}, % Sfondo grigio chiaro
    basicstyle=\ttfamily\footnotesize,
    frame=single,                    % Cornice
    breaklines=true,
    showstringspaces=false
}

\newmdenv[
  linecolor=black,
  linewidth=1pt,
  roundcorner=5pt,
  innertopmargin=4pt,
  innerbottommargin=10pt,
  innerleftmargin=10pt,
  innerrightmargin=10pt
]{bxthm}

\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{cor}{Corollario}

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{exmp}{Esempio}[section]
\newtheorem{xca}[exmp]{Esercizio}

\theoremstyle{remark}
\newtheorem{rem}{Osservazione}
\newtheorem{note}{Nota}
\newtheorem{case}{Caso}

\newcommand{\incfig}[2][\columnwidth]{%
    \def\svgwidth{#1}
    \import{./figures/}{#2.pdf_tex}
}

\begin{document}

\begin{titlepage}
    \centering
    {\scshape\LARGE Università degli Studi della Basilicata \par}
    \vspace{0.5cm}
    {\scshape\Large Dipartimento di Scienze di Base e Applicate - DISBA \par} 
    
    \vspace{2.5cm}

    % Linea orizzontale superiore
    \rule{\linewidth}{0.5mm}
    \vspace{0.4cm}
    
    {\huge\bfseries Calcolo Scientifico \par}
    \vspace{0.2cm}
    {\Large\itshape Appunti rielaborati delle lezioni ed esercizi svolti \par}
    
    \vspace{0.4cm}
    % Linea orizzontale inferiore
    \rule{\linewidth}{0.5mm}
    
    \vspace{3cm}

    % Blocco Studente e Docente
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
            \emph{Studente:}\\
            \textbf{Donato Martinelli}\\
            Matr. 69060 
        \end{flushleft}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \large
            \emph{Docente:}\\
            Prof.ssa \textbf{Maria Carmela De Bonis}
        \end{flushright}
    \end{minipage}

    \vfill

    {\large Anno Accademico 2025/2026 \par}

\end{titlepage}

\tableofcontents

\newpage 

\section{Introduzione}

\paragraph{Il Ruolo della Matematica e la Modellizzazione}
La matematica è uno strumento indispensabile per l'interpretazione e la predizione dei fenomeni reali, ponendosi alla base di tutte le scienze. 
Gli scienziati cercano di ricavare un modello matematico (variabili, parametri, relazioni) che sia rigoroso e coerente con il fenomeno.
La complessità del fenomeno determina la quantità di dati necessari e il numero di variabili del modello. Le proprietà incognite si deducono risolvendo tale modello.

\paragraph{Necessità dell'Approccio Numerico}
Spesso la soluzione non è disponibile in forma esplicita o non è calcolabile analiticamente, rendendo necessario un metodo numerico di approssimazione. 
Anche quando la risoluzione analitica è possibile, essa può risultare troppo onerosa computazionalmente all'aumentare delle dimensioni del problema.

\paragraph{Esempi Introduttivi}
Per chiarire la necessità dell'approssimazione, consideriamo alcuni esempi:
\begin{itemize}
    \item \textbf{Equazione di secondo grado:} Data l'equazione $ax^{2}+bx+c=0$, le soluzioni sono $x_{1,2}=\frac{-b\pm\sqrt{b^{2}-4ac}}{2a}$.
    Se $a=1, b=1, c=-1$, le soluzioni sono $\frac{-1\pm\sqrt{5}}{2}$. Per individuare il valore numerico è necessario approssimare $\sqrt{5}$.
    \item \textbf{Equazione esponenziale:} La soluzione di $2^{x}=100$ è $x=\log_{2}(100)$. Anche qui, la risposta numerica richiede un'approssimazione.
    \item \textbf{Equazione trascendente:} L'equazione $2^{x}+x^{2}-3=0$ non è risolvibile analiticamente (non esplicitabile rispetto a $x$). 
    Sebbene studiabile graficamente, il calcolo delle soluzioni richiede obbligatoriamente un metodo numerico.
\end{itemize}

\paragraph{Definizione di Metodo Numerico}
Un metodo numerico è un processo implementabile su un calcolatore che fornisce una soluzione numerica in un numero finito di passi. 
Esistono metodi matematicamente validi che però non sono implementabili in un calcolatore per via dei tempi di esecuzione.

\paragraph{Caso Studio: Sistemi Lineari (Cramer vs Gauss)}
Un esempio cruciale riguarda la risoluzione di sistemi lineari, problema centrale nella modellistica.
\begin{itemize}
    \item \textbf{Il Metodo di Cramer (Non implementabile):} Se volessimo risolvere un sistema $20 \times 20$ ($n=20$) con Cramer, dovremmo calcolare 21 determinanti di ordine 20. 
    Con la regola di Laplace, le operazioni sono circa $21 \cdot 20! \approx 5.2 \cdot 10^{19}$. Su un processore da 3GHz ($3 \cdot 10^{9}$ op/sec), il tempo richiesto sarebbe superiore a 5 secoli. 
    È una procedura con tempo di esecuzione proibitivo.
    \item \textbf{Il Metodo di Eliminazione di Gauss (Efficiente):} Questo metodo richiede circa $\frac{n^{3}}{3}$ operazioni.
    Per $n=20$, servono solo 2667 operazioni ($0.88 \cdot 10^{-6}$ secondi).
    Per $n=1000$, servono 334 milioni di operazioni, eseguibili in un decimo di secondo.
\end{itemize}

\paragraph{Criteri di Scelta ed Errori}
La scelta del metodo si basa su due criteri fondamentali:
\begin{enumerate}
    \item \textbf{Efficienza:} minor numero di operazioni richieste.
    \item \textbf{Efficacia:} maggiore precisione di calcolo.
\end{enumerate}
Nonostante la potenza dei calcolatori moderni, i matematici affrontano ancora difficoltà. Poiché i numeri sono rappresentati in binario su una memoria finita, ogni memorizzazione introduce un errore .

\paragraph{Propagazione dell'Errore}
Un tema centrale del Calcolo Scientifico è la consapevolezza dell'errore e della sua propagazione. Alcuni metodi propagano gli errori (introdotti nei dati iniziali) al punto da fornire soluzioni completamente sbagliate. 
Si studia quindi la stabilità degli algoritmi rispetto a questi errori.

\newpage

\section{Rappresentazione dei Numeri in un Calcolatore}
\paragraph{Sistemi di Numerazione}
Il numero è un concetto che esiste indipendentemente dall'insieme dei simboli o segni che si utilizzano per rappresentarlo.
Un sistema di numerazione non è altro che uno schema che permette di rappresentare i numeri "ideali" attraverso un insieme di simboli per i quali vengono definite delle tecniche di manipolazione (operazioni aritmetiche).
E' indubbio che tutti i popoli impararono a contare, spinti dalle necessità della vita quotidiana, per quantificare un insieme di elementi.
Gli uomini primitivi limitarono la propria conoscenza ai primi numeri naturali, per indicare i quali ricorsero a nomi di oggetti concreti, io per indicare 1, ali per indicare 2, mano per indicare 5.
In seguito incominciarono a rappresentare i numeri con delle barrette verticali.
Ogni intero era rappresentato con un numero di barrette pari alle unità in esso contenute.
Questo sistema benché sia il più semplice possibile, presenta lo svantaggio di assegnare a ciascun numero una rappresentazione la cui lunghezza è proporzionale al numero stesso.
Così, ben presto, si ebbe la necessità di rappresentare l'infinità dei numeri con un numero limitato di segni particolari, detti cifre e di fissare alcuni numeri fondamentali che facevano da riferimento.
Nacque così l'idea di quella che viene chiamata base di numerazione.
Quasi tutti i popoli scelsero come base di numerazione il 10. Scelta dovuta sicuramente al numero delle dita delle mani.
I sistemi di numerazioni delle civiltà conosciute, a partire dalle popolazioni primitive, si classificano in:
\begin{itemize}
    \item sistema di numerazione additivo
    \item sistema di numerazione posizionale
\end{itemize}
Un sistema di numerazione additivo è un sistema di numerazione basato su una legge additiva applicata a determinati simboli numerici fondamentali.
Ogni numero è rappresentato attraverso una successione di tali simboli ed il suo valore è dato dalla somma dei valori attribuiti a ciascuno di essi.
Nei sistemi additivi non serve un simbolo per lo zero.
Tra i più famosi sistemi additivi, oltre a quello delle barrette, figurano quello romano e quello egizio.
Solo verso il X secolo il sistema di numerazione additivo incominciò ad essere soppiantato dal sistema di numerazione posizionale introdotto dai matematici indiani ed arabi.
Un sistema di numerazione si dice posizionale se i simboli usati per scrivere i numeri assumono valori diversi a seconda della posizione che occupano nella notazione.

\paragraph{Sistema decimale}
Il sistema di numerazione da noi comunemente usato per rappresentare i numeri è quello decimale.
Esso è un sistema di numerazione posizionale basato sull'impiego di 10 cifre 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ed è stato inventato dagli Indiani.
Agli Indiani si deve l'introduzione dello zero che essi chiamavano sunya (nulla) anche se alcuni studiosi affermano che già i Cinesi conoscevano il metodo posizionale fin da tempi antichissimi.
Questo sistema di numerazione venne introdotto in occidente dagli Arabi probabilmente nel XII secolo quando venne tradotta \textit{Algoritmi de numero Indorum} del grande matematico arabo al-Khuwarizmi (IX secolo).
La base del sistema decimale è 10 e ogni numero viene rappresentato come
$$ a = \pm a_{m}a_{m-1}\dots a_{1}a_{0}.a_{-1}a_{-2}\dots a_{-M} $$
con
$$ 0 \le a_{i} \le 9 $$
Il sistema è posizionale perché il valore di ogni cifra varia in funzione della sua posizione nella rappresentazione decimale del numero
$$ z = \pm a_{m}10^{m} + a_{m-1}10^{m-1} + \dots + a_{1}10^{1} + a_{0}10^{0} + a_{-1}10^{-1} + a_{-2}10^{-2} + \dots $$
\begin{exmp}
\begin{itemize}
    \item $931.57 = 9 \times 10^{2} + 3 \times 10^{1} + 1 \times 10^{0} + 5 \times 10^{-1} + 7 \times 10^{-2}$
    \item $34.002 = 3 \times 10^{1} + 4 \times 10^{0} + 0 \times 10^{-1} + 0 \times 10^{-2} + 2 \times 10^{-3}$
    \item $12600.09 = 1 \times 10^{4} + 2 \times 10^{3} + 6 \times 10^{2} + 0 \times 10^{1} + 0 \times 10^{0} + 0 \times 10^{-1} + 9 \times 10^{-2}$
\end{itemize}
\end{exmp}
La rappresentazione decimale di ogni numero reale è unica, eccetto quando la parte frazionaria contiene una sequenza di 9 consecutivi.
\begin{exmp}
\begin{itemize}
    \item $0.319999...9... = 0.32$
    \item $10.99999...9... = 11$
    \item $199.99999...9... = 200$
\end{itemize}
\end{exmp}

\paragraph{Sistema in base N}
Qualunque intero $N>1$ può essere scelto come base ed ogni numero reale $a$ può essere scritto nella forma
$$ z = \pm a_{m}N^{m} + a_{m-1}N^{m-1} + \dots + a_{1}N^{1} + a_{0}N^{0} + a_{-1}N^{-1} + a_{-2}N^{-2} + \dots $$
ovvero
$$ a = \pm a_{m}a_{m-1}\dots a_{1}a_{0}.a_{-1}a_{-2}\dots a_{-M} $$
con $0 \le a_{i} \le N-1$.
La rappresentazione di ogni numero reale in base N è unica, eccetto quando la parte frazionaria contiene una sequenza di cifre $a_{k}=N-1$ consecutive.
Più piccola è la base scelta, più è lunga la stringa di caratteri necessari per rappresentare lo stesso numero.

\paragraph{Sistema binario}
La base del sistema binario è 2.
Le cifre utilizzate da questo sistema sono 0 e 1 e vengono dette bit da \textit{binary digit}.
Ogni numero reale $a$ è rappresentato come una sequenza di 0 e 1, ovvero
$$ a = \pm a_{m}2^{m} + a_{m-1}2^{m-1} + \dots + a_{1}2^{1} + a_{0}2^{0} + a_{-1}2^{-1} + a_{-2}2^{-2} + \dots $$
con $0 \le a_{i} \le 1$.

Questo sistema è particolarmente interessante perché può essere realizzato con qualsiasi oggetto capace di assumere due stati diversi, uno per la cifra 0 e l'altro per la cifra 1.

\begin{exmp}
\begin{itemize}
    \item Stato di magnetizzazione e non di un nucleo di ferrite;
    \item conduttività e non di un diodo.
\end{itemize}    
\end{exmp}

Per queste sue caratteristiche è stato adottato per la rappresentazione dei dati e, in particolare, dei numeri in un calcolatore.

\paragraph{Rappresentazione dei numeri in un calcolatore}
I numeri vengono rappresentati nel calcolatore secondo il sistema binario e, quindi, come una sequenza di bit.
Per ovvie ragioni, ad ogni numero reale viene riservato uno spazio finito di memoria, capace di contenere un numero finito di bit detto \textbf{parola}.
Di conseguenza, non tutti i numeri reali sono rappresentabili in modo esatto.
Detta $l$ la lunghezza della parola, si possono rappresentare esattamente solo quei numeri la cui rappresentazione binaria consta di un numero di bit inferiore o uguale ad $l$.
Si parla di \textbf{numeri macchina}.
Tutte le operazioni fra i numeri macchina vengono effettuate utilizzando l'aritmetica binaria.

\paragraph{Numeri interi}
Con parole di lunghezza $l$ è possibile rappresentare tutti i numeri interi appartenenti all'intervallo
$$ \left[-\frac{2^{l}}{2}, \frac{2^{l}}{2}-1\right] $$
\textbf{Esempio}: Se $l=16$ sono rappresentabili tutti i numeri interi appartenenti all'intervallo
$$ [-32768, 32767] $$

\paragraph{Numeri reali}
Ogni numero reale $a$ può essere scritto nella forma
$$ a = pN^{q} $$
dove $p$ è un numero reale, $N$ è la base del sistema di numerazione e $q$ è un numero intero positivo o negativo.
Questa rappresentazione, detta in virgola mobile (floating-point), non è unica, infatti
$$ 321.25 = 32.125 \times 10^{1} = 0.32125 \times 10^{3} $$
La rappresentazione di $a$ si dice \textbf{normalizzata} quando
$$ N^{-1} \le |p| < 1 $$
Le cifre di $p$ si dicono \textbf{cifre significative}.

\begin{exmp}
\begin{itemize}
    \item La rappresentazione normalizzata di $a=92.25$ è $a=0.9225 \times 10^{2}$;
    \item la rappresentazione normalizzata di $a=0.000718$ è $a=0.718000 \times 10^{-3}$;
    \item la rappresentazione normalizzata di $a=4152.0002156$ è $a=0.41520002156 \times 10^{4}$;
    \item la rappresentazione normalizzata di $a=0.0215600000$ è $a=0.2156 \times 10^{-1}$.
\end{itemize}    
\end{exmp}

Fissata la base $N$, ogni numero reale $a$ è univocamente definito dalla coppia
$$ a = (p, q) $$
$p$ viene detta \textbf{mantissa} di $a$, $q$ viene detto \textbf{esponente} di $a$.
I numeri reali in virgola mobile vengono rappresentati in un calcolatore in forma normalizzata secondo lo Standard IEEE 754 (Institute of Electrical and Electronical Engineering).

\paragraph{Standard IEEE 754: Singola Precisione}
\begin{bxthm}
    \begin{defn}
        La struttura è definita dai seguenti parametri:
        \[ s = 1 \text{ bit, bit 31} \]
        \[ p = 23 \text{ bit} \]
        \[ q = 8 \text{ bit} \]
        
        È possibile rappresentare tutti i numeri reali della seguente forma:
        \[ \pm 0.d_{1}d_{2}d_{3}d_{4}d_{5}d_{6} \times 10^{q} \]
        dove:
        \[ 0 < d_{1} \le 9, \quad 0 \le d_{i} \le 9, \quad i\in\{2,\dots,6\} \]
        e
        \[ -38 \le q \le 38 \]
        Si hanno \textbf{6 cifre significative}.
    \end{defn}
\end{bxthm}

\paragraph{Standard IEEE 754: Doppia Precisione}
\begin{bxthm}
    \begin{defn}
        La struttura è definita dai seguenti parametri:
        \[ s = 1 \text{ bit (bit 63)} \]
        \[ q = 11 \text{ bit (bit 62-52)} \]
        \[ p = 52 \text{ bit (bit 51-0)} \]
        
        È possibile rappresentare tutti i numeri reali della seguente forma:
        \[ \pm 0.d_{1}d_{2}\dots d_{16} \times 10^{q} \]
        dove:
        \[ 0 < d_{1} \le 9, \quad 0 \le d_{i} \le 9, \quad i\in\{2,\dots,16\} \]
        e
        \[ -308 \le q \le 308 \]
        Si hanno \textbf{16 cifre significative}.
    \end{defn}
\end{bxthm}

\begin{exmp}
    Consideriamo due numeri reali da convertire nei rispettivi standard IEEE 754.
    \paragraph{1. Esempio in Singola Precisione ($32$ bit)}
    Rappresentiamo il numero $v = -13.625$.
    \begin{itemize}
        \item \textbf{Segno:} Il numero è negativo, quindi $s = 1$.
        \item \textbf{Conversione Binaria:} 
        \[ 13_{10} = 1101_{2} \]
        \[ 0.625_{10} = 0.101_{2} \quad (0.5 + 0.125) \]
        Unendo le parti: $1101.101_{2}$.
        \item \textbf{Normalizzazione:} Spostiamo la virgola a sinistra fino ad avere $1.m$:
        \[ 1.101101 \times 2^{3} \]
        L'esponente reale è $3$.
        \item \textbf{Esponente Biased ($q=8$):} Si aggiunge il bias $127$ ($2^{q-1}-1$):
        \[ 3 + 127 = 130 \implies 10000010_{2} \]
        \item \textbf{Mantissa ($p=23$):} Si prendono i bit dopo la virgola (il bit intero 1 è implicito):
        \[ 10110100000000000000000 \]
    \end{itemize}
    \textbf{Rappresentazione finale:}
    \[ \underbrace{1}_{s} \;\; \underbrace{10000010}_{q \text{ (esp)}} \;\; \underbrace{10110100000000000000000}_{p \text{ (mantissa)}} \]

    \paragraph{2. Esempio in Doppia Precisione ($64$ bit)}
    Rappresentiamo il numero $v = 0.15625$.
    \begin{itemize}
        \item \textbf{Segno:} Il numero è positivo, quindi $s = 0$.
        \item \textbf{Conversione Binaria:} 
        \[ 0.15625_{10} = \frac{5}{32} = \frac{1}{8} + \frac{1}{32} = 2^{-3} + 2^{-5} = 0.00101_{2} \]
        \item \textbf{Normalizzazione:} Spostiamo la virgola a destra:
        \[ 1.01 \times 2^{-3} \]
        L'esponente reale è $-3$.
        \item \textbf{Esponente Biased ($q=11$):} Si aggiunge il bias $1023$ ($2^{q-1}-1$):
        \[ -3 + 1023 = 1020 \implies 01111111100_{2} \]
        \item \textbf{Mantissa ($p=52$):} Si prendono i bit dopo la virgola, riempiendo con zeri:
        \[ 01000000\dots0 \quad (\text{totale 52 bit}) \]
    \end{itemize}
    \textbf{Rappresentazione finale:}
    \[ \underbrace{0}_{s} \;\; \underbrace{01111111100}_{q \text{ (esp)}} \;\; \underbrace{01000000000000000000\dots0}_{p \text{ (mantissa)}} \]
\end{exmp}


\paragraph{Osservazione}
Nello standard IEEE 754 in Doppia Precisione, benché lo spazio riservato alla mantissa di un numero reale sia di 52 bit, viene recuperato un bit in più non rappresentando il primo bit che è sempre uguale a 1.
Pertanto le mantisse $p$ dei numeri reali vengono rappresentate in doppia precisione con 53 bit.

Supponendo di utilizzare un calcolatore che lavora in doppia precisione, dato
$$ a = \pm 0.d_{1}d_{2}\dots 10^{q}, \quad d_{1} \ne 0 $$
un numero reale non nullo, si possono presentare i seguenti casi:
\begin{enumerate}
    \item L'esponente $q$ è tale che $-308 \le q \le 308$ e le cifre dopo la 16-esima sono tutte nulle ovvero $d_{i}=0$ per ogni $i>16$, cioè
    $$ a = \pm 0.d_{1}d_{2}\dots d_{16} 10^{q} $$
    Allora $a$ è \textbf{esattamente rappresentabile}.
    \item L'esponente $q$ non appartiene all'intervallo $[-308, 308]$.
    Se $q < -308$, si associa 0 ad $a$ e il calcolatore segnala \textbf{underflow}.
    Se $q > 308$, $a$ non viene rappresentato e il calcolatore segnala \textbf{overflow}.
\end{enumerate}

\paragraph{Osservazione}
Nei computer di ultima generazione è possibile rappresentare anche numeri il cui esponente $q$ è tale che
$$ -324 < q < -308 $$
Tali numeri riempiono l'intervallo tra lo zero ed il più piccolo numero normalizzato rappresentabile
$$ \text{realmin} = 2^{-1022} \sim 2.22 \times 10^{-308} $$
e vengono chiamati \textbf{denormalizzati} (o subnormalizzati).
Il più grande numero normalizzato rappresentabile in doppia precisione è
$$ \text{realmax} \sim 1.79 \times 10^{308} $$
Esso è un bit meno di $2^{1024}$.

\begin{enumerate}
    \setcounter{enumi}{2}
    \item L'esponente $q$ appartiene all'intervallo $[-308, 308]$ ma le cifre $d_{i}$, $i>16$, non sono tutte nulle.
    In questo caso è possibile associare al numero $a$ un numero di macchina $fl(a)$ seguendo due criteri diversi:
    \begin{itemize}
        \item troncamento
        \item arrotondamento
    \end{itemize}
\end{enumerate}

\paragraph{Regola di troncamento}
Per troncare un numero
$$ a = 0.d_{1}d_{2}\dots d_{t-1}d_{t}d_{t+1}d_{t+2}\dots N^{q} $$
fino a $t$ cifre significative, si eliminano tutte le cifre $d_{t+1}d_{t+2}\dots$ a destra della t-esima.
Dunque
$$ fl(a) = trn(a) = 0.d_{1}d_{2}\dots d_{t-1}d_{t} N^{q} $$

\paragraph{Regola di arrotondamento}
Per arrotondare un numero
$$ a = 0.d_{1}d_{2}\dots d_{t-1}d_{t}d_{t+1}d_{t+2}\dots N^{q} $$
fino a $t$ cifre significative, si eliminano tutte le cifre $d_{t+1}d_{t+2}\dots$ a destra della t-esima e si rimpiazza $d_{t}$ con la cifra $c$, dove
$$
\begin{cases}
c = d_{t} & \text{se } 0 \le d_{t+1} \le 4 \\
c = d_{t} + 1 & \text{se } 5 \le d_{t+1} \le 9
\end{cases}
$$
Dunque
$$ fl(a) = arr(a) = 0.d_{1}d_{2}\dots d_{t-1}c N^{q} $$

\begin{exmp}
\begin{itemize}
    \item Consideriamo il numero $\pi = 3.1415926536\dots = 0.31415926536\dots 10^{1}$.
    \begin{itemize}
        \item $t=6$: $trn(\pi)=0.314159 \cdot 10^{1}$, $arr(\pi)=0.314159 \cdot 10^{1}$
        \item $t=5$: $trn(\pi)=0.31415 \cdot 10^{1}$, $arr(\pi)=0.31416 \cdot 10^{1}$
        \item $t=4$: $trn(\pi)=0.3141 \cdot 10^{1}$, $arr(\pi)=0.3142 \cdot 10^{1}$
    \end{itemize}
    \item Consideriamo il numero $a = 2\sqrt{2} = 2.828427124746190\dots = 0.2828427124746190\dots 10^{1}$.
    \begin{itemize}
        \item $t=6$: $trn(a)=0.282842 \cdot 10^{1}$, $arr(a)=0.282843 \cdot 10^{1}$
        \item $t=5$: $trn(a)=0.28284 \cdot 10^{1}$, $arr(a)=0.28284 \cdot 10^{1}$
        \item $t=4$: $trn(a)=0.2828 \cdot 10^{1}$, $arr(a)=0.2828 \cdot 10^{1}$
    \end{itemize}
\end{itemize}
\end{exmp}

\paragraph{Vantaggi della rappresentazione normalizzata}
Consideriamo il numero
$$ a = \frac{1}{7000} $$
Poniamo
$$ \overline{a} = 0.000142857142857\dots \quad \text{(rappresentazione non normalizzata)} $$
$$ \overline{\overline{a}} = 0.142857142857142\dots 10^{-3} \quad \text{(rappresentazione normalizzata)} $$
Applichiamo le regole di troncamento e arrotondamento ad $\overline{a}$:
\begin{itemize}
    \item $t=6$: $trn(\overline{a})=0.000142$, $arr(\overline{a})=0.000143$
    \item $t=3$: $trn(\overline{a})=0$, $arr(\overline{a})=0$
\end{itemize}
Applichiamo le regole di troncamento e arrotondamento ad $\overline{\overline{a}}$:
\begin{itemize}
    \item $t=6$: $trn(\overline{\overline{a}})=0.142857 \cdot 10^{-3}$, $arr(\overline{\overline{a}})=0.142857 \cdot 10^{-3}$
    \item $t=3$: $trn(\overline{\overline{a}})=0.142 \cdot 10^{-3}$, $arr(\overline{\overline{a}})=0.143 \cdot 10^{-3}$
\end{itemize}

Dunque, la normalizzazione permette di ridurre l'errore di rappresentazione dovuto al troncamento o all'arrotondamento ad un numero finito di cifre.
Inoltre, l'informazione contenuta negli zeri dopo il punto può essere memorizzata, con minor spreco di memoria, in termini di esponente.

\paragraph{Conseguenze della rappresentazione dei numeri come parole di lunghezza fissa}

\paragraph{Prima conseguenza: Limitatezza dell'insieme dei numeri rappresentabili}
L'insieme $\mathbb{R}$ dei numeri reali è infinito, ma, soltanto un sottoinsieme di $\mathbb{R}$, limitato inferiormente e superiormente, è rappresentabile in un calcolatore.
In seguito denoteremo con $\mathbb{F}$ il sottoinsieme dei numeri reali rappresentabili in un calcolatore.
$$ \mathbb{F} = \{0\} \cup \{ \pm 0.d_{1}d_{2}\dots d_{16} \times 10^{q} \in \mathbb{R} : -308 \le q \le 308 \} $$
$$ 0 < d_{1} \le 9, \quad 0 \le d_{i} \le 9, \quad i\in\{2,\dots,16\} $$
Si ha l'intervallo contenente valori rappresentabili tra -realmax e -realmin, lo zero, e tra realmin e realmax.
Al di fuori di questo intervallo si ha overflow o underflow.

\paragraph{Seconda conseguenza: Insieme dei numeri rappresentabili "bucato"}
L'insieme $\mathbb{R}$ dei numeri reali è denso, cioè tra due numeri reali $r_{1}$ e $r_{2}$ esiste sempre un numero reale $r_{3}$ tale che
$$ r_{1} < r_{3} < r_{2} $$
Invece, il sottoinsieme $\mathbb{F}$, oltre ad essere limitato inferiormente e superiormente, è anche bucato.

\textbf{Esempio}
Utilizzando la rappresentazione in doppia precisione, i numeri macchina
$$ a = 1.123456789123456 $$
e
$$ b = 1.123456789123457 $$
sono consecutivi e tra di essi non esiste alcun altro numero macchina.
Tutti i numeri reali compresi tra $a$ e $b$ vengono approssimati con $a$ o con $b$.

\paragraph{Il Software MatLab}
MatLab (Matrix Laboratory) è un ambiente di calcolo sviluppato a partire dagli anni 70.
La struttura di base è la matrice, per la quale sono già predefinite numerosi tipi elementari (matrice identità, matrice nulla, matrice unità...), funzioni algebriche e di manipolazione (somma, prodotto, calcolo del determinante).
Per lanciare MatLab da ambiente Windows basta cliccare con il mouse sull'icona corrispondente.
Per entrare in confidenza con l'ambiente di lavoro è utile:
\begin{itemize}
    \item lanciare il comando \texttt{demo} che illustra le potenzialità del software attraverso significativi esempi numerici e casi test;
    \item fare costante riferimento all'uso dell'\texttt{help}, ad esempio \texttt{help sqrt} (calcolo della radice quadrata di un numero).
\end{itemize}

\paragraph{Prime istruzioni in MatLab}
Il modo più immediato per interagire con MatLab è scrivere l'istruzione dal prompt \texttt{>>} seguita da Invio.
Esempio: assegnazione del valore 3 alla variabile $a$:
\begin{verbatim}
>> a = 3
a =
     3
\end{verbatim}
Possiamo usare MatLab come una semplice calcolatrice:
\begin{verbatim}
>> b = a * 2
b =
     6
\end{verbatim}
o, come vedremo, come un vero e proprio ambiente di programmazione.
Sia le istruzioni che esegue dal prompt che le routine devono essere scritti utilizzando una speciale sintassi.

Invece di digitare tutti i comandi al prompt, possiamo memorizzare una serie di istruzioni successive (script) sotto formato di file di testo, detto M-file e caratterizzato da estensione .m.
A questo scopo possiamo utilizzare l'Editor di testo integrato.
Per uscire da MatLab: comandi \texttt{quit} o \texttt{exit}.

\paragraph{Alcuni trucchi per risparmiare tempo}
\begin{enumerate}
    \item Durante la sessione di lavoro è possibile richiamare i comandi precedentemente digitati utilizzando i tasti $\leftarrow, \rightarrow, \uparrow, \downarrow$;
    \item immettendo i primi caratteri di un'istruzione già digitata e poi premendo il tasto $\uparrow$, viene completata la riga con l'ultima istruzione che inizia con quegli stessi caratteri;
    \item i tasti $\leftarrow$ e $\rightarrow$ permettono di riposizionare sulla linea di comando il cursore e di modificare il testo scritto;
    \item con il tasto sinistro del mouse sulla finestra di calcolo si possono selezionare parti di testo che è poi possibile copiare, tagliare ed incollare sulla linea di comando.
\end{enumerate}

\paragraph{Le variabili in MatLab}
In MatLab tutte le variabili sono in doppia precisione, ovvero sono rappresentate internamente con 64 bit, cui corrispondono 16 cifre significative.
Quando assegnamo un valore ad una variabile, MatLab risponde con un'eco:
\begin{verbatim}
s = 10
s =
    10
\end{verbatim}
Per sopprimere l'eco, usiamo la sintassi:
\begin{verbatim}
s = 10;
\end{verbatim}
Quando non assegnamo il valore di un'operazione ad una variabile, MatLab assegna tale valore alla variabile \texttt{ans} (che viene così ogni volta sovrascritta):
\begin{verbatim}
>> 3^2
ans =
     9
\end{verbatim}

\paragraph{Un'osservazione sulla precisione di calcolo}
Tutti i calcoli vengono effettuati in doppia precisione, mentre diversa è la visualizzazione delle variabili che viene determinata con il comando \texttt{format}:
\begin{itemize}
    \item \texttt{format short}: virgola fissa con 5 cifre (è il formato di default):
\begin{verbatim}
>> pi
ans =
    3.1416
\end{verbatim}
    \item \texttt{format long}: virgola fissa con 16 cifre:
\begin{verbatim}
>> pi
ans =
    3.141592653589793
\end{verbatim}
    \item \texttt{format short e}: virgola mobile con 5 cifre:
\begin{verbatim}
>> pi
ans =
    3.1416e+00
\end{verbatim}
    \item \texttt{format long e}: virgola mobile con 16 cifre:
\begin{verbatim}
>> pi
ans =
    3.141592653589793e+00
\end{verbatim}
\end{itemize}

\textbf{format long e} \\
Noi useremo sempre il formato \texttt{format long e} perché è quello più vicino alla rappresentazione normalizzata del numero.
Dunque, prima di incominciare qualunque esercizio digitiamo dal prompt:
\begin{verbatim}
>> format long e
\end{verbatim}

\paragraph{Esercizi in MatLab}
\begin{itemize}
    \item Digitare il numero 0.95842567894152678954126354 e verificare che il programma lo arrotonda a 16 cifre significative fornendo
    $$ 9.584256789415268e-001 = 0.9584256789415268 $$
\begin{verbatim}
>> 0.95842567894152678954126354
ans =
     9.584256789415268e-001
\end{verbatim}

    \item Digitare il numero 0.00000052135658365124858985 e verificare che il programma lo arrotonda a 16 cifre significative fornendo
    $$ 5.21356583651249e-007 = 0.521356583651249e-006 $$
\begin{verbatim}
>> 0.00000052135658365124858985
ans =
     5.213565836512486e-007
\end{verbatim}

    \item Digitare il numero 12356.00056256984154646 e verificare che il programma lo arrotonda a 16 cifre significative fornendo
    $$ 1.23560005625698e+004 = 0.123560005625698e+005 $$
\begin{verbatim}
>> 12356.00056256984154646
ans =
     1.235600056256984e+004
\end{verbatim}

    \item Calcolare \texttt{realmin} $= 2^{-1022} \sim 2.225073858507201e-308$
\begin{verbatim}
>> 2^(-1022)
ans =
     2.225073858507201e-308
\end{verbatim}

    \item Calcolare i valori $2^{-k}$ con $k = 1040, 1050, 1060, 1070$ e osservare che il programma riesce a rappresentare anche numeri con esponente compreso tra -308 e -324 (numeri denormalizzati).
\begin{verbatim}
>> k = 1040;
>> 2^(-k)
ans =
     8.487983163861089e-314
>> k = 1050;
>> 2^(-k)
ans =
     8.289046058458095e-317
>> k = 1060;
>> 2^(-k)
ans =
     8.094771541462983e-320
>> k = 1070;
>> 2^(-k)
ans =
     7.90505033459945e-323
\end{verbatim}

    \item Verificare che $2^{-1074} \sim 4.94065645841247e-324$ e che calcolando $2^{-1075}$ il programma restituisce come valore 0, dunque non riesce a rappresentare numeri con esponente più piccolo di -324.
\begin{verbatim}
>> k = 1074;
>> 2^(-k)
ans =
     4.940656458412465e-324
>> k = 1075;
>> 2^(-k)
ans =
     0
\end{verbatim}

    \item Verificare che $2^{1023} \sim 8.988465674311580e+307$ e che calcolando $2^{1024}$ il programma restituisce Inf, dove Inf denota l'overflow.
\begin{verbatim}
>> k = 1023;
>> 2^k
ans =
     8.988465674311580e+307
>> k = 1024;
>> 2^k
ans =
     Inf
\end{verbatim}
\end{itemize}

\vspace{20pt}

\subsection{Errori}

\vspace{10pt}

\paragraph{Cause principali di errori nella risoluzione di problemi matematici con un calcolatore}
Nella risoluzione di un problema matematico con un calcolatore le sorgenti di errore possono essere, in generale, suddivise in due gruppi.
\begin{enumerate}
    \item \textbf{Errori di rappresentazione dei dati o di arrotondamento} \\
    Abbiamo visto che non tutti i numeri reali possono essere rappresentati in un calcolatore. Se il calcolatore utilizza una rappresentazione con $t$ cifre per la mantissa, tutti i numeri reali compresi tra l'estremo inferiore e l'estremo superiore di $\mathbb{F}$ la cui mantissa ha un numero di cifre superiore a $t$ dovranno essere arrotondati o troncati a $t$ cifre.
    
    \item \textbf{Errori nei calcoli} \\
    Effettuando calcoli tra numeri macchina approssimati, gli errori di rappresentazione dei dati iniziali si propagano in qualche misura sui risultati di tali calcoli.
    L'entità della propagazione dipende anche dall'algoritmo utilizzato.
\end{enumerate}

\paragraph{Errori di rappresentazione dei dati}
Nel rappresentare un numero reale $a \ne 0$, con un corrispondente numero macchina $fl(a)$ occorre valutare l'errore commesso che sarà ovviamente nullo se $a$ è esso stesso un numero di macchina, cioè $a = fl(a)$.
A tale scopo, dato un valore reale esatto $a$ e un sua approssimazione $A$ diamo alcune definizioni.

\vspace{10pt}

\paragraph{Errore Assoluto}
\begin{bxthm}
\begin{defn}
La quantità
$$ \Delta a = |a - A| $$
si chiama errore assoluto.    
\end{defn}
\begin{lstlisting}[style=matlabCode, title={Function errAbs}]
function e = errAbs(valExact, valApprox)
% ERRABS Calcola l'errore assoluto (norma Euclidea della differenza).
%   E = ERRABS(VALEXACT, VALAPPROX) restituisce ||valExact - valApprox||_2.
%   Supporta scalari, vettori e matrici multidimensionali.
    
    %% 1. Validazione degli Input
    arguments
        valExact  double {mustBeNumeric}
        valApprox double {mustBeNumeric}
    end

    %% 2. Controllo Dimensioni 
    % isequal è il metodo standard per confrontare le dimensioni di array/matrici
    if ~isequal(size(valExact), size(valApprox))
        error("errAbs:DimensionMismatch", "Dimensioni incompatibili (%d e %d).", size(valExact), size(valApprox));
    end

    %% 3. Calcolo Errore Assoluto
    e = norm(valExact - valApprox);
end
\end{lstlisting}
\end{bxthm}

\vspace{10pt}

\paragraph{Errore Relativo}
L'errore assoluto non riesce a caratterizzare ciò che intuitivamente si può ritenere una misura della precisione del valore approssimato.
Per misurare la bontà di un'approssimazione $A$ di $a$ occorre confrontare l'errore assoluto con l'ordine di grandezza $a$ del dato da approssimare.
Cioè fare il rapporto tra l'errore assoluto e il valore assoluto del dato esatto.
\begin{bxthm}
\begin{defn}
La quantità
$$ \delta a = \frac{|a - A|}{|a|} $$
si chiama errore relativo.    
\end{defn}
\begin{lstlisting}[style=matlabCode, title={Function errRel}]
function e = errRel(valExact, valApprox, tol)
% ERRREL Calcola l'errore relativo in norma 2.
%   E = ERRREL(VALEXACT, VALAPPROX, TOL) calcola ||valExact- valApprox|| / ||valExact||.
%
%   NOTA: Se ||valExact|| < tol, la funzione restituisce l'errore assoluto
%   per evitare divisioni per zero (NaN/Inf).

    %% 1. Validazione degli Input
    arguments
        valExact  double {mustBeNumeric}
        valApprox double {mustBeNumeric}
        tol (1,1) double {mustBeNumeric} = 1e-10
    end

    %% 2. Calcolo Numeratore (Errore Assoluto)
    num = errAbs(valExact, valApprox);
    
    %% 3. Calcolo Denominatore e Gestione Singolarità
    den = norm(valExact);
    
    if den < tol
        % Caso critico: La soluzione esatta è zero. 
        % L'errore relativo non è definito. Restituiamo l'assoluto.
        warning("errRel:ZeroNorm", "La norma della soluzione esatta è minore della tolleranza. Restituisco Errore Assoluto.");
        e = num;
    else
        e = num / den;
    end
end
\end{lstlisting}
\end{bxthm}

\vspace{10pt}

\paragraph{Cifre corrette}
Siano
$$ a = pN^{q} \quad \text{e} \quad A = \overline{p}N^{q} $$
con
$$ N^{-1} \le |p|, |\overline{p}| < 1 $$

\vspace{10pt}

\paragraph{Cifre decimali corrette}
\begin{bxthm}
\begin{defn}
Se
$$ \Delta a \le \frac{1}{2}N^{-t}, \quad t \ge 1, $$
si dice che $A$ ha almeno $t$ cifre decimali corrette nella base $N$.    
\end{defn}
\end{bxthm}
Partendo dalla disuguaglianza che lega l'errore assoluto $\Delta a$ al numero di cifre decimali $t$:
\begin{align*}
    \Delta a &\le \frac{1}{2} \cdot 10^{-t} \\[1ex]
    2 \cdot \Delta a &\le 10^{-t} \\[1ex]
    \log_{10}(2 \cdot \Delta a) &\le - t \\[1ex]
    t &\le - \log_{10}(2 \cdot \Delta a)
\end{align*}
Poiché $t$ deve essere un intero, si assume il massimo valore che soddisfa la condizione (parte intera inferiore):
\[t = \left\lfloor - \log_{10}(2 \cdot \Delta a) \right\rfloor.\]

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function cifDecCorr}]
function n = cifDecCorr(valExact, valApprox)
% CIFDECCORR Calcola il numero di cifre decimali corrette.
%  n = CIFDECCORR(valExact, valApprox) calcola il numero di cifre 
%  decimali corrette basandosi sull'errore assoluto.
%
%  Input:
%    valExact  - Valore esatto (numerico: scalare, vettore o matrice)
%    valApprox - Valore approssimato (numerico: stesse dimensioni di valExact)
%
%  Output:
%    n - Numero intero di cifre decimali corrette.
%      Restituisce Inf se valExact e valApprox sono identici (errAbs 0).

    %% 1. Validazione degli Input
    arguments
        valExact  {mustBeNumeric}
        valApprox {mustBeNumeric}
    end

    %% 2. Controllo Dimensioni
    if ~isequal(size(valExact), size(valApprox))
        error('cifDecCorr:SizeMismatch', 'Gli input devono avere la stessa dimensione.');
    end

    %% 3. Calcolo cifre decimali corrette
    err = errAbs(valExact, valApprox); % Calcolo l'errore assoluto
    n = floor(-log10(2 * err));
end
\end{lstlisting}

\vspace{10pt}

\paragraph{Cifre significative corrette}
\begin{bxthm}
\begin{defn}
Se
$$ \delta a \le \frac{1}{2}N^{-t+1}, \quad t \ge 1, $$
si dice che $A$ ha almeno $t$ cifre significative corrette nella base $N$.    
\end{defn}
\end{bxthm}
Partendo dalla disuguaglianza che lega l'errore relativo $\delta a$ al numero di cifre significative $t$:
\begin{align*}
    \delta a &\le \frac{1}{2} \cdot 10^{1-t} \\[1ex]
    2 \cdot \delta a &\le 10^{1-t} \\[1ex]
    \log_{10}(2 \cdot \delta a) &\le 1 - t \\[1ex]
    t &\le 1 - \log_{10}(2 \cdot \delta a)
\end{align*}
Poiché $t$ deve essere un intero, si assume il massimo valore che soddisfa la condizione (parte intera inferiore):
\[t = \left\lfloor 1 - \log_{10}(2 \cdot \delta a) \right\rfloor.\]

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function cifSigCorr}]
function n = cifSigCorr(valExact, valApprox)
% CIFSIGCORR Calcola il numero di cifre significative corrette.
%  n = CIFSIGCORR(valExact, valApprox) calcola il numero di cifre 
%  significative corrette basandosi sull'errore relativo.
%
%  Input:
%    valExact  - Valore esatto (numerico: scalare, vettore o matrice)
%    valApprox - Valore approssimato (numerico: stesse dimensioni di valExact)
%
%  Output:
%    n - Numero intero di cifre significative corrette.

    %% 1. Validazione degli Input
    arguments
        valExact  {mustBeNumeric}
        valApprox {mustBeNumeric}
    end

    %% 2. Controllo Dimensioni
    if ~isequal(size(valExact), size(valApprox))
        error('cifSigCorr:SizeMismatch', 'Gli input devono avere la stessa dimensione.');
    end

    %% 3. Calcolo cifre significative corrette
    err = errRel(valExact, valApprox); % Calcolo l'errore relativo
    n = floor(1-log10(2 * err));
end
\end{lstlisting}

\vspace{10pt}

\paragraph{Osservazione}
Le cifre significative del numero $a = pN^{q}$ (scritto nella forma normalizzata) sono le cifre decimali corrette della sua mantissa $p$.
Infatti, supponendo che la mantissa $\overline{p}$ di $A$ approssimi la mantissa $p$ di $a$ con $t$ cifre decimali corrette, tenendo conto che $N^{-1} \le |p|, |\overline{p}| < 1$, si ha:
$$ \delta a = \frac{|p - \overline{p}|N^{q}}{|p|N^{q}} = \frac{|p - \overline{p}|}{|p|} \le \frac{\frac{1}{2}N^{-t}}{N^{-1}} = \frac{1}{2}N^{-t+1} $$

\vspace{10pt}

\begin{exmp}
\begin{enumerate}
    \item $a = 15.2000$, $A = 15.1997$, $N = 10$
    $$ \Delta a = 0.3\dots 10^{-3} < \frac{1}{2}10^{-3} $$
    $$ \delta a = 0.197\dots 10^{-4} < \frac{1}{2}10^{-4} $$
    Dunque $A$ ha almeno 3 cifre decimali corrette e almeno 5 cifre significative corrette. In realtà ha 3 cifre decimali corrette e 5 cifre significative corrette.
\begin{lstlisting}[style=matlabCode]
a = 15.2000;   % Valore Esatto
A = 15.1997;   % Valore Approssimato

d = cifDecCorr(a, A);
s = cifSigCorr(a, A);

fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre decimali corrette.\n', A, a, d);
fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre significative corrette.\n', A, a, s);
\end{lstlisting}
\begin{verbatim}
Il valore 15.1997, approssimazione di 15.2, ha almeno 3 cifre decimali corrette.
Il valore 15.1997, approssimazione di 15.2, ha almeno 5 cifre significative corrette.
\end{verbatim}

    \item $a = 199.2000$, $A = 199.1997$, $N = 10$
    $$ \Delta a = 0.3\dots 10^{-3} < \frac{1}{2}10^{-3} $$
    $$ \delta a = 0.15\dots 10^{-5} < \frac{1}{2}10^{-5} $$
    Dunque $A$ ha almeno 3 cifre decimali corrette e almeno 6 cifre significative corrette. In realtà ha 3 cifre decimali corrette e 6 cifre significative corrette.
    \textit{Nota:} Gli errori assoluti degli esempi 1 e 2 sono uguali!
\begin{lstlisting}[style=matlabCode]
a = 199.2000;   % Valore Esatto
A = 199.1997;   % Valore Approssimato

d = cifDecCorr(a, A);
s = cifSigCorr(a, A);

fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre decimali corrette.\n', A, a, d);
fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre significative corrette.\n', A, a, s);
\end{lstlisting}
\begin{verbatim}
Il valore 199.2, approssimazione di 199.2, ha almeno 3 cifre decimali corrette.
Il valore 199.2, approssimazione di 199.2, ha almeno 6 cifre significative corrette.
\end{verbatim}

    \item $a = 1$, $A = 0.9999$, $N = 10$
    $$ \Delta a = 10^{-4} = 0.1 \cdot 10^{-3} < \frac{1}{2}10^{-3} $$
    $$ \delta a < \frac{1}{2}10^{-3} $$
    Dunque $A$ ha almeno 3 cifre decimali corrette e almeno 4 cifre significative corrette. In realtà ha 4 cifre decimali corrette e 4 cifre significative corrette ma nessuna cifra coincidente!
\begin{lstlisting}[style=matlabCode]
a = 1;   % Valore Esatto
A = 0.9999;   % Valore Approssimato

d = cifDecCorr(a, A);
s = cifSigCorr(a, A);

fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre decimali corrette.\n', A, a, d);
fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre significative corrette.\n', A, a, s);
\end{lstlisting}
\begin{verbatim}
Il valore 0.9999, approssimazione di 1, ha almeno 3 cifre decimali corrette.
Il valore 0.9999, approssimazione di 1, ha almeno 4 cifre significative corrette.
\end{verbatim}

    \item $a = 332.122$, $A = 332.129$, $N = 10$
    $$ \Delta a = 0.7\dots 10^{-2} = \frac{1}{2}0.14\dots 10^{-1} < \frac{1}{2}10^{-1} $$
    $$ \delta a = 0.2\dots 10^{-4} < \frac{1}{2}10^{-4} $$
    Dunque $A$ ha almeno 1 cifra decimale corretta e almeno 5 cifre significative corrette. In realtà ha 2 cifre decimali corrette e 5 cifre significative corrette.
\begin{lstlisting}[style=matlabCode]
a = 332.122;   % Valore Esatto
A = 332.129;   % Valore Approssimato

d = cifDecCorr(a, A);
s = cifSigCorr(a, A);

fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre decimali corrette.\n', A, a, d);
fprintf('Il valore %g, approssimazione di %g, ha almeno %d cifre significative corrette.\n', A, a, s);
\end{lstlisting}
\begin{verbatim}
Il valore 332.129, approssimazione di 332.122, ha almeno 1 cifre decimali corrette.
Il valore 332.129, approssimazione di 332.122, ha almeno 5 cifre significative corrette.
\end{verbatim}
\end{enumerate}
\end{exmp}

\vspace{10pt}

\paragraph{Unità di roundoff}
\begin{bxthm}
\begin{thm}
Denotando con $A = fl(a)$ il numero macchina che si ottiene arrotondando il numero reale $a$ a $t$ cifre significative nella base $N$, si ha
$$ \frac{|fl(a) - a|}{|a|} \le u $$
dove
$$ u = \frac{1}{2}N^{1-t} $$
La quantità $u$ è detta \textbf{unità di roundoff}.    
\end{thm}
\end{bxthm}

\vspace{10pt}

Il teorema precedente ci dice che ogni numero può essere rappresentato su un calcolatore con un errore che non eccede $u$.
Infatti, ponendo $\delta = \frac{fl(a) - a}{a}$, si ha
$$ fl(a) = a(1 + \delta), \quad |\delta| \le u $$
La quantità $u$ è una costante propria di ciascuna aritmetica floating-point e rappresenta la massima precisione di calcolo raggiungibile con un calcolatore su cui tale aritmetica è implementata.

\vspace{10pt}

\paragraph{Epsilon-macchina}
Si chiama invece \textbf{precisione di macchina} o \textbf{epsilon di macchina} la distanza tra due numeri macchina consecutivi.
È possibile dimostrare che l'epsilon di macchina è esattamente il doppio della unità di roundoff, cioè $\varepsilon = 2u$.
Dunque parleremo indifferentemente di unità di roundoff o di precisione di macchina.
Nel caso dell'insieme $\mathbb{F}$ (doppia precisione standard IEEE 754, con 53 bit di mantissa inclusa quella nascosta), l'unità di roundoff è uguale a:
$$ u = \frac{1}{2}2^{1-53} = 2^{-53} \approx 1.110223024625157e-016 $$
e quindi
$$ \varepsilon = 2^{-52} \approx 2.220446049250313e-016 $$

\vspace{10pt}

La precisione di macchina $\varepsilon$ rappresenta l'ampiezza del buco esistente tra i numeri macchina consecutivi:
$$ 1.000000000000000 $$
e
$$ 1.000000000000001 $$

\vspace{10pt}

\paragraph{Esercizi in MatLab}
Dato il numero reale $a$ e la sua approssimazione $A$ calcolare l'errore assoluto $\Delta a$ e l'errore relativo $\delta a$ e stabilire quante sono le cifre decimali corrette e le cifre significative corrette di $A$.

\vspace{10pt}

\begin{exmp}
$$ a = 123.1256 \quad \text{e} \quad A = 123.12555551 $$
\begin{verbatim}
>> a=123.1256; A=123.12555551;
>> errAss=abs(a-A)
errAss =
     4.449000000761316e-005
>> errRel=errAss/abs(a)
errRel =
     3.613383407480911e-007
\end{verbatim}
Usando la rappresentazione normalizzata si ha:
$$ \text{errAss} = 4.449\dots 10^{-5} = 0.4449\dots 10^{-4} $$
$$ \text{errRel} = 3.613\dots 10^{-7} = 0.3613\dots 10^{-6} $$
Dunque $A$ ha almeno 4 cifre decimali corrette e almeno 7 cifre significative corrette.
In realtà ha proprio 4 cifre decimali corrette e 7 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
$a = 0.11$ e $A = 0.11111112$
\begin{verbatim}
>> a=0.11; A=0.11111112;
>> errAss=abs(a-A)
errAss =
     1.111119999999993e-003
>> errRel=errAss/abs(a)
errRel =
     1.010109090909085e-002
\end{verbatim}
Usando la rappresentazione normalizzata si ha:
$$ \text{errAss} = 1.111\dots 10^{-3} = 0.1111\dots 10^{-2} $$
$$ \text{errRel} = 1.010\dots 10^{-2} = 0.1010\dots 10^{-1} $$
Dunque $A$ ha almeno 2 cifre decimali corrette e almeno 2 cifre significative corrette. In realtà ha proprio 2 cifre decimali corrette e 2 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
$a = 1000.212324$ e $A = 1000.2123231$
\begin{verbatim}
>> a=1000.212324; A=1000.2123231;
>> errAss=abs(a-A)
errAss =
     8.999999181469320e-007
>> errRel=errAss/abs(a)
errRel =
     8.998088671290278e-010
\end{verbatim}
Usando la rappresentazione normalizzata si ha:
$$ \text{errAss} = 8.999\dots 10^{-7} = \frac{1}{2}0.1799\dots 10^{-5} < \frac{1}{2}10^{-5} $$
$$ \text{errRel} = 8.998\dots 10^{-10} = \frac{1}{2}0.1799\dots 10^{-8} < \frac{1}{2}10^{-8} $$
Dunque $A$ ha almeno 5 cifre decimali corrette e almeno 9 cifre significative corrette. In realtà ha proprio 5 cifre decimali corrette e 9 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\paragraph{Variabile \texttt{eps} in MatLab}
In MatLab la variabile \texttt{eps} contiene il valore
$$ \varepsilon = 2^{-52} = 2.22044604925031e-016 $$
\begin{verbatim}
>> eps
ans =
     2.22044604925031e-016
\end{verbatim}

Verifichiamo che $\varepsilon$ è il buco tra i numeri macchina $1.000000000000000$ e $1.000000000000001$ utilizzando la sua caratterizzazione e ricordando che il programma fornisce come risposta 1 per vero e 0 per falso.
\begin{verbatim}
>> 1+eps>1
ans =
     1
>> 1+eps/2>1
ans =
     0
\end{verbatim}

Osserviamo, però, che, contrariamente a quanto ci aspettiamo:
\begin{verbatim}
>> 1+eps
ans =
     1.00000000000000e+000
\end{verbatim}
Ciò è dovuto ad un errore di visualizzazione del numero $1+eps$.
Invece, rispettando le nostre aspettative:
\begin{verbatim}
>> 1+eps/2
ans =
     1
\end{verbatim}

In MatLab esiste una routine che permette di calcolare il massimo errore relativo che si commette approssimando un qualunque numero macchina $A$.
\begin{verbatim}
>> x=10.23;
>> eps(x)
ans =
     1.77635683940025e-015
>> x+eps(x)>x
ans =
     1
>> x+eps(x)/2>x
ans =
     0
\end{verbatim}

Ovviamente:
\begin{verbatim}
>> eps(1)
ans =
     2.22044604925031e-016
\end{verbatim}
cioè \texttt{eps(1) = eps}.

\vspace{20pt}

\subsection{Operazioni Macchina e Cancellazione Numerica}

\vspace{10pt}

\paragraph{Terza conseguenza della rappresentazione dei numeri come parole di lunghezza fissa - Errori nei calcoli}
Non sempre una operazione tra due o più numeri macchina produce un risultato che è un numero macchina.
Si può verificare una situazione di overflow o underflow.
Inoltre la forma normalizzata del risultato potrebbe avere un numero di cifre decimali superiore alla precisione con cui si sta lavorando.
Pertanto in un calcolatore è impossibile implementare esattamente le operazioni aritmetiche.

Dovremo accontentarci delle cosiddette operazioni macchina: a due numeri macchina viene associato un terzo numero macchina ottenuto arrotondando l'esatto risultato dell'operazione aritmetica.

Siano $a$ e $b$ due numeri reali e siano $A=fl(a)$ e $B=fl(b)$ i corrispondenti numeri macchina.
Denotando con $\oplus, \ominus, \otimes, \oslash$ le operazioni macchina corrispondenti rispettivamente alle operazioni aritmetiche $+,-, \times, /$, si ha:
$$ A \oplus B = fl(A+B) = (A+B)(1+\delta_{1}) $$
$$ A \ominus B = fl(A-B) = (A-B)(1+\delta_{2}) $$
$$ A \otimes B = fl(A \times B) = (A \times B)(1+\delta_{3}) $$
$$ A \oslash B = fl(A/B) = (A/B)(1+\delta_{4}) $$
con
$$ |\delta_{i}| \le u \le \varepsilon, \quad i\in\{1,2,3,4\} $$
Dunque, quando si effettua una operazione macchina si commette un errore dell'ordine della precisione di macchina $\varepsilon$.

\paragraph{Osservazione}
I precedenti risultati valgono per lo standard IEEE 754 che utilizza i cosiddetti bit di guardia, cioè per eseguire le operazioni i numeri macchina vengono memorizzati in opportuni registri con dei bit aggiuntivi allo scopo di ridurre gli effetti degli errori di arrotondamento.
In tale sistema si ha anche:
$$ fl(\sqrt{A}) = \sqrt{A}(1+\delta), \quad |\delta| \le \varepsilon $$

Utilizzando i precedenti risultati è possibile stimare, in via teorica, gli errori in tutte le espressioni.
Per esempio, se $X, Y$ e $Z$ sono numeri macchina si ha:
$$ X \otimes (Y \oplus Z) = X \otimes fl(Y+Z) = fl(X \times fl(Y+Z)) $$
$$ = (X \times fl(Y+Z)) \times (1+\delta_{1}) $$
$$ = (X \times (Y+Z) \times (1+\delta_{2})) \times (1+\delta_{1}) $$
$$ \approx X \times (Y+Z) \times (1+\delta_{1}+\delta_{2}) $$
avendo trascurato il prodotto $\delta_{1}\delta_{2}$ perché piccolo.
Dunque
$$ X \otimes (Y \oplus Z) \approx X \times (Y+Z)(1+\delta_{3}), \quad \delta_{3}=\delta_{1}+\delta_{2}, \quad |\delta_{3}| \le \varepsilon $$

\paragraph{Domanda}
Per le operazioni macchina valgono ancora le note proprietà (commutativa, associativa, distributiva, etc) delle operazioni aritmetiche?
La risposta non è sempre affermativa.

La proprietà commutativa permane:
$$ A \oplus B = B \oplus A \quad \text{e} \quad A \otimes B = B \otimes A $$
mentre, in generale,
$$ (A \oplus B) \oplus C \ne A \oplus (B \oplus C) $$
$$ (A \otimes B) \otimes C \ne A \otimes (B \otimes C) $$
$$ A \otimes (B \oplus C) \ne (A \otimes B) \oplus (A \otimes C) $$

Un'ulteriore relazione anomalia è:
$$ A \oplus B = A, \quad \text{se } |B| \ll |A| $$
dunque, l'elemento neutro della somma non è unico.

\begin{exmp}
$N=10, t=16$
$$ a = \exp(18) $$
$$ b = \exp(-20) $$
$$ A = fl(a) = 0.6565996913733051 \cdot 10^{8} $$
$$ B = fl(b) = 0.2061153622438558 \cdot 10^{-8} $$
$$ A \oplus B = fl(A+B) = 0.6565996913733051 \cdot 10^{8} = A $$    
\end{exmp}

Ciò accade perché, nell'eseguire la somma tra numeri macchina aventi esponenti diversi, il calcolatore esegue i seguenti passi:
\begin{enumerate}
    \item individua il numero avente l'esponente $\overline{q}$ più grande;
    \item memorizza tutti gli altri numeri utilizzando la loro rappresentazione con esponente $\overline{q}$;
    \item esegue la somma.
\end{enumerate}
Infatti, nel precedente esempio, si ha
$$ B = fl(b) = 0.2061153622438558 \cdot 10^{-8} $$
$$ = 0.00000000000000002061153622438558 \cdot 10^{8} $$
e, arrotondando a 16 cifre,
$$ B = fl(b) = 0 $$

\paragraph{Somma di più numeri}
Sempre a causa dell'errore di incolonnamento, se occorre sommare più numeri, tutti positivi e aventi ordini di grandezza diversi, per ottenere un risultato finale accurato conviene procedere in ordine ascendente (dal più piccolo al più grande), cosicché anche i valori più piccoli diano contributo alla somma.

\begin{exmp}
Dati i numeri in base 10:
\begin{center}
0.8999e+4 \quad 0.7889e+3 \quad 0.7767e+3 \\
0.7555e+2 \quad 0.6266e+2 \quad 0.4298e+1 \\
0.2581e+1 \quad 0.2653e+0 \quad 0.1580e+0
\end{center}
Supponendo di lavorare in aritmetica floating-point con $t=4$, sommare i numeri in ordine ascendente e discendente e confrontare i risultati ottenuti con il valore esatto $s=0.107101123e+5$.
Sommiamo prima in ordine discendente:
$$ s1 = fl(0.8999e+4 + 0.7889e+3) = 0.9788e+4 $$
$$ s1 = fl(s1 + 0.7767e+3) = 0.1056e+5 $$
$$ s1 = fl(s1 + 0.7555e+2) = 0.1064e+5 $$
$$ s1 = fl(s1 + 0.6266e+2) = 0.1070e+5 $$
$$ s1 = fl(s1 + 0.4298e+1) = 0.1070e+5 $$
$$ s1 = fl(s1 + 0.2653e+0) = 0.1070e+5 $$
$$ s1 = fl(s1 + 0.2581e+1) = 0.1070e+5 $$
$$ s1 = fl(s1 + 0.1580e+0) = 0.1070e+5 $$
Osserviamo che solo i primi 5 numeri danno contributo alla somma.

Sommiamo ora in ordine ascendente:
$$ s2 = fl(0.1580e+0 + 0.2653e+0) = 0.4233e+0 $$
$$ s2 = fl(s2 + 0.2581e+1) = 0.3004e+1 $$
$$ s2 = fl(s2 + 0.4298e+1) = 0.7302e+1 $$
$$ s2 = fl(s2 + 0.6266e+2) = 0.6996e+2 $$
$$ s2 = fl(s2 + 0.7555e+2) = 0.1455e+3 $$
$$ s2 = fl(s2 + 0.7767e+3) = 0.9222e+3 $$
$$ s2 = fl(s2 + 0.7889e+3) = 0.1711e+4 $$
$$ s2 = fl(s2 + 0.8999e+4) = 0.1071e+5 $$
Osserviamo che tutti i numeri danno contributo alla somma.

Calcolando gli errori relativi:
$$ \frac{|s-s1|}{|s|} = 0.944 \cdot 10^{-3} \quad \text{(somma discendente)} $$
$$ \frac{|s-s2|}{|s|} = 0.105 \cdot 10^{-4} \quad \text{(somma ascendente)} $$
deduciamo che, sommando in ordine discendente la somma viene calcolata con 3 cifre significative corrette, mentre, sommando in ordine ascendente la somma viene calcolata con 5 cifre significative corrette.

\begin{lstlisting}[style=matlabCode, title={Applicazione in MATLAB}]
% Definiamo una "funzione macchina" che arrotonda a 4 cifre significative
% Ogni volta che facciamo un calcolo, dobbiamo passare di qui.
fl = @(x) round(x, 4, 'significant');

% Valore esatto della somma
su = 0.107101123e+5;

% Dati di input
a1 = 0.8999e+4;
b1 = 0.7889e+3;
c1 = 0.7767e+3;
a2 = 0.7555e+2;
b2 = 0.6266e+2;
c2 = 0.4298e+1;
a3 = 0.2581e+1;
b3 = 0.2653e+0;
c3 = 0.1580e+0;

% --- CASO 1: Somma dai Grandi ai Piccoli (Discendente) ---
S = a1;            % Primo numero
S = fl(S + b1);    % Sommo e taglio subito a 4 cifre!
S = fl(S + c1);    % Uso il risultato tagliato per la somma successiva...
S = fl(S + a2);
S = fl(S + b2);
S = fl(S + c2);
S = fl(S + a3);
S = fl(S + b3);
S = fl(S + c3);

% --- CASO 2: Somma dai Piccoli ai Grandi (Ascendente) ---
s = c3;            % Ultimo numero
s = fl(s + b3);    % Sommo e taglio...
s = fl(s + a3);
s = fl(s + c2);
s = fl(s + b2);
s = fl(s + a2);
s = fl(s + c1);
s = fl(s + b1);
s = fl(s + a1);

% --- Risultati ---
fprintf('Risultato somma discendente: %.4e\n', S);
fprintf('Risultato somma ascendente: %.4e\n', s);

% Errori relativi tra le due simulazioni di somma
% Somma discendente
diff = errRel(su,S);
fprintf('Errore relativo somma discendente (t=4): %.4e\n', diff);
% Somma ascendente
diff = errRel(su,s);
fprintf('Errore relativo somma ascendente (t=4): %.4e\n', diff);
\end{lstlisting}
\begin{verbatim}
Risultato somma discendente: 1.0700e+04
Risultato somma ascendente: 1.0710e+04
Errore relativo somma discendente (t=4): 9.4418e-04
Errore relativo somma ascendente (t=4): 1.0485e-05
\end{verbatim}
\end{exmp}

\paragraph{Cancellazione Numerica}
È un fenomeno che si verifica durante l'operazione di sottrazione tra due numeri quasi uguali.
Siano $x_{1}$ e $x_{2}$ due numeri reali. Se $x=x_{1}-x_{2}$ è molto piccolo, l'errore relativo
$$ \delta x = \left| \frac{fl(x_{1}-x_{2})-x}{x} \right| $$
può essere molto grande e ciò produce una perdita di cifre significative nel calcolo di $fl(x_{1}-x_{2})$.

\paragraph{Osservazione}
È sempre preferibile evitare la sottrazione tra numeri macchina quasi uguali utilizzando, laddove possibile, delle espressioni alternative.
Per esempio:
\begin{enumerate}
    \item $$ \sqrt{x+\delta}-\sqrt{x} = \frac{\delta}{\sqrt{x+\delta}+\sqrt{x}} $$
    \item $$ \cos(x+\delta)-\cos(x) = -2 \sin\left(\frac{\delta}{2}\right)\sin\left(x+\frac{\delta}{2}\right) $$
\end{enumerate}
In generale, se si presenta il caso di dover computare
$$ f(x+\delta)-f(x), $$
con $|\delta| \ll |x|$, può convenire considerare l'espansione di Taylor
$$ f(x+\delta)-f(x) = f'(x)\delta + f(x)\frac{\delta}{2} + \dots $$

Un problema di cancellazione si può presentare anche nella formula risolutiva dell'equazione di secondo grado $ax^{2}+bx+c=0$:
$$ x_{1,2} = \frac{-b \pm \sqrt{b^{2}-4ac}}{2a} $$
per cui conviene usare la formula alternativa:
$$ x_{1} = \frac{-b - \text{sign}(b)\sqrt{b^{2}-4ac}}{2a} $$
$$ x_{2} = \frac{c}{ax_{1}} $$

\paragraph{Esempi in MatLab}
\begin{exmp}
Verifichiamo che non sempre una operazione tra numeri macchina produce un numero macchina.
\begin{lstlisting}[style=matlabCode]
a = 1.000345566e+150;
b = 0.15646464e+200;
disp(a*b)

a = 1.000345566e-25;
b = 1.56464646e-300;
disp(a*b)
\end{lstlisting}
\begin{verbatim}
   Inf

     0
\end{verbatim}    
\end{exmp}

\vspace{10pt}

Da quest'ultimo esempio si evince anche che il prodotto tra due numeri macchina può essere nullo anche se entrambi i numeri sono non nulli, cioè non vale la legge di annullamento del prodotto.

\vspace{10pt}

\begin{exmp}
Siano $a=0.23371258 \cdot 10^{-4}$, $b=0.33678429 \cdot 10^{2}$, $c=-0.33677811 \cdot 10^{2}$, verificare che $(a \oplus b) \oplus c \ne a \oplus (b \oplus c)$.
\begin{lstlisting}[style=matlabCode]
format long e;

% Dati
a = 0.23371258e-4;
b = 0.33678429e+2;
c = -0.33677811e+2;

% Output: [a, b, c]
fprintf('a: %.15e\nb: %.15e\nc: %.15e\n', a, b, c);

% Calcoli
s1 = (a + b) + c;
s2 = a + (b + c);

% Output: [s1, s2, differenza]
fprintf('(a + b) + c: %.15e\na + (b + c): %.15e\nDifferenza tra le due somme: %.15e\n', s1, s2, s1-s2);
\end{lstlisting}
\begin{verbatim}
a: 2.337125800000000e-05
b: 3.367842900000000e+01
c: -3.367781100000000e+01
(a + b) + c: 6.413712580055630e-04
a + (b + c): 6.413712580028940e-04
Differenza tra le due somme: 2.669088908224815e-15
\end{verbatim}    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Siano $a=0.1234567$, $b=6666.325$ e $c=-6666.325$, verificare che $(a \oplus b) \oplus c \ne a \oplus (b \oplus c)$.
\begin{lstlisting}[style=matlabCode]
format long e;

% Dati
a = 0.1234567;
b = 6666.325;
c = -6666.325;

% Output: [a, b, c]
fprintf('a: %.15e\nb: %.15e\nc: %.15e\n', a, b, c);

% Calcoli
s1 = (a + b) + c;
s2 = a + (b + c);

% Output: [s1, s2, differenza]
fprintf('(a + b) + c: %.15e\na + (b + c): %.15e\nDifferenza tra le due somme: %.15e\n', s1, s2, s1-s2);
\end{lstlisting}
\begin{verbatim}
a: 1.234567000000000e-01
b: 6.666325000000000e+03
c: -6.666325000000000e+03
(a + b) + c: 1.234567000001334e-01
a + (b + c): 1.234567000000000e-01
Differenza tra le due somme: 1.334210519843282e-13
\end{verbatim}
\end{exmp}

\vspace{10pt}

\begin{exmp}
Siano $a=0.8e+9$, $b=0.500009e+4$ e $c=-0.500008e+4$, verificare che $a \otimes (b \oplus c) \ne a \otimes b \oplus a \otimes c$.
\begin{lstlisting}[style=matlabCode]
format long e;

% Dati
a = 0.8e+9
b = 0.500009e+4
c = -0.500008e+4

% Output: [a, b, c]
fprintf('a: %.15e\nb: %.15e\nc: %.15e\n', a, b, c);

% Calcoli
s1 = a * (b + c);
s2 = a * b + a * c;

% Output: [s1, s2, differenza]
fprintf('a * (b + c): %.15e\na * b + a * c: %.15e\nDifferenza tra le due operazioni: %.15e\n', s1, s2, s1-s2);
\end{lstlisting}
\begin{verbatim}
a: 8.000000000000000e+08
b: 5.000090000000000e+03
c: -5.000080000000000e+03
a * (b + c): 8.000000000174623e+06
a * b + a * c: 8.000000000000000e+06
Differenza tra le due operazioni: 1.746229827404022e-04
\end{verbatim}
Infatti:
%\begin{lstlisting}[style=matlabCode]
%format long e;
%
%% Dati
%a = 0.8e+9;
%b = 0.500009e+4;
%c = -0.500008e+4;
%
%% Output: [a, b, c]
%%fprintf('a: %.15e\nb: %.15e\nc: %.15e\n', a, b, c);
%
%% Calcoli
%s1 = a * b;
%s2 = a * c;
%s3 = a * b + a * c;
%
%% Output: [s1, s2, s3]
%fprintf('a * b: %.15e\na * c: %.15e\na * b + a * c: %.15e\n', s1, s2, s3);
%\end{lstlisting}
\begin{verbatim}
a * b: 4.000072000000000e+12
a * c: -4.000064000000000e+12
a * b + a * c: 8.000000000000000e+06
\end{verbatim}
\end{exmp}

\vspace{10pt}

\begin{exmp}
Siano $a=7.45700244034177e-001$, $b=3.77400852642836e-001$ e $c=7.62332001118890e-001$, verificare che $(a \otimes b) \otimes c \ne a \otimes (b \otimes c)$.
\begin{lstlisting}[style=matlabCode]
format long e;

% Dati
a = 7.45700244034177e-001;
b = 3.77400852642836e-001;
c = 7.62332001118890e-001;

% Output: [a, b, c]
fprintf('a: %.15e\nb: %.15e\nc: %.15e\n', a, b, c);

% Calcoli
p1 = (a * b) * c;
p2 = a * (b * c);

% Output: [s1, s2, differenza]
fprintf('(a * b) * c: %.15e\na * (b * c): %.15e\nDifferenza tra i due prodotti: %.15e\n', p1, p2, p1-p2);
\end{lstlisting}
\begin{verbatim}
a: 7.457002440341770e-01
b: 3.774008526428360e-01
c: 7.623320011188900e-01
(a * b) * c: 2.145415002111401e-01
a * (b * c): 2.145415002111401e-01
Differenza tra i due prodotti: 2.775557561562891e-17
\end{verbatim}
Osserviamo che i valori $p1$ e $p2$ risultano essere uguali anche se i corrispondenti numeri macchina non lo sono (la loro differenza non è nulla). 
Ciò accade per un errore di visualizzazione o di conversione dal binario al decimale.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Verifichiamo che dati $c=0.164646415647 \cdot 10^{+9}$ e $d=0.38731646 \cdot 10^{-7}$ si ha $c \oplus d = fl(c+d) = fl(c)$.
\begin{verbatim}
>> c = 0.164646415647e+9;
>> d = 0.38731646e-7;
>> c+d
ans =
     1.646464156470000e+008
\end{verbatim}    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Verificare che l'uguaglianza
$$ \sqrt{x+\delta}-\sqrt{x} = \frac{\delta}{\sqrt{x+\delta}+\sqrt{x}} $$
non è vera se si lavora in aritmetica finita.
Supponiamo $x=4$ e $\delta=0.002$.
\begin{verbatim}
>> a = sqrt(4.002)
a =
     2.000499937515620e+000
>> b = sqrt(4)
b =
     2
>> a-b
ans =
     4.999375156202746e-004
>> 0.002/(a+b)
ans =
     4.999375156201189e-004
\end{verbatim}
Con la prima formula si perdono 4 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Verificare che l'uguaglianza
$$ \cos(x+\delta)-\cos(x) = -2 \sin\left(\frac{\delta}{2}\right)\sin\left(x+\frac{\delta}{2}\right) $$
non è vera se si lavora in aritmetica finita.
Supponiamo $x=\pi$ e $\delta=0.0001$.
\begin{verbatim}
>> cos(pi+0.0001)-cos(pi)
ans =
     4.99999969612645e-009
>> -2*sin(0.0001/2)*sin(pi+0.0001/2)
ans =
     4.999999995809435e-009
\end{verbatim}
Con la prima formula si perdono 8 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Data l'equazione di secondo grado
$$ x^{2}-0.4002x+0.8e-4=0, $$
calcolare le radici con le formule:
$$ x_{1,2} = \frac{-b \pm \sqrt{b^{2}-4ac}}{2a}, $$
e
$$ x_{1} = \frac{-b - \text{sign}(b)\sqrt{b^{2}-4ac}}{2a}, \quad x_{2} = \frac{c}{ax_{1}} $$
e confrontare i risultati ottenuti con i valori esatti $x_{1}=0.4$ e $x_{2}=0.0002$.
Osserviamo prima di tutto che se calcoliamo il valore del polinomio di secondo grado nelle sue radici non si ottiene 0 ma uno degli infiniti zeri macchina.
\begin{verbatim}
>> x = 0.4;
>> x^2 - 0.4002*x + 0.8e-4
ans =
     3.102173466024150e-017
>> x = 0.0002;
>> x^2 - 0.4002*x + 0.8e-4
ans =
     1.355252715606881e-020
\end{verbatim}
Dunque l'equazione è numericamente verificata a meno di un errore dell'ordine dell'epsilon-macchina.

\begin{verbatim}
>> a = 1; b = -0.4002; c = 0.8e-4;
>> x1 = (-b + sqrt(b^2 - 4*a*c))/(2*a)
x1 =
     4.000000000000000e-001
>> x2 = (-b - sqrt(b^2 - 4*a*c))/(2*a)
x2 =
     1.999999999999780e-004
>> x22 = c/x1
x22 =
     2.000000000000000e-004
\end{verbatim}
È evidente che con la prima formula si perdono 3 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Valutare le espressioni analiticamente equivalenti
$$ y_{1} = \frac{1-\cos(t)}{t^{2}} \quad \text{e} \quad y_{2} = \frac{1}{2}\frac{\sin^{2}(t/2)}{(t/2)^{2}} $$
nel punto $t=1.2e-5$.
\begin{verbatim}
>> t = 1.2e-5;
>> y1 = (1-cos(t))/t^2
y1 =
     4.999997329749008e-001
>> y2 = 0.5*(sin(t/2)/(t/2))^2
y2 =
     4.999999999940000e-001
\end{verbatim}
La diversità dei risultati ottenuti è dovuta al fenomeno della cancellazione numerica verificatosi nel calcolo di $1-\cos(t)$, perché, con la scelta fatta di $t$, $1$ e $\cos(t)$ sono quasi uguali.
Poiché l'espressione $y_{2}$ è definita mediante operazioni che non introducono alcuna perdita di precisione, possiamo ritenere che il valore $y2$ abbia 15 cifre significative corrette.
Pertanto, confrontando $y1$ con $y2$ deduciamo che con l'espressione $y_{1}$ si ottengono solo 6 cifre significative corrette.    
\end{exmp}

\vspace{10pt}

\begin{exmp}
Valutare le espressioni, analiticamente equivalenti,
$$ y_{1} = (1-x)^{6} \quad \text{e} \quad y_{2} = x^{6}-6x^{5}+15x^{4}-20x^{3}+15x^{2}-6x+1 $$
in 100 punti equidistanti nell'intervallo $[1-\delta, 1+\delta]$ per $\delta=0.1, 0.01, 0.005, 0.0025$. Rappresentare graficamente $y_{1}$ e $y_{2}$ per ciascun valore di $\delta$ assegnato.

\begin{verbatim}
>> delta = 0.1;
>> x = linspace(1-delta, 1+delta);
>> y1 = (1-x).^6;
>> y2 = x.^6 - 6*x.^5 + 15*x.^4 - 20*x.^3 + 15*x.^2 - 6*x + 1;
>> figure(1)
>> plot(x, y1, x, y2, 'r')
\end{verbatim}
Si ottiene il grafico relativo a $\delta=0.1$.

\begin{verbatim}
>> delta = 0.01;
>> x = linspace(1-delta, 1+delta);
>> y1 = (1-x).^6;
>> y2 = x.^6 - 6*x.^5 + 15*x.^4 - 20*x.^3 + 15*x.^2 - 6*x + 1;
>> figure(2)
>> plot(x, y1, x, y2, 'r')
\end{verbatim}
Si ottiene il grafico relativo a $\delta=0.01$.

\begin{verbatim}
>> delta = 0.005;
>> x = linspace(1-delta, 1+delta);
>> y1 = (1-x).^6;
>> y2 = x.^6 - 6*x.^5 + 15*x.^4 - 20*x.^3 + 15*x.^2 - 6*x + 1;
>> figure(3)
>> plot(x, y1, x, y2, 'r')
\end{verbatim}
Si ottiene il grafico relativo a $\delta=0.005$.

\begin{verbatim}
>> delta = 0.0025;
>> x = linspace(1-delta, 1+delta);
>> y1 = (1-x).^6;
>> y2 = x.^6 - 6*x.^5 + 15*x.^4 - 20*x.^3 + 15*x.^2 - 6*x + 1;
>> figure(4)
>> plot(x, y1, x, y2, 'r')
\end{verbatim}
Si ottiene il grafico relativo a $\delta=0.0025$.

Osservando i precedenti grafici, deduciamo che le rappresentazioni grafiche ottenute utilizzando le espressioni $y_{1}$ e $y_{2}$ si discostano l'una dall'altra sempre di più al diminuire di $\delta$.
Infatti, mentre $y_{1}$ rimane fedele al reale andamento del polinomio, $y_{2}$ ha un andamento sempre più oscillante (come se avesse tanti zeri!).
Tali oscillazioni sono dovute al fenomeno della cancellazione numerica che si verifica quando in $y_{2}$ si esegue la somma tra
$$ x^{6}-6x^{5}+15x^{4}-20x^{3}+15x^{2}-6x \quad \text{e} \quad 1 $$
per valori molto vicini a 1 ($x^{6}-6x^{5}+15x^{4}-20x^{3}+15x^{2}-6x < 0$ per ogni $x \in [-1, 1]$).
Osserviamo che il suddetto fenomeno, seppure non evidente graficamente, si verifica anche quando in $y_{1}$ si esegue la sottrazione tra 1 e $x$ per valori di $x$ molto vicini a 1.
Tuttavia, essendo le quantità
$$ (1-x)^{6} \ll (1-x) $$
le quantità
$$ x^{6}-6x^{5}+15x^{4}-20x^{3}+15x^{2}-6x \quad \text{e} \quad 1 $$
hanno più cifre in comune delle quantità 1 e $x$.
Ne consegue che nella valutazione di $y_{2}$ si ha una perdita di precisione maggiore rispetto a $y_{1}$.    
\end{exmp}

\vspace{20pt}

\subsection{Propagazione degli Errori Introdotti nei Dati Iniziali}

\vspace{10pt}

\paragraph{Condizionamento di un problema}
L'esempio della cancellazione numerica introduce un concetto molto rilevante del Calcolo Scientifico: il condizionamento di un problema.
Nella risoluzione di un problema matematico è in molti casi utile avere una misura di quanto i risultati siano sensibili a piccoli cambiamenti dei dati iniziali.
In generale se a piccoli cambiamenti nei dati iniziali corrispondono grandi cambiamenti nei dati finali diremo che il problema è \textbf{mal condizionato}.
In caso contrario diremo che è \textbf{ben condizionato}.
La quantità responsabile dell'amplificazione degli errori nei dati iniziali si chiama \textbf{indice di condizionamento}.
Il condizionamento è una caratteristica propria del problema matematico e non ha alcun legame né con gli errori di arrotondamento delle operazioni macchina, né con gli algoritmi eventualmente scelti per risolverlo.
Determinare l'indice di condizionamento di un problema non è sempre facile.
Consideriamo il caso semplice del problema numerico della somma.
Siano $x, y$ e $z$ tre numeri reali con
$$ z = x + y $$
Supponiamo di perturbare $x$ con $\Delta x$ e $y$ con $\Delta y$ e sia
$$ \overline{z} = (x + \Delta x) + (y + \Delta y) $$
Calcoliamo l'errore relativo che commettiamo approssimando $z$ con $\overline{z}$:
\begin{align*}
\delta z &= \frac{|z - \overline{z}|}{|z|} = \frac{|(x + y) - (x + \Delta x + y + \Delta y)|}{|x + y|} = \frac{|-\Delta x - \Delta y|}{|x + y|} = \frac{|\Delta x + \Delta y|}{|x + y|} \\[1ex]
&\le \frac{|\Delta x| + |\Delta y|}{|x + y|} = \frac{1}{|x + y|} \left( |\Delta x| \cdot \frac{|x|}{|x|} + |\Delta y| \cdot \frac{|y|}{|y|} \right) \\[1ex]
&= \frac{1}{|x + y|} \left( |x| \underbrace{\frac{|\Delta x|}{|x|}}_{\delta x} + |y| \underbrace{\frac{|\Delta y|}{|y|}}_{\delta y} \right) = \frac{1}{|x + y|} (|x| \delta x + |y| \delta y) \\[1ex]
&\le \frac{1}{|x + y|} \left( \max\{|x|, |y|\} \delta x + \max\{|x|, |y|\} \delta y \right) = \frac{\max\{|x|, |y|\}}{|x + y|} (\delta x + \delta y)
\end{align*}
La quantità
$$ \frac{\max\{|x|, |y|\}}{|x + y|} $$
rappresenta l'\textbf{indice di condizionamento}.
Ciò significa che se facciamo la somma tra due numeri molto vicini e di segno opposto, anche se gli errori relativi $\delta x$ e $\delta y$ sono piccoli, il valore $\overline{z}$ approssimerà $z$ con un errore relativo grande, cioè l'operazione somma diventa mal condizionata e si verifica il fenomeno della Cancellazione Numerica.
In generale, denotato con $f(x)$ un generico problema numerico avente $x$ come dato iniziale, si cerca di determinare una relazione del tipo
$$ \frac{|f(x) - f(\overline{x})|}{|f(x)|} \le K \frac{|\Delta x|}{|x|} $$
dove $K$ rappresenta l'indice di condizionamento del problema $f$.
Quanto più è piccolo $K$, tanto più il problema è ben condizionato.

\vspace{10pt}

\begin{exmp}
Consideriamo il sistema lineare di due equazioni in due incognite
$$ \begin{cases} x_1 - x_2 = 1 \\ x_1 - 1.00001x_2 = 0 \end{cases} $$
le cui soluzioni esatte sono
$$ x_1 = 100001, \quad x_2 = 100000 $$
Perturbiamo di una piccola quantità pari a $0.2 \cdot 10^{-4}$ il coefficiente dell'incognita $x_2$ nella seconda equazione. Il suo nuovo coefficiente sarà $-1.00001 + 0.2 \cdot 10^{-4} = -0.99999$.
Il nuovo sistema diventa
$$ \begin{cases} y_1 - y_2 = 1 \\ y_1 - 0.99999y_2 = 0 \end{cases} $$
e le sue soluzioni sono
$$ y_1 = -99999, \quad y_2 = -100000 $$
\end{exmp}

\vspace{10pt}

\paragraph{Osservazioni}
\begin{itemize}
    \item Provando a risolvere i precedenti sistemi con il metodo di Cramer o il metodo di sostituzione si ottengono soluzioni diverse da quelle indicate. Per verificare che le soluzioni indicate sono corrette, basta sostituirle in ciascuna riga del sistema.
    \item Poiché le soluzioni sono state ricavate lavorando in aritmetica infinita, l'unico motivo per il quale, in seguito alla piccola perturbazione, si sono ottenuti risultati tanto diversi è che la risoluzione del sistema di partenza è un problema mal condizionato.
\end{itemize}

\vspace{10pt}

\paragraph{Stabilità di un algoritmo}
\begin{bxthm}
\begin{defn}
Un algoritmo, per un problema numerico, è una sequenza finita di operazioni non ambigue che trasforma i dati di input e fornisce uno o più valori di output.
Diremo che un algoritmo è \textbf{instabile} se amplifica gli errori di arrotondamento durante la computazione della soluzione di un problema ben condizionato.
Dunque il concetto di stabilità di un algoritmo è strettamente connesso con la precisione di calcolo finita e con le operazioni in cui si risolve l'algoritmo stesso.    
\end{defn}
\end{bxthm}

\vspace{10pt}

\begin{exmp}
Consideriamo ora il seguente sistema lineare di due equazioni in due incognite
$$ \begin{cases} \alpha x_1 + x_2 = 1 + \alpha \\ x_1 = 1 \end{cases} $$
la cui soluzione esatta è
$$ x_1 = x_2 = 1 $$
Si dimostra che il problema della sua risoluzione è ben condizionato.
Nonostante l'immediatezza della soluzione, seguiamo lo schema di calcolo del metodo di eliminazione di Gauss.
Moltiplicando ambo i membri della prima equazione per $-\frac{1}{\alpha}$, otteniamo
$$ -\frac{1}{\alpha}(\alpha x_1 + x_2) = -\frac{1}{\alpha}(1 + \alpha), $$
cioè
$$ -x_1 - \frac{1}{\alpha}x_2 = -\frac{1 + \alpha}{\alpha} $$
Sommando la precedente equazione alla seconda equazione ($x_1 = 1$), otteniamo
$$ -x_1 - \frac{1}{\alpha}x_2 + x_1 = 1 - \frac{1 + \alpha}{\alpha} $$
$$ -\frac{1}{\alpha}x_2 = 1 - \frac{1 + \alpha}{\alpha} $$
Rimpiazzando la seconda equazione con quella ottenuta, otteniamo il seguente sistema equivalente al precedente:
$$ \begin{cases} \alpha x_1 + x_2 = 1 + \alpha \\ -\frac{1}{\alpha}x_2 = 1 - \frac{1 + \alpha}{\alpha} \end{cases}. $$
Computiamo i coefficienti del sistema per $\alpha = 0.5 \times 10^{-11}$.
Si ha
$$ 0.5 \times 10^{-11} x_1 + x_2 = 1 + 0.5 \times 10^{-11} $$
$$ -2 \times 10^{11} x_2 = 1 - \frac{1 + 0.5 \times 10^{-11}}{0.5 \times 10^{-11}} $$
e, dunque, eseguendo la computazione in base alla priorità degli operatori aritmetici:
$$ \begin{cases} x_2 = \frac{1}{-2 \times 10^{11}} \left( 1 - \frac{(1 + 0.5 \times 10^{-11})}{0.5 \times 10^{-11}} \right) = 1 \\ x_1 = \frac{(1 + 0.5 \times 10^{-11} - 1)}{0.5 \times 10^{-11}} = 1.000000082740371 \end{cases} $$
Scambiamo ora le equazioni tra di loro (pivoting):
$$ \begin{cases} x_1 = 1 \\ \alpha x_1 + x_2 = 1 + \alpha \end{cases} $$
e ripetiamo la procedura di eliminazione dell'incognita $x_1$ dalla seconda equazione.
Moltiplicando ambo i membri della prima equazione per $-\alpha$, otteniamo
$$ -\alpha x_1 = -\alpha $$
Sommando la precedente equazione alla seconda equazione, otteniamo
$$ -\alpha x_1 + \alpha x_1 + x_2 = -\alpha + 1 + \alpha $$
$$ x_2 = 1 $$
Dunque
$$ \begin{cases} x_1 = 1 \\ x_2 = 1 \end{cases} .$$
\end{exmp}

\vspace{10pt}

Questa procedura che, sotto opportune condizioni, prevede lo scambio delle righe di un sistema lineare prende il nome di \textbf{pivoting}.
La causa di questo bizzarro comportamento è da ricercarsi nella sequenza di operazioni che ci conducono dai dati iniziali ai risultati, ovvero nell'algoritmo.
Il metodo di eliminazione di Gauss in generale non è stabile, ma se viene combinato con il pivoting è sempre stabile.

\vspace{10pt}

Abbiamo detto che il problema della somma tra due numeri molto vicini è mal condizionato e genera il fenomeno della Cancellazione Numerica. 
Per questo tipo di problema è ancora più importante scegliere un algoritmo stabile.

\vspace{10pt}

Se consideriamo il problema della differenza tra due radicali, abbiamo visto che
$$ \sqrt{x + \delta} - \sqrt{x} = \frac{\delta}{\sqrt{x + \delta} + \sqrt{x}} $$
Come abbiamo già verificato con dei tests numerici, l'algoritmo che implementa la formula al primo membro è instabile mentre quello che implementa la formula al secondo membro è sempre stabile.

\vspace{10pt}

Analogo discorso si può fare per:
\begin{itemize}
    \item il calcolo della differenza tra due coseni con le formule equivalenti:
    $$ \cos(x + \delta) - \cos(x) $$
    e
    $$ -2 \sin\left(\frac{\delta}{2}\right)\sin\left(x + \frac{\delta}{2}\right) $$
    \item il calcolo delle radici dell'equazione $ax^2 + bx + c = 0$ con le formule equivalenti:
    $$ x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$
    e
    $$ x_1 = \frac{-b - \text{sign}(b)\sqrt{b^2 - 4ac}}{2a}, \quad x_2 = \frac{c}{ax_1} $$
\end{itemize}
Abbiamo già verificato con dei tests numerici, che per entrambi l'algoritmo che implementa la prima formula è instabile mentre quello che implementa la seconda è sempre stabile.

\vspace{10pt}

Dunque il controllo degli errori passa essenzialmente per due fasi:
\begin{enumerate}
    \item studio del condizionamento del problema matematico;
    \item uso di un algoritmo stabile per la soluzione del problema.
\end{enumerate}

\vspace{10pt}

\paragraph{Conclusioni}
\begin{itemize}
    \item Qualsiasi algoritmo applicato ad un problema mal condizionato genererà amplificazione degli errori di rappresentazione dei dati.
    \item Si avrà propagazione degli errori di arrotondamento applicando un algoritmo instabile ad un problema ben condizionato.
    \item Si raggiunge la massima precisione di calcolo quando un algoritmo stabile viene applicato ad un problema ben condizionato.
    \item A volte può accadere che un problema sia ben condizionato per certi dati iniziali e mal condizionato per altri (vedi l'operazione di somma).
\end{itemize}
L'obiettivo dei matematici che si occupano del Calcolo Scientifico è quello di risolvere problemi numerici di cui hanno studiato a priori il condizionamento con algoritmi che siano il più possibile stabili.

\vspace{10pt}

\paragraph{Esercizi in MatLab}

\begin{exmp}
Risolviamo il sistema
$$ \begin{cases} x_1 - x_2 = 1 \\ x_1 - 1.00001x_2 = 0 \end{cases} $$
Definiamo la matrice del sistema:
\begin{verbatim}
>> A = [1, -1; 1, -1.00001]
A =
   1.000000000000000e+000  -1.000000000000000e+000
   1.000000000000000e+000  -1.000010000000000e+000
\end{verbatim}
Definiamo il vettore dei termini noti:
\begin{verbatim}
>> b = [1; 0]
b =
     1
     0
\end{verbatim}
Verifichiamo che il vettore $[100001, 100000]'$ è la soluzione esatta del sistema:
\begin{verbatim}
>> A * [100001; 100000]
ans =
     1
     0
\end{verbatim}
Risolviamo il sistema $Ax=b$ con la function \textbackslash del MatLab:
\begin{verbatim}
>> x = A\b
x =
   1.00009999993449e+005
   9.99999999934487e+004
\end{verbatim}
Calcoliamo gli errori relativi:
\begin{verbatim}
>> err1 = abs(100001 - 1.000009999993449e+005)/abs(100001)
err1 =
   6.550915688349306e-12
>> err2 = abs(100000 - 9.999999999934487e+004)/abs(100000)
err2 =
   6.551272235810757e-12
\end{verbatim}
Si ha
$$ \text{err1} = 0.655\dots 10^{-11} < \frac{1}{2} 10^{-10} $$
$$ \text{err2} = 0.655\dots 10^{-11} < \frac{1}{2} 10^{-10} $$
cioè entrambe le soluzioni sono state calcolate con solo 11 cifre significative corrette.
La function \textbackslash del MatLab risolve i sistemi lineari con un algoritmo stabile che implementa il metodo di eliminazione di Gauss con pivoting.
Pertanto la perdita di cifre significative è dovuta al mal condizionamento della matrice del sistema.
Se $A$ denota la matrice dei coefficienti di un sistema lineare il comando \texttt{cond(A,inf)} del MatLab permette di calcolare l'indice di condizionamento del problema della risoluzione del sistema.
\begin{verbatim}
>> C = cond(A,inf)
C =
   4.000040000073795e+005
\end{verbatim}
Poiché le perturbazioni che induciamo sui dati iniziali $A$ e $b$ del problema sono dell'ordine dell'epsilon-macchina, moltiplicando l'indice di condizionamento ottenuto per la variabile \texttt{eps} del MatLab, otteniamo una stima dell'errore con cui calcoleremo la soluzione $x$.
\begin{verbatim}
>> deltax = C*eps
deltax =
   8.881873015007080e-011
\end{verbatim}
cioè
$$ \text{deltax} = 0.888\dots 10^{-10} < \frac{1}{2} 10^{-9} $$
La precedente stima ci dice che nel risolvere un sistema lineare avente $A$ come matrice dei coefficienti con un algoritmo stabile possiamo ottenere almeno 10 cifre significative corrette.
Tale stima è confermata dai tests numerici, infatti abbiamo 11 cifre.
\end{exmp}

\vspace{10pt}

\begin{exmp}
Risolviamo il sistema
$$ \begin{cases} y_1 - y_2 = 1 \\ y_1 - 0.99999y_2 = 0 \end{cases} $$
Definiamo la matrice del sistema:
\begin{verbatim}
>> A = [1, -1; 1, -0.99999]
A =
   1.000000000000000e+000  -1.000000000000000e+000
   1.000000000000000e+000  -9.999900000000000e-001
\end{verbatim}
Definiamo il vettore dei termini noti:
\begin{verbatim}
>> b = [1; 0]
\end{verbatim}
Verifichiamo che il vettore $[-99999, -100000]'$ è la soluzione esatta del sistema:
\begin{verbatim}
>> A * [-99999; -100000]
ans =
     1
     0
\end{verbatim}
Risolviamo il sistema $Ax=b$ con la function \textbackslash del MatLab:
\begin{verbatim}
>> x = A\b
x =
  -9.999900000045510e+004
  -1.000000000004551e+005
\end{verbatim}
Calcoliamo gli errori relativi:
\begin{verbatim}
>> err1 = abs(-99999 + 9.999900000045510e+004)/abs(-99999)
err1 =
   4.551011478634235e-012
>> err2 = abs(-100000 + 1.000000000004551e+005)/abs(-100000)
err2 =
   4.550965968519449e-12
\end{verbatim}
Si ha
$$ \text{err1} < \frac{1}{2} 10^{-11}, \quad \text{err2} < \frac{1}{2} 10^{-11} $$
cioè entrambe le soluzioni sono state calcolate con solo 12 cifre significative corrette.

Calcoliamo l'indice di condizionamento del problema.
\begin{verbatim}
>> C = cond(A,inf)
C =
   4.000000000018204e+005
\end{verbatim}
Stima dell'errore:
\begin{verbatim}
>> deltax = C*eps
deltax =
   8.881784197041673e-011
\end{verbatim}
cioè $\text{deltax} < \frac{1}{2} 10^{-9}$. Dunque nel risolvere un sistema lineare avente $A$ come matrice dei coefficienti con un algoritmo stabile possiamo ottenere almeno 10 cifre significative corrette. Ne abbiamo ottenute 12.
\end{exmp}

\vspace{10pt}

\begin{exmp}
Risolviamo il seguente sistema lineare con $\alpha = 0.5 \times 10^{-11}$
$$ \begin{cases} \alpha x_1 + x_2 = 1 + \alpha \\ x_1 = 1 \end{cases} $$
La sua soluzione esatta è $x_1 = x_2 = 1$ per ogni scelta di $\alpha$.
Definiamo la matrice del sistema e il vettore dei termini noti:
\begin{verbatim}
>> al = 0.5e-11;
>> A = [al, 1; 1, 0];
>> b = [1+al; 1];
\end{verbatim}
Calcoliamo l'indice di condizionamento del problema:
\begin{verbatim}
>> C = cond(A,inf)
C =
   1.000000000010000e+000
>> deltax = C*eps
deltax =
   2.220446049272518e-016
\end{verbatim}
Da cui deduciamo che il problema della risoluzione del sistema è ben condizionato e se lo risolviamo con un algoritmo stabile possiamo ottenere una soluzione con 16 cifre significative corrette.

Abbiamo visto che se lo risolviamo applicando il metodo di Gauss (instabile in questo caso) otteniamo:
\begin{verbatim}
>> x1 = (1 + 0.5e-11 - 1)/0.5e-11
x1 =
   1.000000082740371e+000
>> x2 = (1 - (1 + 0.5e-11)/0.5e-11)/(-2.e+11)
x2 =
     1
\end{verbatim}
Dunque, se un problema ben condizionato viene risolto con un algoritmo instabile si ottengono soluzioni poco accurate.

Se, invece, lo risolviamo con la function \textbackslash del MatLab che implementa il metodo di eliminazione di Gauss con pivoting (stabile), otteniamo:
\begin{verbatim}
>> A\b
ans =
     1
     1
\end{verbatim}
Dunque, se un problema ben condizionato viene risolto con un algoritmo stabile si ottengono soluzioni molto accurate.
\end{exmp}

\vspace{10pt}

\begin{exmp}
Siano $a = 1.4e154$ e $b = 1.3e154$. Calcolare le espressioni equivalenti
$$ (a^2 - b^2) \quad \text{e} \quad (a-b)(a+b) $$
e confrontare i risultati.
\begin{verbatim}
>> a = 1.4e154; b = 1.3e154;
>> (a^2 - b^2)
ans =
   Inf
>> (a-b)*(a+b)
ans =
   2.700000000000000e+307
\end{verbatim}
Si può vedere che con il primo algoritmo si ottiene un risultato che non è un numero macchina (overflow).
Ciò si verifica perché
\begin{verbatim}
>> a^2
ans =
   Inf
\end{verbatim}
Possiamo concludere che il secondo algoritmo è più stabile del primo.
\end{exmp}

\newpage

\section{Risoluzione di un Sistema Lineare Quadrato}

\vspace{20pt}

\subsection{Il Problema della Risoluione di un Sistema Lineare}

\vspace{10pt}

\paragraph{Generalità sui Sistemi Lineari}
Molti problemi dell'ingegneria, della fisica, della chimica, dell'informatica e dell'economia, si modellizzano mediante equazioni lineari, cioè equazioni in cui le incognite appaiono con esponente uguale ad uno e sono legate tra loro da relazioni lineari (prodotto per scalari e somme algebriche).
Un sistema di $n$ equazioni lineari in $n$ incognite è definito come segue:
$$ \sum_{j=1}^{n} a_{i,j}x_{j} = b_{i} \quad i\in\{1,\ldots,n\} $$
ovvero:
$$
\begin{cases}
a_{1,1}x_{1} + a_{1,2}x_{2} + \dots + a_{1,n}x_{n} = b_{1} \\
a_{2,1}x_{1} + a_{2,2}x_{2} + \dots + a_{2,n}x_{n} = b_{2} \\
\vdots \\
a_{n,1}x_{1} + a_{n,2}x_{2} + \dots + a_{n,n}x_{n} = b_{n}
\end{cases}
$$
Assumeremo che tutte le quantità in gioco siano numeri reali.

\paragraph{Forma Matriciale}
Ponendo $A$ come la matrice dei coefficienti, $b$ come il vettore dei termini noti e $x$ come il vettore delle soluzioni:
$$ A = \begin{pmatrix} a_{1,1} & \dots & a_{1,n} \\ \vdots & \ddots & \vdots \\ a_{n,1} & \dots & a_{n,n} \end{pmatrix}, \quad b = \begin{pmatrix} b_{1} \\ \vdots \\ b_{n} \end{pmatrix}, \quad x = \begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} $$
il sistema si può riscrivere come:
$$ Ax = b $$
dove $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^{n}$ e $x \in \mathbb{R}^{n}$.

\paragraph{Esistenza e Unicità della Soluzione}
È noto che il sistema ammette un'unica soluzione se e solo se la matrice $A$ è non singolare, o equivalentemente se $\det(A) \neq 0$.
In questo caso, denotata con $A^{-1}$ la matrice inversa di $A$, si ha:
$$ x = A^{-1}b $$
La matrice inversa è infatti l'unica matrice per la quale $A^{-1}A = AA^{-1} = I$, dove $I$ è la matrice identità.

\paragraph{Il Metodo di Cramer}
La teoria dell'Algebra Lineare propone come metodo risolutivo il metodo di Cramer:
$$ x_{i} = \frac{\det(A_{i})}{\det(A)}, \quad i\in\{1,\ldots,n\} $$
dove $A_{i}$ denota la matrice ottenuta da $A$ sostituendo la colonna $i$-esima con il vettore $b$. I determinanti possono essere calcolati utilizzando la regola di Laplace:
$$ \det(A) = \sum_{j=1}^{n} (-1)^{j+1} a_{1,j} \det(A_{1,j}) $$
dove $A_{1,j}$ è la matrice di ordine $n-1$ ottenuta da $A$ eliminando la prima riga e la $j$-esima colonna.

\paragraph{Costo Computazionale e Necessità dei Metodi Numerici}
Sfortunatamente il metodo di Cramer ha un costo computazionale molto elevato, pari a circa $[(n+1)(n-1)n]!$ operazioni elementari (flops). Ciò si traduce in tempi effettivi per la computazione inaccettabili.
Supponendo per esempio di poter effettuare $10^{6}$ flops al secondo, per risolvere un sistema di 20 equazioni occorrono circa $3 \cdot 10^{7}$ anni (300.000 secoli).
Anche i metodi classici per il calcolo dell'inversa $A^{-1}$ hanno un costo computazionale di tipo fattoriale. Poiché nei problemi reali i sistemi possono essere costituiti da centinaia o migliaia di equazioni, occorrono metodi numerici efficienti che tengano conto della dimensione e delle proprietà della matrice.

\vspace{10pt}

\paragraph{Matrice Densa}
\begin{bxthm}
\begin{defn}
Una matrice $A \in \mathbb{R}^{n \times n}$ si dice \textbf{densa} se la maggior parte dei suoi elementi è non nulla.    
\end{defn}
\end{bxthm}

\vspace{10pt}

\paragraph{Matrice Sparsa}
\begin{bxthm}
\begin{defn}
Una matrice $A \in \mathbb{R}^{n \times n}$ si dice \textbf{sparsa} se possiede un numero di elementi non nulli dell'ordine di $n$. Per memorizzare una matrice sparsa è possibile utilizzare solo tre vettori: uno per gli elementi non nulli, uno per gli indici di riga e uno per gli indici di colonna.    
\end{defn}
\end{bxthm}

\vspace{10pt}

\paragraph{Matrici Strutturate}
\begin{bxthm}
\begin{defn}
Una matrice $A \in \mathbb{R}^{n \times n}$ si dice \textbf{strutturata} se i suoi elementi sono disposti secondo una regola nota.
\begin{itemize}
\item \textbf{Matrice Diagonale:} $a_{i,j} = 0$ per $i \neq j$.
$$
\begin{pmatrix}
a_{1,1} & 0 & \dots & 0 \\
0 & a_{2,2} & \dots & \vdots \\
\vdots & \vdots & \ddots & 0 \\
0 & \dots & 0 & a_{n,n}
\end{pmatrix}
$$

\item \textbf{Matrice Triangolare Inferiore:} $a_{i,j} = 0$ per $i < j$.
$$
\begin{pmatrix}
a_{1,1} & 0 & \dots & 0 \\
a_{2,1} & a_{2,2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
a_{n,1} & a_{n,2} & \dots & a_{n,n}
\end{pmatrix}
$$

\item \textbf{Matrice Triangolare Superiore:} $a_{i,j} = 0$ per $i > j$.
$$
\begin{pmatrix}
a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
0 & a_{2,2} & \dots & a_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & a_{n,n}
\end{pmatrix}
$$

\item \textbf{Matrice Tridiagonale:} elementi non nulli solo sulla diagonale principale, sulla sovra-diagonale e sulla sotto-diagonale.
$$
\begin{pmatrix}
a_{1,1} & a_{1,2} & 0 & \dots & 0 \\
a_{2,1} & a_{2,2} & a_{2,3} & \ddots & \vdots \\
0 & \ddots & \ddots & \ddots & 0 \\
\vdots & \ddots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\
0 & \dots & 0 & a_{n,n-1} & a_{n,n}
\end{pmatrix}
$$


\item \textbf{Hessenberg Inferiore:} $a_{i,j} = 0$ per $j > i+1$ (elementi nulli sopra la prima sovra-diagonale).
$$
\begin{pmatrix}
a_{1,1} & a_{1,2} & 0 & \dots & 0 \\
a_{2,1} & a_{2,2} & a_{2,3} & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
a_{n-1,1} & a_{n-1,2} & \dots & a_{n-1,n-1} & a_{n-1,n} \\
a_{n,1} & a_{n,2} & \dots & a_{n,n-1} & a_{n,n}
\end{pmatrix}
$$

\item \textbf{Hessenberg Superiore:} $a_{i,j} = 0$ per $i > j+1$ (elementi nulli sotto la prima sotto-diagonale).
$$
\begin{pmatrix}
a_{1,1} & a_{1,2} & \dots & a_{1,n-1} & a_{1,n} \\
a_{2,1} & a_{2,2} & \dots & a_{2,n-1} & a_{2,n} \\
0 & a_{3,2} & \ddots & \vdots & \vdots \\
\vdots & \ddots & \ddots & a_{n-1,n-1} & a_{n-1,n} \\
0 & \dots & 0 & a_{n,n-1} & a_{n,n}
\end{pmatrix}
$$
\end{itemize}
\end{defn}
\end{bxthm}

\vspace{10pt}

\paragraph{Matrice Simmetrica}
\begin{bxthm}
\begin{defn}
Una matrice $A$ si dice \textbf{simmetrica} se coincide con la sua trasposta, cioè $A = {}^tA$.
\end{defn}
\end{bxthm}

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function isSymm}]
function isSym = isSymm(A, tol)
%ISSYMM Verifica se una matrice è simmetrica con una data tolleranza.
%  isSym = ISSYMM(A) restituisce true se A è simmetrica (entro 1e-10).
%  isSym = ISSYMM(A, tol) permette di specificare la tolleranza.
%
%  Input:
%    A   - Matrice numerica quadrata (double)
%    tol - (Opzionale) Soglia di tolleranza. Default: 1e-10
%
%  Output:
%    isSym - Logical true (1) se simmetrica, false (0) altrimenti

    %% 1. Validazione degli Input
    arguments
        A (:,:) double {mustBeNumeric}
        tol (1,1) double {mustBeNonnegative} = 1e-10
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni della matrice
    [r, c] = size(A);

    % Verifico che la matrice sia quadrata
    if r ~= c
        error('isSymm:DimError', 'La matrice deve essere quadrata (Input size: %dx%d).', r, c);
    end

    %% 3. Verifica Simmetria
    % Calcoliamo la trasposta una sola volta
    AT = A'; 
    
    % Confrontiamo vettorialmente usando la tolleranza
    isSym = all( abs(A(:) - AT(:)) <= tol );

end
\end{lstlisting}

\vspace{10pt}

\paragraph{Matrice Definita Positiva}
\begin{bxthm}
\begin{defn}
Una matrice simmetrica $A$ si dice \textbf{definita positiva} se
$$\forall\,\mathbf{y} \in \mathbb{R}^{n}\; (\mathbf{y} \neq \mathbf{0}),\quad{}^t\mathbf{y}A\mathbf{y} > 0.$$
\end{defn}
\end{bxthm}

\vspace{10pt}

\paragraph{Criterio di Sylvester}
\begin{bxthm}
\begin{prop}
Una matrice simmetrica $A \in \mathbb{R}^{n \times n}$ è definita positiva se e solo se:
$$ \det(A_{k}) > 0, \quad k\in\{1,\ldots,n\} $$
dove $\det(A_{k})$ è il determinante della sottomatrice principale di testa di ordine $k$.
\end{prop}
\end{bxthm}

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function isDefPos}]
function isPositive = isDefPos(A)
% ISDEFPOS Determina se una matrice è definita positiva.
%  isPositive = ISDEFPOS(A) restituisce true se la matrice A è definita
%  positiva utilizzando il criterio di Sylvester (tutti i minori principali
%  di testa devono avere determinante strettamente positivo).
%
%  Input:
%    A - Matrice quadrata numerica (reale e simmetrica).
%
%  Output:
%    isPositive - Logical true (1) se simmetrica, false (0) altrimenti.

    %% 1 - Validazione degli Input
    arguments
        A (:,:) {mustBeNumeric, mustBeReal}
    end

    %% 2 - Controllo Dimensioni e Simmetria
    % Estraggo le dimensioni della matrice
    [r,c] = size(A);

    % Verifico che la matrice sia quadrata
    if r ~= c
        error('isDefPos:NonSquareMatrix', 'La matrice di input deve essere quadrata (%dx%d).', rows, cols);
    end

    % Verifica della simmetria (condizione necessaria per def. pos. reale)
    if ~isSymm(A)
        error('isDefPos:NonSymmetricMatrix', 'La matrice deve essere simmetrica per applicare il criterio.');
    end

    %% 3 - Verifica Definita Positività
    % Inizializzazione
    isPositive = true;

    % Criterio di Sylvester con uscita anticipata
    for k = 1:r
        minorDet = det(A(1:k, 1:k));
        
        if minorDet <= 0
            isPositive = false;
            return; % Interrompe il calcolo se un minore non soddisfa la condizione
        end
    end

end
\end{lstlisting}

\vspace{10pt}

\paragraph{Matrice a Diagonale Dominante}
\begin{bxthm}
\begin{defn}
Una matrice $A$ è detta a \textbf{diagonale dominante per righe} se:
$$ |a_{i,i}| > \sum_{j=1, j \neq i}^{n} |a_{i,j}|, \quad i\in\{1,\ldots,n\} .$$
È detta a \textbf{diagonale dominante per colonne} se:
$$ |a_{j,j}| > \sum_{i=1, i \neq j}^{n} |a_{i,j}|, \quad j\in\{1,\ldots,n\} .$$
\end{defn}
\end{bxthm}

\vspace{10pt}

\paragraph{Esempio di Matrice a Diagonale Dominante per Righe}
Consideriamo la seguente matrice $A$:
$$ A = \begin{bmatrix} 
5 & 1 & 2 \\ 
-2 & 6 & 1 \\ 
1 & 3 & 8 
\end{bmatrix} $$
\vspace{10pt}
Verifichiamo la condizione $|a_{i,i}| > \sum_{j \neq i} |a_{i,j}|$ per ogni riga $i$:
\begin{itemize}
    \item \textbf{Riga 1 ($i=1$):} elemento diagonale $5$. Somma degli altri elementi: $|1| + |2| = 3$.
    $$ 5 > 3 \quad (\text{Vero}) $$
    \item \textbf{Riga 2 ($i=2$):} elemento diagonale $6$. Somma degli altri elementi: $|-2| + |1| = 3$.
    $$ 6 > 3 \quad (\text{Vero}) $$
    \item \textbf{Riga 3 ($i=3$):} elemento diagonale $8$. Somma degli altri elementi: $|1| + |3| = 4$.
    $$ 8 > 4 \quad (\text{Vero}) $$
\end{itemize}
\vspace{10pt}
Poiché la condizione è soddisfatta per tutte le righe, la matrice $A$ è a diagonale dominante per righe.

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function diagDomRow}]
function isDom = diagDomRow(A)
% DIAGDOMROW Verifica se una matrice è a diagonale dominante per righe.
%   isDom = DIAGDOMROW(A) restituisce true se la matrice A è a diagonale
%   dominante per righe.
%
%   Input:
%       A - Matrice quadrata numerica.
%
%   Output:
%       isDom - Logical true (1) se dominante per righe, false (0) altrimenti. 

    %% 1. Validazione Input
    arguments
        A (:,:) {mustBeNumeric}
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni
    [m, n] = size(A);
    if m ~= n
        error('diagDomRow:NonSquareMatrix', 'La matrice deve essere quadrata (Input %dx%d).', m, n);
    end

    %% 3. Verifico Proprietà
    % Calcolo i valori assoluti
    absA = abs(A);

    % Estraggo la diagonale
    d_diag = diag(absA);

    % Calcolo la somma di ogni riga
    sum_rows = sum(absA,2);

    % Calcolo la somma degli elementi extra-diagonali
    sum_off_diag = sum_rows - d_diag;

    % Verifica della condizione 
    isDom = true;
    for i = 1:m
        if d_diag(i) <= sum_off_diag(i)
            isDom = false;
            return; % Interrompe appena trova una riga che viola la condizione
        end
    end

end
\end{lstlisting}

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function diagDomCol}]
function isDom = diagDomCol(A)
% DIAGDOMCOL Verifica se una matrice è a diagonale dominante per colonne.
%   isDom = DIAGDOMCOL(A) restituisce true se la matrice A è a diagonale
%   dominante per colonne.
%
%   Input:
%       A - Matrice quadrata numerica.
%
%   Output:
%       isDom - Logical true (1) se dominante per colonne, false (0) altrimenti. 

    %% 1. Validazione Input
    arguments
        A (:,:) {mustBeNumeric}
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni
    [m, n] = size(A);
    if m ~= n
        error('diagDomCol:NonSquareMatrix', 'La matrice deve essere quadrata (Input %dx%d).', m, n);
    end

    %% 3. Verifico Proprietà
    % Calcolo i valori assoluti
    absA = abs(A);

    % Estraggo la diagonale
    d_diag = diag(absA);

    % Calcolo la somma di ogni colonna
    sum_cols = sum(absA).';

    % Calcolo la somma degli elementi extra-diagonali
    sum_off_diag = sum_cols - d_diag;

    % Verifica della condizione 
    isDom = true;
    for j = 1:n
        if d_diag(j) <= sum_off_diag(j)
            isDom = false;
            return; % Interrompe appena trova una colonna che viola la condizione
        end
    end

end
\end{lstlisting}

\vspace{10pt}



\paragraph{Norme Vettoriali}
\begin{bxthm}
\begin{defn}
Una \textbf{norma vettoriale} è una funzione che associa ad un vettore $x \in \mathbb{R}^{n}$ un numero reale $||x||$ con le seguenti proprietà:
\begin{enumerate}
    \item $||x|| > 0, \forall x \neq 0$ e $||x|| = 0 \iff x = 0$;
    \item $||cx|| = |c| ||x||, \forall c \in \mathbb{R}$;
    \item $||x+y|| \le ||x|| + ||y||, \forall y \in \mathbb{R}^{n}$.
\end{enumerate}
Le norme più utilizzate sono:
\begin{itemize}
    \item \textbf{Norma infinito:} $$||x||_{\infty} = \max_{1 \le i \le n} |x_{i}|;$$
    \item \textbf{Norma 1:} $$||x||_{1} = \sum_{i=1}^{n} |x_{i}|;$$
    \item \textbf{Norma Euclidea (o norma 2):} $$||x||_{2} = \sqrt{\sum_{i=1}^{n} |x_{i}|^{2}} = \sqrt{{}^txx}.$$
\end{itemize}
\end{defn}
\end{bxthm}

\vspace{10pt}

\begin{exmp}
Consideriamo $x = (1, -2, 3)^T$.
\begin{itemize}
    \item $||x||_{\infty} = \max\{1, 2, 3\} = 3$;
    \item $||x||_{1} = 1 + 2 + 3 = 6$;
    \item $||x||_{2} = \sqrt{1 + 4 + 9} = \sqrt{14}$.
\end{itemize}
\end{exmp}

\vspace{10pt}

\paragraph{Norme Matriciali}
\begin{bxthm}
\begin{defn}
Una \textbf{norma matriciale} associa ad una matrice $A$ un numero reale $||A||$ soddisfacendo proprietà analoghe a quelle vettoriali, con l'aggiunta della proprietà di sub-moltiplicatività:
\begin{itemize}
    \item $||AB|| \le ||A|| ||B||, \forall B \in \mathbb{R}^{n \times n}$.
\end{itemize}
Le norme matriciali più utilizzate sono:
\begin{itemize}
    \item \textbf{Norma infinito:} $$||A||_{\infty} = \max_{1 \le i \le n} \sum_{j=1}^{n} |a_{i,j}|$$ (massima somma per righe).
    \item \textbf{Norma 1:} $$||A||_{1} = \max_{1 \le j \le n} \sum_{i=1}^{n} |a_{i,j}|$$ (massima somma per colonne).
    \item \textbf{Norma Spettrale (o norma 2):} $$||A||_{2} = \sqrt{\rho({}^tAA)},$$ dove $\rho(B)$ è il raggio spettrale di $B$ (modulo massimo degli autovalori).
    \item \textbf{Norma di Frobenius:} $$||A||_{F} = \sqrt{\sum_{i=1}^{n}\sum_{j=1}^{n} |a_{i,j}|^{2}}.$$
\end{itemize}
\end{defn}
\end{bxthm}

\vspace{10pt}

\begin{exmp}
Sia $A = \begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 4 \\ 0 & 4 & 3 \end{pmatrix}$.
\begin{itemize}
    \item $||A||_{\infty} = \max(2, 3+4, 4+3) = 7$ (somma righe);
    \item $||A||_{1} = \max(2, 3+4, 4+3) = 7$ (somma colonne);
    \item $||A||_{2}$: Poiché $A$ è simmetrica, $||A||_2 = \max|\lambda(A)|$.
    Gli autovalori sono $\lambda_1=2$ e, dal blocco inferiore, $\lambda_{2,3} = 3 \pm 4 \Rightarrow \{7, -1\}$.
    Quindi $||A||_{2} = \max\{2, 7, |-1|\} = 7$.
    \item $||A||_{F} = \sqrt{2^2 + 3^2 + 4^2 + 0 + 4^2 + 3^2} = \sqrt{54} \approx 7.35$;
\end{itemize}
\end{exmp}

\vspace{10pt}

\paragraph{Norme Compatibili e Indotte}
\begin{bxthm}
\begin{defn}
Una norma vettoriale e una matriciale sono \textbf{compatibili} se $||Ax|| \le ||A|| ||x||$.
Una norma matriciale si dice \textbf{naturale} o \textbf{indotta} dalla norma vettoriale se:
$$ ||A|| = \sup_{x \neq 0} \frac{||Ax||}{||x||} = \sup_{||x||=1} ||Ax|| .$$
Per le norme naturali vale $||I|| = 1$. Le norme matriciali $||\cdot||_{\infty}, ||\cdot||_{1}$ e $||\cdot||_{2}$ sono naturali (indotte dalle rispettive norme vettoriali), mentre la norma di Frobenius non lo è (poiché $||I||_{F} = \sqrt{n}$).    
\end{defn}
\end{bxthm}

\vspace{10pt}

\paragraph{Condizionamento di un sistema lineare}
Vogliamo ora studiare il condizionamento del sistema lineare $Ax=b$. Ci chiediamo come il problema "risponda" a eventuali perturbazioni nei dati iniziali (matrice $A$ e vettore $b$).
Supponiamo di introdurre una perturbazione $\Delta A \in \mathbb{R}^{n \times n}$ sulla matrice $A$ e una perturbazione $\Delta b \in \mathbb{R}^{n}$ sul vettore dei termini noti $b$.
La soluzione del problema perturbato non sarà $x$ ma $y=x+\Delta x$, con $\Delta x \in \mathbb{R}^{n}$. Il sistema perturbato è:
$$ (A+\Delta A)(x+\Delta x) = (b+\Delta b) .$$

\vspace{10pt}

\paragraph{Teorema (Principale)}
\begin{bxthm}
\begin{thm}
Siano $A \in \mathbb{R}^{n \times n}$ una matrice non singolare e $\Delta A \in \mathbb{R}^{n \times n}$ tale che sia soddisfatta la condizione
$$ ||A^{-1}|| ||\Delta A|| \le \frac{1}{2} $$
per una generica norma matriciale indotta $||\cdot||$. Allora anche la matrice $A+\Delta A$ è non singolare e se $x \in \mathbb{R}^{n}$ è soluzione del sistema $Ax=b$ con $b \in \mathbb{R}^{n}$ ($b \neq 0$) e $\Delta x \in \mathbb{R}^{n}$ verifica il sistema perturbato $(A+\Delta A)(x+\Delta x) = (b+\Delta b)$ per $\Delta b \in \mathbb{R}^{n}$, si ha che
$$ \frac{||\Delta x||}{||x||} \le 2K(A) \left( \frac{||\Delta A||}{||A||} + \frac{||\Delta b||}{||b||} \right) $$
ove $K(A) = ||A|| ||A^{-1}||$ è il numero di condizionamento della matrice $A$.    
\end{thm}
\end{bxthm}

\vspace{10pt}

Per dimostrare il precedente teorema ci occorre il seguente risultato:
\paragraph{Teorema (Ausiliario)}
\begin{bxthm}
\begin{thm}
Sia $A$ una matrice quadrata, se $||A|| < 1$ allora la matrice $I+A$ è invertibile e vale la seguente disuguaglianza
$$ ||(I+A)^{-1}|| \le \frac{1}{1-||A||} $$
essendo $||\cdot||$ una norma matriciale indotta tale che $||A|| < 1$.
\end{thm}
\end{bxthm}

\vspace{10pt}

\begin{proof}
Poiché $A+\Delta A = A(I+A^{-1}\Delta A)$ e, per ipotesi,
$$ ||A^{-1}\Delta A|| \le ||A^{-1}|| ||\Delta A|| \le \frac{1}{2} < 1 $$
applicando il teorema ausiliario, si ha che la matrice $I+A^{-1}\Delta A$ e quindi la matrice $A+\Delta A$, è non singolare e risulta
$$ ||(I+A^{-1}\Delta A)^{-1}|| \le \frac{1}{1-||A^{-1}|| ||\Delta A||} \le 2 $$
Sottraendo membro a membro le equazioni $Ax=b$ e $(A+\Delta A)(x+\Delta x) = (b+\Delta b)$ si ottiene
$$ (A+\Delta A)\Delta x = -\Delta A x + \Delta b $$
moltiplicando ambo i membri per $A^{-1}$ si ha
$$ (I+A^{-1}\Delta A)\Delta x = A^{-1}(-\Delta A x + \Delta b) $$
da cui
$$ \Delta x = (I+A^{-1}\Delta A)^{-1}A^{-1}(-\Delta A x + \Delta b) $$
e passando alle norme
$$ ||\Delta x|| \le 2 ||A^{-1}|| (||\Delta A|| ||x|| + ||\Delta b||) $$
Dividendo e moltiplicando per $||A||$ a secondo membro, si ha
$$ ||\Delta x|| \le 2 K(A) \left( \frac{||\Delta A||}{||A||}||x|| + \frac{||\Delta b||}{||A||} \right) $$
Poiché per ipotesi è $b \neq 0$ e $A$ è non singolare, risulta $||x|| > 0$ per cui dividendo ambo i membri per $||x||$ e tenendo conto che $||b|| = ||Ax|| \le ||A|| ||x||$, si ha
$$ \frac{||\Delta x||}{||x||} \le 2 K(A) \left( \frac{||\Delta A||}{||A||} + \frac{||\Delta b||}{||A|| ||x||} \right) \le 2 K(A) \left( \frac{||\Delta A||}{||A||} + \frac{||\Delta b||}{||b||} \right) $$
Dunque, otteniamo
$$ \frac{||\Delta x||}{||x||} \le 2 K(A) \left( \frac{||\Delta A||}{||A||} + \frac{||\Delta b||}{||b||} \right) $$
Dalla stima si evince che l'eventuale amplificazione delle perturbazioni introdotte su matrice dei coefficienti e vettore dei termini noti si deve alla quantità $K(A) = ||A|| ||A^{-1}||$. Tale quantità viene chiamata indice o numero di condizionamento della matrice $A$.
\end{proof}

\vspace{10pt}

\paragraph{Osservazioni sulla stima dell'errore}
Dalla stima ottenuta si evincono alcune proprietà fondamentali:
\begin{itemize}
    \item Per ogni norma matriciale $||\cdot||$ naturale (che soddisfi la proprietà di sub-moltiplicatività), si ha:
    $$ K(A) = ||A|| ||A^{-1}|| \ge ||AA^{-1}|| = ||I|| = 1 $$
    \item Il condizionamento è una caratteristica propria del sistema lineare e non è legato al particolare metodo numerico usato per determinarne la soluzione.
    \item Ponendo $K_{\infty}(A) = ||A||_{\infty} ||A^{-1}||_{\infty}$, dalla stima dell'errore relativo, se:
    $$ \frac{||\Delta x||_{\infty}}{||x||_{\infty}} \le 2 K_{\infty}(A) \text{eps} \le \frac{1}{2} 10^{1-p} $$
    allora $p$ cifre significative della soluzione calcolata mediante un algoritmo stabile sono da ritenere corrette.
\end{itemize}

\vspace{10pt}

\paragraph{Calcolo del Condizionamento in Matlab}
In Matlab esiste una function predefinita per calcolare $K(A)$:
\begin{itemize}
    \item \texttt{cond(A, inf)}: calcola il condizionamento di $A$ in norma infinito ($K_{\infty}(A)$).
    \item \texttt{cond(A, 1)}: calcola il condizionamento di $A$ in norma 1 ($K_{1}(A)$).
    \item \texttt{cond(A)}: calcola il condizionamento di $A$ in norma 2 ($K_{2}(A)$).
\end{itemize}

\vspace{10pt}

\paragraph{Esempi di Matrici Mal Condizionate}
\subparagraph{Matrici di Hilbert}
\begin{exmp}
Sono matrici definite da elementi $h_{i,j} = \frac{1}{i+j-1}$.
La struttura della matrice $H_n$ è la seguente:
$$
H_n =
\begin{pmatrix}
1 & \frac{1}{2} & \frac{1}{3} & \dots & \frac{1}{n} \\
\frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \dots & \frac{1}{n+1} \\
\frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \dots & \frac{1}{n+2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\frac{1}{n} & \frac{1}{n+1} & \frac{1}{n+2} & \dots & \frac{1}{2n-1}
\end{pmatrix}
$$
Tale matrice risulta essere mal condizionata anche per piccoli valori di $n$. Ad esempio, per $n=5$ si ha:
$$ \text{cond}(H_5) \simeq 10^5 $$    
\end{exmp}
\subparagraph{Matrici di Vandermonde}
\begin{exmp}
Fissato un vettore $[y_1, \dots, y_n]$, con $y_i \neq y_j$ per $i \neq j$, sono matrici non singolari definite da:
$$
V_n =
\begin{pmatrix}
1 & 1 & 1 & \dots & 1 \\
y_1 & y_2 & y_3 & \dots & y_n \\
y_1^2 & y_2^2 & y_3^2 & \dots & y_n^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_1^{n-1} & y_2^{n-1} & y_3^{n-1} & \dots & y_n^{n-1}
\end{pmatrix}
$$
Anche in questo caso l'indice di condizionamento cresce esponenzialmente rispetto alla dimensione della matrice.    
\end{exmp}

\vspace{10pt}

\paragraph{Classificazione dei Metodi Numerici}
I metodi numerici si suddividono in:
\begin{itemize}
    \item \textbf{Metodi diretti:} In assenza di errori di arrotondamento calcolano la soluzione esatta in un numero finito di passi.
    \item \textbf{Metodi iterativi:} Generano una successione infinita di vettori che converge alla soluzione.
\end{itemize}
La scelta dipende da stabilità, occupazione di memoria e costo computazionale.

\vspace{10pt}

\paragraph{Metodi Diretti e Sparsezza}
I metodi diretti trasformano il problema in problemi equivalenti. Sono efficienti per matrici dense. Tuttavia, per matrici sparse, i metodi diretti causano il fenomeno del \textit{fill-in} (riempimento): il numero di elementi non nulli cresce durante il procedimento, potendo saturare la memoria. In questi casi sono preferibili i metodi iterativi, che lasciano inalterata la matrice $A$.

\vspace{20pt}

\subsection{Sistemi Diagonali e Triangolari}

\vspace{10pt}

\paragraph{Metodi Diretti}
Si chiamano metodi diretti quei metodi numerici che risolvono sistemi lineari in un numero finito di passi. In altri termini, supponendo di effettuare i calcoli in precisione infinita, tali metodi forniscono la soluzione esatta del sistema mediante un numero finito di operazioni, noto a priori.
In precisione di calcolo finita invece, se il metodo proposto è stabile, esso fornisce una soluzione approssimata con la precisione massima ottenibile, cioè in accordo con il condizionamento della matrice dei coefficienti.

\vspace{10pt}

\paragraph{Sistemi Diagonali}
Il più banale dei metodi diretti si ottiene nel caso delle matrici diagonali.
Se denotiamo con $x={}^t(x_{1}, x_{2}, \dots, x_{n})$ e $b={}^t(b_{1}, b_{2}, \dots, b_{n})$ rispettivamente il vettore delle incognite e quello dei termini noti, e se la matrice dei coefficienti è del tipo:
$$
D = \begin{pmatrix}
a_{1,1} & 0 & 0 & \dots & 0 \\
0 & a_{2,2} & 0 & \dots & 0 \\
\vdots & \vdots & \ddots & \dots & \vdots \\
0 & 0 & 0 & \dots & a_{n,n}
\end{pmatrix}
$$
il sistema $Dx=b$, nell'ipotesi che $D$ sia non singolare (cioè $a_{i,i} \neq 0, \forall i\in\{1,\ldots,n\}$), si risolve banalmente mediante le $n$ divisioni:
$$ x_{i} = \frac{b_{i}}{a_{i,i}}, \quad i\in\{1,\ldots,n\} $$
con un costo computazionale complessivo di $n$ operazioni di divisione.
L'algoritmo è ben posto se la matrice è non singolare. Inoltre, poiché l'algoritmo è composto da $n$ divisioni indipendenti l'una dall'altra, esso è banalmente stabile.

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function diagSub}]
function x = diagSub(D, b, tol)
% DIAGSUB Risolve il sistema lineare diagonale Dx = b. 
%  X = DIAGSUB(D, B, TOL) restituisce il vettore soluzione X.
%
%  Input:
%    D - Matrice quadrata diagonale (n x n)
%    b - Vettore dei termini noti (n x 1)
%    tol (opzionale) - Tolleranza per verifica della singolarità (1 x 1)
%
%  Output:
%    x - Vettore soluzione (n x 1)

    %% 1. Validazione degli Input
    arguments
        D (:,:) double {mustBeNumeric}
        b (:,1) double {mustBeNumeric}
        tol (1,1) double {mustBeNumeric} = 1e-10
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni
    [n,m] = size(D);
    
    % Verifico che D sia quadrata
    if n ~= m
        error('diagSub:NonSquareMatrix', 'La matrice D deve essere quadrata (Input %dx%d).', n, m);
    end

    % Verifico che b sia compatibile con D
    if m ~= length(b)
        error('diagSub:DimensionMismatch', 'Le dimensioni di D e b non corrispondono (Input (%dx%d)x(%dx1)).',n,m,length(b));
    end
    
    %% 3. Controllo Singolarità
    if any(abs(diag(D)) < tol)
        error('diagSub:SingularMatrix', 'La matrice è singolare.');
    end

    %% 4. Sostituzione in Avanti
    % Pre-allocazione
    x = zeros(n, 1);

    % Ciclo principale
    for i = 1 : n 
        x(i) = b(i) / D(i,i);
    end

end
\end{lstlisting}

\vspace{10pt}

\paragraph{Sistemi Triangolari}
Consideriamo ora il caso dei sistemi con matrice dei coefficienti triangolare.
Denotiamo con $L$ una generica matrice triangolare inferiore ("lower triangular"):
$$
L = \begin{pmatrix}
l_{1,1} & 0 & 0 & \dots & 0 \\
l_{2,1} & l_{2,2} & 0 & \dots & 0 \\
\vdots & \vdots & \ddots & \dots & \vdots \\
l_{n-1,1} & l_{n-1,2} & \dots & l_{n-1,n-1} & 0 \\
l_{n,1} & l_{n,2} & \dots & l_{n,n-1} & l_{n,n}
\end{pmatrix}
$$
e con $U$ una generica matrice triangolare superiore ("upper triangular"):
$$
U = \begin{pmatrix}
u_{1,1} & u_{1,2} & \dots & u_{1,n-1} & u_{1,n} \\
0 & u_{2,2} & \dots & u_{2,n-1} & u_{2,n} \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \dots & u_{n-1,n-1} & u_{n-1,n} \\
0 & 0 & \dots & 0 & u_{n,n}
\end{pmatrix}
$$
Vogliamo fornire degli algoritmi per la risoluzione dei sistemi $L\mathbf{x}=\mathbf{b}$ e $U\mathbf{x}=\mathbf{b}$.
Una matrice triangolare è non singolare se e solo se i suoi elementi diagonali sono tutti diversi da zero. Infatti:
$$ \det(L) = \prod_{i=1}^{n}l_{ii} \neq 0 \;\iff\; l_{i,i} \neq 0,\quad \forall i\in\{1,\ldots,n\} ;$$
$$ \det(U) = \prod_{i=1}^{n}u_{ii} \neq 0 \;\iff\; u_{i,i} \neq 0,\quad \forall i\in\{1,\ldots,n\} .$$
Gli algoritmi per la risoluzione di sistemi con matrice dei coefficienti triangolare sono essenzialmente basati sulla tecnica di sostituzione.

\vspace{10pt}

\subparagraph{Algoritmo di Sostituzione in Avanti (Forward Substitution)}
Esaminiamo il caso della matrice triangolare inferiore $L\mathbf{x}=\mathbf{b}$. Esplicitamente il sistema può scriversi come:
$$
\begin{cases}
l_{1,1}x_{1} = b_{1} \\
l_{2,1}x_{1} + l_{2,2}x_{2} = b_{2} \\
l_{3,1}x_{1} + l_{3,2}x_{2} + l_{3,3}x_{3} = b_{3} \\
\vdots \\
l_{n,1}x_{1} + l_{n,2}x_{2} + l_{n,3}x_{3} + \dots + l_{n,n}x_{n} = b_{n}
\end{cases}
$$
Si osserva che dalla prima equazione si ricava immediatamente $x_{1}$. Tale valore può essere sostituito nella seconda equazione per ricavare $x_{2}$, e così via.
$$ x_{1} = \frac{b_{1}}{l_{1,1}} $$
$$ x_{2} = \frac{b_{2} - l_{2,1}x_{1}}{l_{2,2}} $$
$$ x_{3} = \frac{b_{3} - l_{3,1}x_{1} - l_{3,2}x_{2}}{l_{3,3}} $$
$$ \dots $$
$$ x_{i} = \frac{b_{i} - \sum_{k=1}^{i-1} l_{i,k}x_{k}}{l_{i,i}}, \quad i\in\{2,\dots,n\} $$
L'algoritmo è ben posto se la matrice $L$ è non singolare.

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
x(1) = b(1) / L(1,1)                  % 1 flop (divisione)
for i = 2 : n
    x(i) = b(i)
    for k = 1 : i-1
        x(i) = x(i) - L(i,k) * x(k)   % 2 flops (1 mult, 1 sub)
    end
    x(i) = x(i) / L(i,i)              % 1 flop (divisione)
end
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo Computazionale}
$$ \sum_{i=1}^{n} i \simeq \frac{n(n+1)}{2} \simeq \frac{n^{2}}{2} \text{ operazioni (flops)} $$
È possibile provare che l'algoritmo è stabile, cioè non produce amplificazione dell'errore di arrotondamento.

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function forwardSub}]
function x = forwardSub(L, b, tol)
% FORWARDSUB Risolve il sistema lineare triangolare inferiore Lx = b usando la sostituzione in avanti. 
%  x = FORWARDSUB(L, b) restituisce il vettore soluzione x.
%
%  Input:
%    L - Matrice quadrata triangolare inferiore (n x n)
%    b - Vettore dei termini noti (n x 1)
%    tol (opzionale) - Tolleranza per verifica della singolarità (1 x 1)
%
%  Output:
%    x - Vettore soluzione (n x 1)

    %% 1. Validazione degli Input
    arguments
        L (:,:) double {mustBeNumeric}
        b (:,1) double {mustBeNumeric}
        tol (1,1) double {mustBeNumeric} = 1e-10
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni
    [n,m] = size(L);
    
    % Verifico che L sia quadrata
    if n ~= m
        error('forwardSub:NonSquareMatrix', 'La matrice L deve essere quadrata (Input %dx%d).', n, m);
    end

    % Verifico che b sia compatibile con L
    if m ~= length(b)
        error('forwardSub:DimensionMismatch', 'Le dimensioni di L e b non corrispondono (Input (%dx%d)x(%dx1)).',n,m,length(b));
    end
    
    %% 3. Controllo Singolarità
    if any(abs(diag(L)) < tol)
        error('forwardSub:SingularMatrix', 'La matrice è singolare.');
    end

    %% 4. Sostituzione in Avanti
    % Pre-allocazione
    x = zeros(n, 1);

    % Primo elemento
    x(1) = b(1) / L(1,1);

    % Ciclo principale
    for i = 2 : n 
        x(i) = b(i);
        % Ciclo interno: accumulo la somma parziale
        for k = 1:i-1
            x(i) = x(i) - L(i,k) * x(k);
        end
        % Divisione finale per l'elemento diagonale
        x(i) = x(i) / L(i,i); 
    end

end
\end{lstlisting}

\vspace{10pt}

\paragraph{Calcolo dell'Inversa di una Matrice Triangolare Inferiore}
Il metodo di sostituzione in avanti può essere adattato per il calcolo esplicito dell'inversa di una matrice triangolare inferiore $L$.
I vettori colonna $\mathbf{v}_{i}$ dell'inversa $V=(\mathbf{v}_{1}, \dots, \mathbf{v}_{n})$ di $L$ soddisfano i sistemi lineari:
$$ L\mathbf{v}_{i} = \mathbf{e}_{i}, \quad i\in\{1,\ldots,n\} $$
dove $\mathbf{e}_{i}$ è l'$i$-esima colonna della matrice identità.
Poiché l'inversa di una matrice triangolare inferiore è anch'essa triangolare inferiore, le colonne di $V$ hanno tutti gli elementi sovradiagonali nulli. Sfruttando questa proprietà, per ogni colonna $j$ le uniche componenti incognite sono quelle da $j$ a $n$.
Il calcolo delle componenti non nulle $v_{i,j}$ (con $i \ge j$) avviene risolvendo i sistemi con la sostituzione in avanti.

\vspace{10pt}

\subparagraph{Algoritmo di Inversione}
$$ v_{j,j} = \frac{1}{l_{j,j}} , \quad \text{per } j\in\{1,\dots,n\},$$
$$ v_{i,j} = -\frac{1}{l_{i,i}} \sum_{k=j}^{i-1} l_{i,k}v_{k,j}, \quad \text{per } i\in\{j+1, \dots, n\} .$$

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
for j = 1 : n
    v(j,j) = 1 / L(j,j)
    for i = j+1 : n
        v(i,j) = 0
        for k = j : i-1
            v(i,j) = v(i,j) - L(i,k) * v(k,j)
        end
        v(i,j) = v(i,j) / L(i,i)
    end
end
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo Computazionale}
$$ \sum_{j=1}^{n} \sum_{i=j}^{n} (i-j+1) \simeq \frac{n^{3}}{6} \text{ operazioni} .$$

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function invL}]
function V = invL(L, tol)
% INVL calcola l'inversa di una matrice triangolare inferiore ottimizzando
% la sostituzione in avanti con una certa tolleranza
%  V = INVL(L, tol) restituisce l'inversa della matrice L.
%
%  Input:
%    L - Matrice triangolare inferiore numerica (n x n)
%    tol (opzionale) - Tolleranza per la verifica della singolarità (1 x 1)
%    
%  Output:
%    V - Matrice triangolare inferiore numerica inversa di L (n x n)

    %% 1. Validazione Input
    arguments
        L (:,:) double {mustBeNumeric}
        tol (1,1) double {mustBeNumeric} = 1e-10
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni
    [n,m] = size(L);

    % Verifico che L sia quadrata
    if n ~= m
        error('invL:NonSquareMatrix', 'La matrice L deve essere quadrata (Input %dx%d).', n, m);
    end

    % Verifico che L sia non singolare
    if any(abs(diag(L)) < tol)
        error('invL:SingularMatrix', 'La matrice L è singolare.');
    end
    
        %% 3. Calcolo Inversa
    % Pre-allocazione (V sarà triangolare inferiore, il resto rimane 0)
    V = zeros(n);

    for j = 1:n
        % 1. Diagonale: inverso del pivot
        V(j,j) = 1 / L(j,j);
        
        % 2. Sostituzione in avanti per la colonna j (righe i > j)
        for i = j+1:n    
            V(i,j) = 0;
            % Inizializzazione dell'accumulatore per la sommatoria
            for k = j:i-1
                V(i,j) = V(i,j) - L(i,k) * V(k,j);
            end
            
            % Divisione finale per il coefficiente della diagonale
            V(i,j) = V(i,j) / L(i,i);
        end
    end
    
end
\end{lstlisting}

\vspace{10pt}

\subparagraph{Algoritmo di Sostituzione all'Indietro (Backward Substitution)}
Si vuole risolvere il sistema $U\mathbf{x}=\mathbf{b}$ dove $U$ è triangolare superiore:
$$
\begin{cases}
u_{1,1}x_{1} + u_{1,2}x_{2} + \dots + u_{1,n}x_{n} = b_{1} \\
u_{2,2}x_{2} + \dots + u_{2,n}x_{n} = b_{2} \\
\vdots \\
u_{n-1,n-1}x_{n-1} + u_{n-1,n}x_{n} = b_{n-1} \\
u_{n,n}x_{n} = b_{n}
\end{cases}
$$
Procedendo dall'ultima equazione verso la prima:
$$ x_{n} = \frac{b_{n}}{u_{n,n}} $$
$$ x_{n-1} = \frac{b_{n-1} - u_{n-1,n}x_{n}}{u_{n-1,n-1}} $$
$$ \dots $$
$$ x_{i} = \frac{b_{i} - \sum_{k=i+1}^{n} u_{i,k}x_{k}}{u_{i,i}}, \quad i\in\{n-1, \dots, 1\} $$
Anche in questo caso l'algoritmo è ben posto se $U$ è non singolare.

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
x(n) = b(n) / U(n,n)                  % 1 flop (divisione)
for i = n-1 : -1 : 1
    x(i) = b(i)
    for k = i+1 : n
        x(i) = x(i) - U(i,k) * x(k)   % 2 flops (1 mult, 1 sub)
    end
    x(i) = x(i) / U(i,i)              % 1 flop (divisione)
end
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo Computazionale}
$$ \sum_{i=1}^{n} (n-i+1) = \frac{n(n+1)}{2} \simeq \frac{n^{2}}{2} \text{ operazioni} .$$
Anche l'algoritmo di sostituzione all'indietro è stabile.

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function backwardSub}]
function x = backwardSub(U, b, tol)
% BACKWARDSUB Risolve il sistema lineare triangolare superiore Ux = b usando la sostituzione all'indietro. 
%  x = BACKWARDSUB(U, b, tol) restituisce il vettore soluzione x.
%
%  Input:
%    U - Matrice quadrata triangolare superiore (n x n)
%    b - Vettore dei termini noti (n x 1)
%    tol (opzionale) - Tolleranza per verifica della singolarità (1 x 1)
%    
%  Output:
%    x - Vettore soluzione (n x 1)

    %% 1. Validazione degli Input
    arguments
        U (:,:) double {mustBeNumeric}
        b (:,1) double {mustBeNumeric}
        tol (1,1) double {mustBeNumeric} = 1e-10
    end

    %% 2. Controllo Dimensioni
    % Ricaviamo le dimensioni
    [n, m] = size(U);
    
    % Verifichiamo che U sia quadrata
    if n ~= m
        error('backwardSub:NonSquareMatrix', 'La matrice U deve essere quadrata (Input %dx%d).', n, m);
    end

    % Verifichiamo che b sia compatibile con U
    if m ~= length(b)
        error('backwardSub:DimensionMismatch', 'Le dimensioni di U e b non corrispondono (Input (%dx%d)x(%dx1)).', n, m, length(b));
    end
    
    %% 3. Controllo Singolarità
    if any(abs(diag(U)) < tol)
        error('backwardSub:SingularMatrix', 'La matrice è singolare.');
    end

    %% 4. Sostituzione all'indietro
    % Pre-allocazione
    x = zeros(n, 1);

    % Ultimo elemento
    x(n) = b(n) / U(n,n);
    
    % Ciclo principale
    for i = n-1 : -1 : 1
        x(i) = b(i);
        % Ciclo interno: accumulo la somma parziale
        for k = i+1:n
            x(i) = x(i) - U(i,k) * x(k);
        end
        % Divisione finale per l'elemento diagonale
        x(i) = x(i) / U(i,i);
    end

end
\end{lstlisting}

\vspace{10pt}

\paragraph{Calcolo dell'Inversa di una Matrice Triangolare Superiore}
Il metodo di sostituzione all'indietro può essere adattato per il calcolo esplicito dell'inversa di una matrice triangolare superiore $U$.
I vettori colonna $\mathbf{v}_{i}$ dell'inversa $V=(\mathbf{v}_{1}, \dots, \mathbf{v}_{n})$ di $U$ soddisfano i sistemi lineari:
$$ U\mathbf{v}_{i} = \mathbf{e}_{i}, \quad i\in\{1,\ldots,n\} $$
dove $\mathbf{e}_{i}$ è l'$i$-esima colonna della matrice identità.
Poiché l'inversa di una matrice triangolare superiore è anch'essa triangolare superiore, le colonne di $V$ hanno tutti gli elementi subdiagonali nulli. Sfruttando questa proprietà, indichiamo con $\mathbf{v}_{j}' = {}^t(v_{1,j}, \dots, v_{j,j})$ il vettore di dimensione $j$ tale che:
$$ U_{j}\mathbf{v}_{j}' = \mathbf{e}_{j}', \quad j\in\{1,\dots,n\}$$
essendo $U_{j}$ la sottomatrice principale di testa $U$ di ordine $j$ e $\mathbf{e}_{j}'$ il vettore di $\mathbb{R}^{j}$ con tutte le componenti nulle fuorché l'ultima che è 1.
I sistemi sono triangolari superiori di ordine $j$ e si risolvono con la sostituzione all'indietro.

\vspace{10pt}

\subparagraph{Algoritmo di Inversione}
$$ v_{j,j} = \frac{1}{u_{j,j}} , \quad \text{per } j\in\{1,\dots,n\},$$
$$ v_{i,j} = -\frac{1}{u_{i,i}} \sum_{k=i+1}^{j} u_{i,k}v_{k,j}, \quad \text{per } i\in\{j-1, \dots, 1\} .$$

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
for j = 1 : n
    v(j,j) = 1 / U(j,j)
    for i = j-1 : -1 : 1
        v(i,j) = 0
        for k = i+1 : j
            v(i,j) = v(i,j) - U(i,k) * v(k,j)
        end
        v(i,j) = v(i,j) / U(i,i)
    end
end
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo Computazionale}
$$ \sum_{j=1}^{n} \sum_{i=1}^{j} (j-i+1) \simeq \frac{n^{3}}{6} \text{ operazioni} .$$

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function invU}]
function V = invU(U, tol)
% INVU calcola l'inversa di una matrice triangolare superiore ottimizzando
% la sostituzione all'indietro con una certa tolleranza
%  V = INVU(U, tol) restituisce l'inversa della matrice U.
%
%  Input:
%    U - Matrice triangolare superiore numerica (n x n)
%    tol (opzionale) - Tolleranza per la verifica della singolarità (1 x 1)
%    
%  Output:
%    V - Matrice triangolare superiore numerica inversa di U (n x n)

    %% 1. Validazione Input
    arguments
        U (:,:) double {mustBeNumeric}
        tol (1,1) double {mustBeNumeric} = 1e-10
    end

    %% 2. Controllo Dimensioni
    % Estraggo le dimensioni
    [n,m] = size(U);

    % Verifico che U sia quadrata
    if n ~= m
        error('invU:NonSquareMatrix', 'La matrice U deve essere quadrata (Input %dx%d).', n, m);
    end

    % Verifico che U sia non singolare
    if any(abs(diag(U)) < tol)
        error('invU:SingularMatrix', 'La matrice U è singolare.');
    end

    %% 3. Calcolo Inversa
    % Pre-allocazione
    V = zeros(n);

    % Costruzione dell'inversa V
    for j = 1:n
        % Calcolo dell'elemento diagonale (Pivot)
        V(j,j) = 1 / U(j,j);
        for i = j-1:-1:1
            % Inizializzazione dell'accumulatore per la sommatoria
            V(i,j) = 0;
            for k = i+1:j
                V(i,j) = V(i,j) - U(i,k) * V(k,j);
            end
            % Divisione finale per il coefficiente della diagonale
            V(i,j) = V(i,j) / U(i,i);
        end
    end
    
end
\end{lstlisting}

\vspace{20pt}

\subsection{Metodo di Eliminazione di Gauss}

\vspace{10pt}

\paragraph{Metodo di eliminazione di Gauss}
Consideriamo il caso di una generica matrice di ordine $n$. Sia dunque $A \in \mathbb{R}^{n \times n}$ e come al solito $x, b \in \mathbb{R}^{n}$. Vogliamo risolvere il generico sistema $Ax=b$.
Il metodo di Gauss è basato sulla tecnica di eliminazione delle incognite da fissate equazioni. Obiettivo del metodo è ridurre, attraverso un numero finito di passi, il sistema di partenza ad uno ad esso equivalente che abbia la matrice dei coefficienti triangolare.

\paragraph{Descrizione del metodo (caso $n=4$)}
Per semplicità, incominciamo con il descrivere il metodo nel caso di un sistema di dimensione $n=4$.
$$
\begin{cases}
a_{1,1}x_{1} + a_{1,2}x_{2} + a_{1,3}x_{3} + a_{1,4}x_{4} = b_{1} \\
a_{2,1}x_{1} + a_{2,2}x_{2} + a_{2,3}x_{3} + a_{2,4}x_{4} = b_{2} \\
a_{3,1}x_{1} + a_{3,2}x_{2} + a_{3,3}x_{3} + a_{3,4}x_{4} = b_{3} \\
a_{4,1}x_{1} + a_{4,2}x_{2} + a_{4,3}x_{3} + a_{4,4}x_{4} = b_{4}
\end{cases}
$$
Assumiamo che $a_{1,1} \neq 0$ e consideriamo la quantità $m_{2,1} = -\frac{a_{2,1}}{a_{1,1}}$.
Moltiplicando ambo i membri della prima equazione per $m_{2,1}$ e sommando alla seconda equazione, si ottiene:
$$ (-a_{2,1} + a_{2,1})x_{1} + (a_{2,2} - \frac{a_{2,1}}{a_{1,1}}a_{1,2})x_{2} + \dots + (a_{2,4} - \frac{a_{2,1}}{a_{1,1}}a_{1,4})x_{4} = (b_{2} - \frac{a_{2,1}}{a_{1,1}}b_{1}) $$
cioè
$$ a_{2,2}^{(2)}x_{2} + a_{2,3}^{(2)}x_{3} + a_{2,4}^{(2)}x_{4} = b_{2}^{(2)} $$
dunque
$$ a_{2,j}^{(2)} = a_{2,j} + m_{2,1}a_{1,j}, \quad j=2,3,4 $$
Ripetendo il procedimento per la terza e quarta equazione con i moltiplicatori $m_{3,1} = -\frac{a_{3,1}}{a_{1,1}}$ e $m_{4,1} = -\frac{a_{4,1}}{a_{1,1}}$, otteniamo il sistema equivalente al passo 2:
$$
\begin{cases}
a_{1,1}x_{1} + a_{1,2}x_{2} + a_{1,3}x_{3} + a_{1,4}x_{4} = b_{1} \\
a_{2,2}^{(2)}x_{2} + a_{2,3}^{(2)}x_{3} + a_{2,4}^{(2)}x_{4} = b_{2}^{(2)} \\
a_{3,2}^{(2)}x_{2} + a_{3,3}^{(2)}x_{3} + a_{3,4}^{(2)}x_{4} = b_{3}^{(2)} \\
a_{4,2}^{(2)}x_{2} + a_{4,3}^{(2)}x_{3} + a_{4,4}^{(2)}x_{4} = b_{4}^{(2)}
\end{cases}
$$
dove in generale $a_{i,j}^{(2)} = a_{i,j} + m_{i,1}a_{1,j}$ e $b_{i}^{(2)} = b_{i} + m_{i,1}b_{1}$.

Assumendo ora che $a_{2,2}^{(2)} \neq 0$, eliminiamo l'incognita $x_{2}$ dalla terza e quarta equazione usando i moltiplicatori $m_{3,2} = -\frac{a_{3,2}^{(2)}}{a_{2,2}^{(2)}}$ e $m_{4,2} = -\frac{a_{4,2}^{(2)}}{a_{2,2}^{(2)}}$.
Si ottiene il sistema al passo 3:
$$
\begin{cases}
a_{1,1}x_{1} + a_{1,2}x_{2} + a_{1,3}x_{3} + a_{1,4}x_{4} = b_{1} \\
a_{2,2}^{(2)}x_{2} + a_{2,3}^{(2)}x_{3} + a_{2,4}^{(2)}x_{4} = b_{2}^{(2)} \\
a_{3,3}^{(3)}x_{3} + a_{3,4}^{(3)}x_{4} = b_{3}^{(3)} \\
a_{4,3}^{(3)}x_{3} + a_{4,4}^{(3)}x_{4} = b_{4}^{(3)}
\end{cases}
$$
Infine, assumendo $a_{3,3}^{(3)} \neq 0$, eliminiamo $x_{3}$ dall'ultima equazione con $m_{4,3} = -\frac{a_{4,3}^{(3)}}{a_{3,3}^{(3)}}$, ottenendo il sistema triangolare finale:
$$
\begin{cases}
a_{1,1}x_{1} + a_{1,2}x_{2} + a_{1,3}x_{3} + a_{1,4}x_{4} = b_{1} \\
a_{2,2}^{(2)}x_{2} + a_{2,3}^{(2)}x_{3} + a_{2,4}^{(2)}x_{4} = b_{2}^{(2)} \\
a_{3,3}^{(3)}x_{3} + a_{3,4}^{(3)}x_{4} = b_{3}^{(3)} \\
a_{4,4}^{(4)}x_{4} = b_{4}^{(4)}
\end{cases}
$$

\paragraph{Generalizzazione al caso di ordine n}
Assumiamo che sia $a_{1,1} \neq 0$. Possiamo eliminare l'incognita $x_{1}$ dalla $2^a, \dots, n$-esima equazione sommando all'$i$-esima equazione ($i=2,\dots,n$) la prima equazione moltiplicata per $m_{i,1} = -\frac{a_{i,1}}{a_{1,1}}$.
Le formule di aggiornamento generali al passo $k$ (per eliminare l'incognita $x_{k}$ dalle equazioni successive) sono:
$$ m_{i,k} = -\frac{a_{i,k}^{(k)}}{a_{k,k}^{(k)}}, \quad i\in\{k+1,\dots,n\} $$
$$ a_{i,j}^{(k+1)} = a_{i,j}^{(k)} + m_{i,k}a_{k,j}^{(k)}, \quad j\in\{k+1,\dots,n\} $$
$$ b_{i}^{(k+1)} = b_{i}^{(k)} + m_{i,k}b_{k}^{(k)} $$
Gli elementi $a_{1,1}, a_{2,2}^{(2)}, a_{3,3}^{(3)}, \dots$ che compaiono al denominatore vengono detti \textit{elementi pivot}, mentre le quantità $m_{i,k}$ sono dette \textbf{moltiplicatori}.
Dopo $n-1$ passi, se tutti i pivot sono non nulli, si ottiene un sistema triangolare superiore risolvibile con la sostituzione all'indietro.

\vspace{10pt}

\begin{exmp}
Consideriamo il sistema:
$$
\begin{cases}
2x_{1} - x_{2} + x_{3} - 2x_{4} = 0 \\
2x_{2} - x_{4} = 1 \\
x_{1} - 2x_{3} + x_{4} = 0 \\
2x_{2} + x_{3} + x_{4} = 4
\end{cases}.
$$
\begin{enumerate}
\item \textbf{Passo 1:} $m_{2,1}=0, m_{3,1}=-1/2, m_{4,1}=0$. Il sistema diventa:
$$
\begin{cases}
2x_{1} - x_{2} + x_{3} - 2x_{4} = 0 \\
2x_{2} - x_{4} = 1 \\
\frac{1}{2}x_{2} - \frac{5}{2}x_{3} + 2x_{4} = 0 \\
2x_{2} + x_{3} + x_{4} = 4
\end{cases}
$$
\item \textbf{Passo 2:} $m_{3,2}=-1/4, m_{4,2}=-1$. Il sistema diventa:
$$
\begin{cases}
2x_{1} - x_{2} + x_{3} - 2x_{4} = 0 \\
2x_{2} - x_{4} = 1 \\
-\frac{5}{4}x_{3} + \frac{9}{4}x_{4} = -\frac{1}{4} \\
x_{3} + 2x_{4} = 3
\end{cases}
$$
\item \textbf{Passo 3:} $m_{4,3} = \frac{4}{5}$. Il sistema finale è:
$$
\begin{cases}
2x_{1} - x_{2} + x_{3} - 2x_{4} = 0 \\
2x_{2} - x_{4} = 1 \\
-\frac{5}{4}x_{3} + \frac{9}{4}x_{4} = -\frac{1}{4} \\
\frac{38}{10}x_{4} = \frac{28}{10} \implies x_{4}=1 \dots
\end{cases}
$$
\end{enumerate}
Sostituendo all'indietro si ottiene la soluzione $x = (1, 1, 1, 1)^T$.    
\end{exmp}

\vspace{10pt}

\paragraph{Algoritmo}
L'algoritmo formale per $k\in\{1, 2, \dots, n-1\}$ è :
$$ m_{i,k} = -\frac{a_{i,k}^{(k)}}{a_{k,k}^{(k)}}, \quad i\in\{k+1,\dots,n\} ;$$
$$ a_{i,j}^{(k+1)} = a_{i,j}^{(k)} + m_{i,k}a_{k,j}^{(k)}, \quad j\in\{k+1,\dots,n\} ;$$
$$ b_{i}^{(k+1)} = b_{i}^{(k)} + m_{i,k}b_{k}^{(k)}, \quad i\in\{k+1,\dots,n\} .$$
L'algoritmo è ben posto se e solo se tutti gli elementi pivot sono diversi da zero.

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
for k = 1 : n-1
    for i = k+1 : n
        mol = -a(i,k) / a(k,k)      % 1 operazione
        for j = k+1 : n
            a(i,j) = a(i,j) + mol * a(k,j)  % n-k operazioni
        end
        b(i) = b(i) + mol * b(k)    % 2 operazione
    end
end
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo Computazionale}
$$ \sum_{k=1}^{n-1} [2(n-k) + (n-k)^2] \simeq \frac{n^3}{3} \text{ operazioni} $$

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function gaussElim}]
function [Ag,bg] = gaussElim(A,b)
% GAUSSELIM Applica il metodo dell'eliminazione di Gauss alla matrice dei coefficienti e al vettore dei termini noti.
%  [Ag,bg] = gaussElim(A,b)  restituisce A e b risultanti dall'eliminazione.
%    
%  Input:
%    A - Matrice dei coefficienti (n x n)
%    b - Vettore dei termini noti (n x 1)
%    
%  Output:
%    Ag - Matrice dei coefficienti post-GE (n x n)
%    bg - Vettore dei termini noti post-GE (n x 1)
 
    %% 1. Validazione degli Input
    arguments
        A (:,:) double {mustBeNumeric}
        b (:,1) double {mustBeNumeric}
    end
    
    %% 2. Verifica Dimensioni
    [n, m] = size(A);
    
    % Verifico se la matrice è quadrata
    if n ~= m
        error('gaussElim:NonSquareMatrix', 'La matrice non è quadrata (Size %dx%d).', n, m);
    end
    
    % Verifico se il prodotto Ab è compatibile
    if n ~= length(b)
        error('gaussElim:NonCompatible', 'Il prodotto non è compatibile (Size (%dx%d)*(%dx1)).', n, m, length(b));
    end
    
    
    %% 3. Eliminazione di Gauss
    for k = 1 : n-1
        % Controllo pivot nullo (sicurezza)
        if A(k,k) == 0
             error('gaussElim:ZeroPivot', 'Pivot nullo alla riga %d.', k);
        end
        
        % Eliminazione
        for i = k+1 : n
            mol = -A(i,k) / A(k,k);
            for j = k+1:n
                A(i,j) = A(i,j) + mol * A(k,j);
            end
            b(i) = b(i) + mol * b(k);
        end
    end
    
    % Nota: Gli elementi A(i,k) sotto la diagonale non vengono azzerati fisicamente
    % per risparmiare tempo, ma sono logicamente considerati zero.
    
    Ag = A;
    bg = b;
end
\end{lstlisting}

\paragraph{Esistenza dei Pivot}
\begin{bxthm}
\begin{thm}
Sia $A \in \mathbb{R}^{n \times n}$. Gli elementi pivot $a_{k,k}^{(k)}$ sono tutti diversi da zero se e soltanto se tutte le matrici principali di testa di $A$, denotate con $A_k = (a_{i,j})_{i,j\in\{1,\dots,k\}}$, sono non singolari, cioè:
$$ \det(A_k) \neq 0 \quad \forall k\in\{1,\dots,n\} .$$    
\end{thm}
\end{bxthm}

\vspace{10pt}

\begin{note}
Poiché questa condizione è difficile da verificare, esistono famiglie di matrici per cui è garantita a priori:
\begin{itemize}
    \item Matrici a \textbf{diagonale dominante per righe o per colonne}.;
    \item Matrici \textbf{simmetriche definite positive} (${}^tA=A$ e ${}^t\mathbf{y}A\mathbf{y} > 0, \forall \mathbf{y} \neq \mathbf{0}$).
\end{itemize}
\end{note}

\vspace{20pt}

\subsection{Strategia Pivoting e Fattorizzazione LU}

\vspace{10pt}

\paragraph{Necessità del Pivoting}
Il metodo di Gauss descritto in precedenza si arresta se, a un certo passo $k$, l'elemento pivot $a_{k,k}^{(k)}$ risulta nullo. Tuttavia, se la matrice è non singolare, deve esistere almeno un elemento $a_{i,k}^{(k)} \neq 0$ con $i > k$ sulla stessa colonna. Per proseguire è sufficiente scambiare la riga $k$-esima con la riga $i$-esima.
Anche quando il pivot non è nullo ma è molto piccolo in valore assoluto rispetto agli altri elementi, possono verificarsi errori di arrotondamento disastrosi (amplificazione degli errori o cancellazione numerica). Per garantire la stabilità numerica, è necessario permutare le righe in modo da scegliere un pivot "ottimale". Tale strategia è detta \textit{pivoting}.

\vspace{10pt}

\paragraph{Strategie di Pivoting}
\begin{itemize}
    \item \textbf{Pivoting Parziale:} Al passo $k$, si sceglie come pivot l'elemento di modulo massimo nella colonna $k$-esima, al di sotto o sulla diagonale principale. Si cerca l'indice $r \ge k$ tale che:
    $$ |a_{r,k}^{(k)}| = \max_{k \le i \le n} |a_{i,k}^{(k)}| $$
    Se $r \neq k$, si scambia la riga $k$ con la riga $r$.
    \item \textbf{Pivoting Totale:} Si cerca l'elemento massimo su tutta la sottomatrice attiva. Si scelgono indici $r, s \ge k$ tali che:
    $$ |a_{r,s}^{(k)}| = \max_{k \le i, j \le n} |a_{i,j}^{(k)}| $$
    Si scambiano le righe $k$ ed $r$ e le colonne $k$ ed $s$ (quest'ultimo scambio comporta il riordinamento delle incognite).
\end{itemize}

\vspace{10pt}

Il pivoting parziale è la strategia più utilizzata perché meno costosa computazionalmente ($O(n^2)$ confronti) e generalmente sufficiente a garantire stabilità.

\vspace{10pt}

\paragraph{Stabilità senza Pivoting}
Il metodo di eliminazione di Gauss senza pivoting è stabile a priori solo per alcune classi di matrici:
\begin{itemize}
    \item Matrici a \textbf{diagonale dominante per righe o per colonne};
    \item Matrici \textbf{simmetriche} e \textbf{definite positive}.
\end{itemize}

\vspace{10pt}

\paragraph{Esempio di Instabilità}
\begin{exmp}
Consideriamo un sistema di ordine $n=18$ con matrice definita da $a_{i,j} = \cos((j-1)\frac{2i-1}{2n}\pi)$ e termine noto somma degli elementi della riga (soluzione esatta $x_i=1$). La matrice è ben condizionata ($K \approx 16.9$).
Risolvendo con Gauss senza pivoting, si ottengono errori significativi (es. $x_{15} \approx 1.0000000002...$).
Risolvendo con Gauss e pivoting parziale, si ottiene la soluzione corretta con massima precisione (es. $x_{15} \approx 1.000000000000000$).
\begin{center}
\begin{footnotesize}
\begin{tabular}{|c|c|}
\hline
\textbf{Gauss (senza pivoting)} & \textbf{Gauss + Pivoting Parziale} \\
\hline
9.999999968425205e-001 & 9.999999999999993e-001 \\
1.000000005848771e+000 & 1.000000000000000e+000 \\
9.999999953310822e-001 & 1.000000000000001e+000 \\
1.000000003291504e+000 & 1.000000000000001e+000 \\
9.999999977773912e-001 & 9.999999999999997e-001 \\
1.000000001703849e+000 & 1.000000000000000e+000 \\
9.999999983501708e-001 & 1.000000000000000e+000 \\
1.000000001767299e+000 & 1.000000000000001e+000 \\
9.999999982508074e-001 & 9.999999999999998e-001 \\
1.000000001433063e+000 & 9.999999999999999e-001 \\
9.999999991425489e-001 & 9.999999999999993e-001 \\
1.000000000200525e+000 & 1.000000000000000e+000 \\
1.000000000347843e+000 & 9.999999999999996e-001 \\
9.999999993163441e-001 & 9.999999999999990e-001 \\
1.000000000800448e+000 & 9.999999999999996e-001 \\
9.999999992564082e-001 & 1.000000000000000e+000 \\
1.000000000564552e+000 & 1.000000000000000e+000 \\
9.999999996959291e-001 & 9.999999999999999e-001 \\
\hline
\end{tabular}
\end{footnotesize}
\end{center}
\end{exmp}

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
for k = 1 : n-1
    % Ricerca del pivot massimo nella colonna k-esima
    trovo p tale che: |a(p,k)| = max(|a(i,k)|) per i >= k
    
    if k ~= p
        scambio righe k-esima e p-esima di A
        scambio righe k-esima e p-esima di b
    end
    
    % Eliminazione standard di Gauss
    for i = k+1 : n
        mol = -a(i,k) / a(k,k)
        for j = k+1 : n
            a(i,j) = a(i,j) + mol * a(k,j)
        end
        b(i) = b(i) + mol * b(k)
    end
end
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo Computazionale}
Il costo computazionale totale dell'algoritmo di Gauss con pivoting è dato dalla somma delle operazioni aritmetiche (flops) necessarie per l'eliminazione e dei confronti necessari per la ricerca del pivot massimo.
\begin{itemize}
    \item \textbf{Operazioni Aritmetiche:} $\approx \frac{n^{3}}{3}$ (invariato rispetto a Gauss senza pivoting).
    \item \textbf{Confronti:} Ad ogni passo $k$ si effettuano $n-k$ confronti. Il totale è:
    $$ \sum_{k=1}^{n-1} (n-k) = \frac{n(n-1)}{2} \approx \frac{n^{2}}{2} \text{ confronti} $$
\end{itemize}
Poiché $n^2$ è trascurabile rispetto a $n^3$ per $n$ grandi, il costo asintotico rimane dominato da $n^3/3$.

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function gaussElimPP}]
function [Ag,bg] = gaussElimPP(A,b)
% GAUSSELIMPP Applica il metodo dell'eliminazione di Gauss alla matrice dei
%  coefficienti e al vettore dei termini noti implementando il pivoting parziale.
%  [Ag,bg] = gaussElimPP(A,b)  restituisce A e b risultanti dall'eliminazione con pivoting parziale.
%    
%  Input:
%    A - Matrice dei coefficienti (n x n)
%    b - Vettore dei termini noti (n x 1)
%    
%  Output:
%    Ag - Matrice dei coefficienti post-GE (n x n)
%    bg - Vettore dei termini noti post-GE (n x 1)
    
    %% 1. Validazione degli Input
    arguments
        A (:,:) double {mustBeNumeric}
        b (:,1) double {mustBeNumeric}
    end
    
    %% 2. Verifica Dimensioni
    [n, m] = size(A);
    
    % Verifico se la matrice è quadrata
    if n ~= m
        error('gaussElimPP:NonSquareMatrix', 'La matrice non è quadrata (Size %dx%d).', n, m);
    end
    
    % Verifico se il prodotto Ab è compatibile
    if n ~= length(b)
        error('gaussElimPP:NonCompatible', 'Il prodotto non è compatibile (Size (%dx%d)*(%dx1)).', n, m, length(b));
    end
    
    
    %% 3. Eliminazione di Gauss
    for k = 1 : n-1        
        % Pivoting parziale
        % Trovo il massimo nella colonna k (dal pivot in giù)
        [~, r_rel] = max(abs(A(k:n, k)));
            
        % Converto in indice globale
        r_glob = r_rel + k - 1;
            
        % Se il pivot migliore non è quello attuale, scambio
        if r_glob ~= k
            A([k, r_glob], :) = A([r_glob, k], :);
            b([k, r_glob])    = b([r_glob, k]);
        end
        
        % Controllo pivot nullo (sicurezza)
        if A(k,k) == 0
             error('gaussElimPP:ZeroPivot', 'Pivot nullo alla riga %d.', k);
        end
        
        % Eliminazione di Gauss
        for i = k+1 : n
            mol = -A(i,k) / A(k,k);
            
            for j = k+1:n
                A(i,j) = A(i,j) + mol * A(k,j);
            end
            
            % Aggiorno il termine noto
            b(i) = b(i) + mol * b(k);
        end
    end
    
    % Nota: Gli elementi A(i,k) sotto la diagonale non vengono azzerati fisicamente
    % per risparmiare tempo, ma sono logicamente considerati zero.

    Ag = A;
    bg = b;
end
\end{lstlisting}

\vspace{10pt}

\paragraph{La Fattorizzazione LU}
Il metodo di eliminazione di Gauss, dal punto di vista matriciale, può essere riletto come la costruzione di una successione di matrici:
$$ [A|\mathbf{b}] = [A^{(1)}|\mathbf{b}^{(1)}], \dots, [A^{(k)}|\mathbf{b}^{(k)}], \dots, [A^{(n)}|\mathbf{b}^{(n)}] $$
in modo tale che $A^{(n)}$ sia triangolare superiore e $\mathbf{b}^{(n)}$ sia il nuovo termine noto.
Le matrici della successione sono tra loro legate da una trasformazione del tipo:
$$ [A^{(k+1)}|\mathbf{b}^{(k+1)}] = M^{(k)}[A^{(k)}|\mathbf{b}^{(k)}], \quad k\in\{1,\dots,n-1\} $$
dove $M^{(k)}$ è detta \textbf{matrice elementare di Gauss} ed è definita come:
$$
M^{(k)} =
\begin{pmatrix}
1 & \dots & 0 & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & & \vdots \\
0 & \dots & 1 & 0 & \dots & 0 \\
0 & \dots & m_{k+1,k} & 1 & \dots & 0 \\
\vdots & & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & m_{n,k} & 0 & \dots & 1
\end{pmatrix}
$$
Si ha quindi che:
$$ A = [M^{(1)}]^{-1}A^{(2)} = [M^{(1)}]^{-1}[M^{(2)}]^{-1}A^{(3)} = \dots = [M^{(1)}]^{-1} \dots [M^{(n-1)}]^{-1}A^{(n)} $$
e analogamente per il termine noto:
$$ \mathbf{b} = [M^{(1)}]^{-1} \dots [M^{(n-1)}]^{-1}\mathbf{b}^{(n)} $$
Ponendo:
$$ L = [M^{(1)}]^{-1} \dots [M^{(n-1)}]^{-1}, \quad U = A^{(n)}, \quad \mathbf{y} = \mathbf{b}^{(n)}, $$
si ottiene la fattorizzazione:
$$ A = LU \quad \text{e} \quad \mathbf{b} = L\mathbf{y} $$
dove $U$ è la matrice triangolare superiore che si ottiene alla fine del metodo di eliminazione di Gauss e $L$ è una matrice triangolare inferiore.
La struttura di $L$ è la seguente:
$$
L =
\begin{pmatrix}
1 & 0 & \dots & & \dots & 0 \\
-m_{2,1} & 1 & 0 & & & \vdots \\
\vdots & \ddots & \ddots & \ddots & & \\
 & \ddots & 1 & & & \vdots \\
\vdots & & -m_{k+1,k} & 1 & \ddots & \\
 & & -m_{k+2,k} & \ddots & \ddots & \\
\vdots & & \vdots & & \ddots & 1 & 0 \\
-m_{n,1} & \dots & -m_{n,k} & \dots & \dots & -m_{n,n-1} & 1
\end{pmatrix}
$$
Per costruirla basta, ad ogni passo del metodo di Gauss, memorizzare i moltiplicatori cambiati di segno.

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
for k = 1 : n-1
    for i = k+1 : n
        A(i,k) = -A(i,k) / A(k,k)      % Calcolo moltiplicatori
        for j = k+1 : n
            A(i,j) = A(i,j) + A(i,k) * A(k,j)
        end
    end
end
L = eye(n) - tril(A, -1)   % Parte triangolare inferiore con 1 sulla diagonale
U = triu(A)                % Parte triangolare superiore
\end{verbatim}

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title={Function factorLU}]
function [L, U] = factorLU(A)
% FACTORLU Esegue la fattorizzazione LU su una matrice A.
%  [L, U] = factorLU(A) fattorizza la matrice A in L * U usando 
%  l'eliminazione di Gauss senza pivoting.
%    
%  Input:
%    A - Matrice dei coefficienti (n x n)
%
%  Output:
%    L - Matrice triangolare inferiore unitaria (n x n)
%    U - Matrice triangolare superiore (n x n)

    %% 1. Validazione degli Input
    arguments
        A (:,:) double {mustBeNumeric}
    end

    %% 2. Verifica Dimensioni
    % Estraggo le dimensioni
    [n, m] = size(A);

    % Verifico se la matrice è quadrata
    if n ~= m
        error('factorLU:NonSquareMatrix', 'La matrice dei coefficienti deve essere quadrata (Size: %dx%d).', n, m);
    end

    %% 3. Eliminazione di Gauss
    for k = 1 : n-1
        % Controllo pivot nullo
        if A(k,k) == 0
             error('factorLU:ZeroPivot', 'Pivot nullo incontrato alla riga %d.', k);
        end
        
        % Ciclo sulle righe sottostanti
        for i = k+1 : n
            % Calcolo e assegnazione del moltiplicatore
            A(i,k) = -A(i,k) / A(k,k);
            
            % Aggiornamento della riga i-esima (Eliminazione)
            for j = k+1:n
                A(i,j) = A(i,j) + A(i,k) * A(k,j);
            end
        end
    end
    
    % Costruzione di L e U
    L = eye(n) - tril(A, -1);
    U = triu(A);
end
\end{lstlisting}

\vspace{10pt}

\subparagraph{Risoluzione di Sistemi con $LU$}
Una volta fattorizzata $A=LU$, il sistema $A\mathbf{x}=\mathbf{b}$ diventa $LU\mathbf{x}=\mathbf{b}$. 
Si risolve in due passi:
\begin{enumerate}
    \item Risoluzione del sistema triangolare inferiore $L\mathbf{y} = \mathbf{b}$ (sostituzione in avanti).
    \item Risoluzione del sistema triangolare superiore $U\mathbf{x} = \mathbf{y}$ (sostituzione all'indietro).
\end{enumerate}

\vspace{10pt}

\subparagraph{Costo Computazionale}
Costo totale: $\frac{n^3}{3}$ (fattorizzazione) + $n^2$ (soluzione sistemi).

\vspace{10pt}

\subparagraph{Applicazioni della Fattorizzazione LU}
\begin{enumerate}
    \item \textbf{Calcolo del Determinante di $A$}\\
    Il determinante di una matrice $A$ può essere calcolato utilizzando la formula di Binet. Avendo la fattorizzazione $A=LU$, si ha:
    $$ \det(A) = \det(LU) = \det(L)\det(U) $$
    Poiché $L$ è triangolare inferiore con tutti 1 sulla diagonale, $\det(L)=1$. Poiché $U$ è triangolare superiore, il suo determinante è il prodotto degli elementi diagonali. Dunque:
    $$ \det(A) = \det(U) = \prod_{i=1}^{n} u_{i,i} $$
    Il costo computazionale complessivo è $\frac{n^{3}}{3} + n$ (dove $\frac{n^3}{3}$ è il costo della fattorizzazione).
    \item \textbf{Risoluzione di $p$ sistemi con la stessa matrice dei coefficienti}\\
    Supponiamo di dover risolvere i sistemi:
$$ A\mathbf{x}_{1}=\mathbf{b}_{1}, \quad A\mathbf{x}_{2}=\mathbf{b}_{2}, \quad \dots, \quad A\mathbf{x}_{p}=\mathbf{b}_{p} $$
o in altri termini il sistema matriciale:
$$ AX=B $$
con $A \in \mathbb{R}^{n \times n}$ e $X, B \in \mathbb{R}^{n \times p}$.
Calcolate le matrici $L$ e $U$ una sola volta, ogni sistema $A\mathbf{x}_{i}=\mathbf{b}_{i}$ viene risolto risolvendo i due sistemi triangolari:
$$
\begin{cases}
L\mathbf{y} = \mathbf{b}_{i} \\
U\mathbf{x}_{i} = \mathbf{y}
\end{cases} \quad i\in\{1,\dots,p\}
$$
mediante gli algoritmi di sostituzione.
Il costo computazionale complessivo è:
$$ \frac{n^{3}}{3} + pn^{2} $$
Se invece si risolvesse ciascun sistema indipendentemente dagli altri con il metodo di Gauss, il costo sarebbe molto più alto: $p(\frac{n^{3}}{3} + \frac{n^{2}}{2})$.
    \item \textbf{Calcolo dell'inversa di $A$}\\
    Se $p=n$ e $B=I$ (matrice identità), risolvere il sistema $AX=B$ è equivalente a calcolare l'inversa $A^{-1}$.
Si risolvono quindi i sistemi:
$$
\begin{cases}
L\mathbf{y} = \mathbf{e}_{i} \\
U\mathbf{x}_{i} = \mathbf{y}
\end{cases}, \quad i\in\{1,\ldots,n\}
$$
dove i vettori $\mathbf{e}_{i}$ rappresentano le colonne della matrice $I$ (vettori della base canonica).
Il costo computazionale complessivo "grezzo" sarebbe $\frac{n^{3}}{3} + n^{3}$.
Tuttavia, sfruttando opportunamente la natura dei vettori $\mathbf{e}_{i}$ (che contengono molti zeri), il costo si riduce a circa $n^{3}$.
Nel dettaglio:
\begin{itemize}
    \item Risolvere il sistema matriciale $LY=I$ ha un costo di $\frac{n^{3}}{6}$.
    \item Risolvere il sistema matriciale $UX=Y$ ha un costo di $\frac{n^{3}}{2}$.
\end{itemize}
Sommando il costo della fattorizzazione ($\frac{n^3}{3}$), il costo totale asintotico è $n^3$.
\end{enumerate}

\vspace{10pt}

\paragraph{Il metodo di Gauss con la variante del pivoting}
Il metodo di Gauss con la variante del pivoting esegue ancora una fattorizzazione di matrice nei seguenti termini:
$$ PA = \hat{L}U \quad \text{e} \quad Pb = \hat{L}\mathbf{y} $$
dove $P \in \mathbb{R}^{n \times n}$, detta matrice di permutazione, contiene le informazioni relative agli scambi di righe.
Vale il seguente:

\vspace{10pt}

\begin{bxthm}
\begin{thm}
Per ogni matrice $A \in \mathbb{R}^{n \times n}$ esiste una matrice di permutazione $P \in \mathbb{R}^{n \times n}$ tale che
$$ PA = \hat{L}U .$$    
\end{thm}
\end{bxthm}

\vspace{10pt}

\paragraph{Le matrici di permutazione}
Siano $1 \le r, s \le n$ e sia $P_{r,s}$ la matrice ottenuta da $I$ scambiando la $r$-esima e la $s$-esima riga.
$$
P_{r,s} =
\begin{array}{cl}
% 1. LA MATRICE (Blocco a sinistra)
\left(
\begin{array}{ccccccc}
1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & & \vdots & & \vdots \\
0 & \cdots & 0 & \cdots & 1 & \cdots & 0 \\
\vdots & & \vdots & \ddots & \vdots & & \vdots \\
0 & \cdots & 1 & \cdots & 0 & \cdots & 0 \\
\vdots & & \vdots & & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & \cdots & 0 & \cdots & 1
\end{array}
\right)
&
% 2. LE FRECCE LATERALI (Blocco a destra)
% Usiamo \phantom per mantenere l'altezza delle righe allineata
\begin{array}{l}
\phantom{1} \\
\phantom{\vdots} \\
\leftarrow r \\
\phantom{\vdots} \\
\leftarrow s \\
\phantom{\vdots} \\
\phantom{1}
\end{array}
\\
% 3. LE FRECCE INFERIORI (Blocco in basso)
% Usiamo \phantom per copiare la larghezza esatta delle colonne della matrice
\begin{array}{ccccccc}
\phantom{1} & \phantom{\cdots} & \uparrow & \phantom{\cdots} & \uparrow & \phantom{\cdots} & \phantom{0} \\
 & & r & & s & &
\end{array}
&
\end{array}
$$


\vspace{10pt}

\begin{bxthm}
\begin{prop}
La matrice $P_{r,s}$ è detta \textbf{matrice di permutazione} e gode delle seguenti proprietà:
\begin{enumerate}
    \item $det(P_{r,s}) = -1$;
    \item $P_{r,s} = {}^tP_{r,s}$;
    \item $P_{r,s}^{2} = I$;
    \item $P_{r,s}^{-1} = P_{r,s}$.
\end{enumerate}    
\end{prop}
\end{bxthm}
\begin{proof}
\begin{enumerate}
    \item Banale;
    \item Segue dal fatto che l'elemento di posto $(r, s)$ coincide con quello di posto $(s, r)$, mentre tutti gli altri elementi non nulli giacciono sulla diagonale;
    \item $$ P_{r,s}P_{r,s} = {}^t(\mathbf{e}_1, \dots, \mathbf{e}_{r-1}, \mathbf{e}_s, \mathbf{e}_{r+1}, \dots, \mathbf{e}_{s-1}, \mathbf{e}_r, \mathbf{e}_{s+1}, \dots, \mathbf{e}_n) (\mathbf{e}_1, \dots, \mathbf{e}_s, \dots, \mathbf{e}_r, \dots, \mathbf{e}_n) $$
$$ = \mathbf{e}_1{}^t\mathbf{e}_1 + \dots + \mathbf{e}_{r-1}{}^t\mathbf{e}_{r-1} + \mathbf{e}_s {}^t\mathbf{e}_s + \mathbf{e}_{r+1}{}^t\mathbf{e}_{r+1} + \dots + \mathbf{e}_{s-1}{}^t\mathbf{e}_{s-1} + \mathbf{e}_r {}^t\mathbf{e}_r + \mathbf{e}_{s+1}{}^t\mathbf{e}_{s+1} + \dots + \mathbf{e}_n {}^t\mathbf{e}_n = I $$
dove $\mathbf{e}_i$ è l'$i$-esimo vettore colonna della base canonica;
    \item Segue dalla 3.
\end{enumerate}
\end{proof}

\vspace{10pt}

\begin{rem}
Si vede inoltre che:
\begin{itemize}
    \item il prodotto $P_{r,s}A$ produce lo scambio delle righe $r$-esima ed $s$-esima in $A$.
    \item il prodotto $AP_{r,s}$ produce lo scambio delle colonne $r$-esima ed $s$-esima in $A$.
\end{itemize}    
\end{rem}

\vspace{10pt}

\paragraph{Fattorizzazione LU con pivoting}
I passi della fattorizzazione LU senza pivoting della matrice $A$ possono essere sintetizzati come segue:
\begin{itemize}
    \item Se $a_{1,1} \neq 0$: annullamento elementi prima colonna a partire dalla seconda riga della matrice iniziale, ottenuto dal prodotto $M^{(1)}A$.
    \item Se $a_{2,2}^{(2)} \neq 0$: annullamento elementi della seconda colonna a partire dalla terza riga della matrice attiva, ottenuto dal prodotto $M^{(2)}M^{(1)}A$.
    \item \dots
    \item Se $a_{n-1,n-1}^{(n-1)} \neq 0$: annullamento elementi della $(n-1)$-esima colonna a partire dalla $n$-esima riga della matrice attiva, ottenuto dal prodotto $M^{(n-1)} \dots M^{(2)}M^{(1)}A$.
\end{itemize}
Al termine si ha $M^{(n-1)} \dots M^{(2)}M^{(1)}A = U$ triangolare superiore.
Possiamo descrivere la tecnica del pivoting usando le matrici di permutazione, nel seguente modo:
\begin{itemize}
    \item Sia $|a_{r_1,1}| = \max_{i \ge 1} |a_{i,1}|$. Se $r_1 \neq 1$ si pone $P_1 = P_{1,r_1}$ e si esegue lo scambio della prima riga con la $r_1$-esima con il prodotto $P_1 A$. Se $r_1=1$ si pone $P_1=I$ e non si effettuano scambi.
    Si esegue l'annullamento degli elementi della prima colonna a partire dalla seconda riga della matrice $P_1 A$ con il prodotto $M^{(1)}P_1 A$.
    \item Sia $|a_{r_2,2}^{(2)}| = \max_{i \ge 2} |a_{i,2}^{(2)}|$. Se $r_2 \neq 2$ si pone $P_2 = P_{2,r_2}$ e si esegue lo scambio della seconda riga con la $r_2$-esima della matrice $M^{(1)}P_1 A$ con il prodotto $P_2 M^{(1)}P_1 A$. Se $r_2=2$ si pone $P_2=I$.
    Si esegue l'annullamento degli elementi della seconda colonna a partire dalla terza riga della matrice $P_2 M^{(1)}P_1 A$ con il prodotto $M^{(2)}P_2 M^{(1)}P_1 A$.
    \item \dots
\end{itemize}
Al termine della procedura si ottiene:
$$ M^{(n-1)}P_{n-1} \dots M^{(2)}P_2 M^{(1)}P_1 A = U $$
ovvero
$$ GA = U, \quad G = M^{(n-1)}P_{n-1} \dots M^{(2)}P_2 M^{(1)}P_1 .$$
Vogliamo ora mostrare come dalla $GA=U$ si perviene alla seguente fattorizzazione della matrice $A$:
$$ PA = \hat{L}U $$
ove $P$ è un'opportuna matrice di permutazione, che tiene conto di tutti gli scambi di righe, $\hat{L}$ è una matrice triangolare inferiore con elementi sulla diagonale uguali a 1 e $U$ è la matrice triangolare superiore del metodo di eliminazione di Gauss. Ovviamente, in assenza di scambi $P=I$.
Si possono infatti riordinare le matrici in modo da ottenere:
$$ M^{(n-1)}P_{n-1} \dots M^{(2)}P_2 M^{(1)}P_1 A = (\overline{L}_{n-1} \dots \overline{L}_2 \overline{L}_1)^{-1} (P_{n-1} \dots P_2 P_1) A $$
e, posto $\hat{L}^{-1} = \overline{L}_{n-1} \dots \overline{L}_2 \overline{L}_1$ e $P = P_{n-1} \dots P_2 P_1$, possiamo scrivere:
$$ \hat{L}^{-1}PA = U $$
da cui
$$ PA = \hat{L}U .$$

\vspace{10pt}

\begin{exmp}
Esaminiamo il caso particolare $n=4$. Si ha:
$$ M^{(3)}P_3 M^{(2)}P_2 M^{(1)}P_1 A = M^{(3)}P_3 M^{(2)}P_3 P_3 P_2 M^{(1)}P_2 P_2 P_1 A $$
Posto $L_1^{(1)} = P_2 M^{(1)}P_2$:
$$ = M^{(3)}P_3 M^{(2)}P_3 P_3 L_1^{(1)}P_2 P_1 A $$
Posto $L_2^{(1)} = P_3 M^{(2)}P_3$:
$$ = M^{(3)}L_2^{(1)}P_3 L_1^{(1)}P_3 P_3 P_2 P_1 A $$
Posto $L_1^{(2)} = P_3 L_1^{(1)}P_3$:
$$ = M^{(3)}L_2^{(1)}L_1^{(2)}P_3 P_2 P_1 A $$
\end{exmp}

\vspace{10pt}

In generale (si può provare per induzione):
$$ M^{(n-1)}P_{n-1}M^{(n-2)}P_{n-2} \dots M^{(2)}P_2 M^{(1)}P_1 A = \overline{L}_{n-1} \dots \overline{L}_2 \overline{L}_1 P_{n-1} \dots P_2 P_1 A $$
con
$$ \overline{L}_j =
\begin{cases}
L_j^{(n-1-j)} = (P_{n-1}P_{n-2} \dots P_{j+1}) M^{(j)} (P_{j+1} \dots P_{n-2}P_{n-1}), & j < n-1 \\
M^{(j)}, & j = n-1
\end{cases}
$$

\vspace{10pt}

\begin{rem}
Le matrici $M^{(i)}$ e $\overline{L}_i$ (e quindi anche $L$ e $\hat{L}$) differiscono solo per un diverso ordinamento (nell'ambito della stessa colonna e sotto l'elemento diagonale) dei moltiplicatori $m_{i,j}$, in conseguenza degli scambi di riga effettuati in precedenza.
Al termine del processo di eliminazione al posto della matrice iniziale $A$ avremo:
$$
\begin{pmatrix}
u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
m_{21} & u_{22} & u_{23} & \dots & u_{2n} \\
m_{31} & m_{32} & u_{33} & \dots & u_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
m_{n1} & m_{n2} & m_{n3} & \dots & u_{nn}
\end{pmatrix}
$$
\end{rem}

\vspace{10pt}

\begin{rem}
La costruzione della matrice $P$ viene fatta scambiando, ogni volta che viene fatto uno scambio di righe della matrice $A$, le righe corrispondenti della matrice $I$.
\end{rem}

\vspace{10pt}

\subparagraph{Schema Algoritmo}
\begin{verbatim}
for k = 1:n-1
    trovo p : |a(p,k)| = max(|a(i,k)|) per i >= k
    if k ~= p
        scambio righe k-esima e p-esima di A
        scambio righe k-esima e p-esima di I
    end
    for i = k+1:n
        a(i,k) = a(i,k) / a(k,k)
        for j = k+1:n
            a(i,j) = a(i,j) - a(i,k) * a(k,j)
        end
    end
end
L = eye(n) + tril(A, -1)
U = triu(A)
\end{verbatim}

\vspace{10pt}

\subparagraph{Costo computazionale}
$$ \frac{n^3}{3} \text{ operazioni} + \sum_{k=1}^{n-1} (n-k) \text{ confronti} \simeq \frac{n^3}{3} \text{ operazioni} + \frac{n^2}{2} \text{ confronti} $$

\vspace{10pt}

\begin{lstlisting}[style=matlabCode, title = {Function factorLUPP}]
function [L, U, P, s] = factorLUPP(A)
% FACTORLUPP Esegue la fattorizzazione LU con pivoting parziale.
%  [L, U, P, s] = factorLUPP(A) calcola la fattorizzazione PA = LU e
%  restituisce il numero di scambi effettuati.
%
%    Input:
%      A - Matrice dei coefficienti (n x n)
%
%    Output:
%      L - Matrice triangolare inferiore unitaria (n x n)
%      U - Matrice triangolare superiore (n x n)
%      P - Matrice di permutazione (n x n)
%      s - Numero di scambi effettuati (1 x 1)

    %% 1. Validazione degli Input
    arguments
        A (:,:) double {mustBeNumeric}
    end

    %% 2. Verifica Dimensioni
    % Estraggo le dimensioni
    [n, m] = size(A);

    % Verifico se la matrice è quadrata
    if n ~= m
        error('factorLUPP:NonSquareMatrix', 'La matrice deve essere quadrata (Size: %dx%d).', n, m);
    end

    %% 3. Eliminazione di Gauss con Pivoting Parziale
    % Inizializzo P come matrice identità
    P = eye(n);
    % Inizializzo s
    s = 0;

    for k = 1 : n-1
        % Pivoting parziale
        % Trovo il massimo nella colonna k (dal pivot in giù)
        [~, r_rel] = max(abs(A(k:n, k)));

        % Converto in indice globale
        r_glob = r_rel + k - 1;
            
        % Se il pivot migliore non è quello attuale, scambio
        if r_glob ~= k
            A([k, r_glob], :) = A([r_glob, k], :);
            
            % Scambio le righe della matrice di permutazione P
            P([k, r_glob], :) = P([r_glob, k], :);

            % Aggiorno variabile s
            s = s + 1;
        end
        
        % Controllo pivot nullo (sicurezza)
        if A(k,k) == 0
             error('factorLUPP:ZeroPivot', 'Matrice singolare (pivot nullo alla riga %d).', k);
        end
        
        % Ciclo sulle righe sottostanti
        for i = k+1 : n
            % Calcolo e assegnazione del moltiplicatore
            A(i,k) = -A(i,k) / A(k,k);
            
            % Aggiornamento riga i-esima
            for j = k+1:n
                A(i,j) = A(i,j) + A(i,k) * A(k,j);
            end
        end
    end
    
    %% 4. Costruzione Output
    L = eye(n) - tril(A, -1);
    U = triu(A);
    % P è già stata costruita durante i cicli
end
\end{lstlisting}

\vspace{10pt}

\subparagraph{Risoluzione di sistemi lineari mediante $LU$ con pivoting}
Nota la fattorizzazione del tipo $PA=\hat{L}U$, per determinare la soluzione del sistema lineare $A\mathbf{x}=\mathbf{b}$ si risolvono due sistemi triangolari:
$$
\begin{cases}
\hat{L}\mathbf{y} = P\mathbf{b} \\
U\mathbf{x} = \mathbf{y}
\end{cases}
$$
che deduciamo dalla relazione $PA\mathbf{x}=P\mathbf{b}$, ovvero $\hat{L}U\mathbf{x}=P\mathbf{b}$.

\vspace{10pt}

\subparagraph{Costo computazionale}
L'intera procedura (fattorizzazione $PA=\hat{L}U$ e risoluzione dei due sistemi triangolari) consta di $\frac{n^3}{3} + n^2$ operazioni aritmetiche. 
Ci sono poi $(n-1) + (n-2) + \dots + 1 = \frac{n(n-1)}{2}$ confronti per determinare il massimo pivot in ogni passo.

\vspace{10pt}

\subparagraph{Applicazioni della Fattorizzazione LU}
\begin{enumerate}
    \item \textbf{Calcolo del determinante}\\
    Da $PA = \hat{L}U$, essendo $P^{-1} = P^T$,
    $$ \det(A) = \det(P^T) \det(\hat{L}) \det(U) = \det(P) \det(U) $$
    Poiché
    $$ \det(P) = \det(P_{n-1}) \dots \det(P_2) \det(P_1), $$
    e
    $$
    \det(P_j) =
    \begin{cases}
    1 & \text{se } P_j = I \\
    -1 & \text{altrimenti}
    \end{cases}
    $$
    si ha
    $$ \det(A) = (-1)^s \prod_{i=1}^{n} a_{i,i}^{(i)} $$
    dove $s$ denota il numero complessivo di scambi effettuati.
    \item \textbf{Calcolo dell'inversa di $A$}\\
    Denotate con $\mathbf{x}_1, \dots, \mathbf{x}_n$ le colonne della matrice inversa di $A$, quest'ultima può essere calcolata risolvendo i $2n$ sistemi:
    $$
    \begin{cases}
    \hat{L}\mathbf{y} = P\mathbf{e}_i \\
    U\mathbf{x}_i = \mathbf{y}
    \end{cases}
    \quad i\in\{1,\ldots,n\}
    $$
    il primo triangolare inferiore e il secondo triangolare superiore.
    Il costo computazionale complessivo è $\frac{n^3}{3} + n^3$ ma può essere ridotto a $n^3$ se si tiene conto dalla natura dei vettori $P\mathbf{e}_i$.
\end{enumerate}

\vspace{20pt}

\subsection{Metodo di Cholesky}

\vspace{10pt}

\paragraph{Fattorizzazione LDU}
Sia $A \in \mathbb{R}^{n \times n}$ una matrice per cui esiste la fattorizzazione LU di $A$ senza pivoting.
Questa decomposizione può essere sempre riscritta come:
$$ A = LU = LDU_{1} $$
essendo $L, U_{1}$ entrambe matrici triangolari a diagonale unitaria e $D$ matrice diagonale tale che $(D)_{i,i} = (U)_{i,i}$.
Se $A$ è simmetrica, allora $U_{1} = {}^tL$, e si ha:
$$ A = LD{}^tL $$

\vspace{10pt}

\paragraph{Fattorizzazione $LL^T$}
Se $A \in \mathbb{R}^{n \times n}$ è simmetrica definita positiva, essendo gli elementi di $D$ tutti positivi, si ha:
$$ D = D^{\frac{1}{2}}D^{\frac{1}{2}} $$
dove gli elementi di $(D^{\frac{1}{2}})_{i,i} = \sqrt{D_{i,i}}$.
Da cui segue il teorema:

\vspace{10pt}

\begin{bxthm}
\begin{thm}
Se $A \in \mathbb{R}^{n \times n}$ è una matrice simmetrica definita positiva esiste ed è unica la fattorizzazione:
$$ A = L_{1}{}^tL_{1} \quad \text{con} \quad L_{1} = LD^{\frac{1}{2}} $$
dove $L_{1}$ è una matrice triangolare inferiore con elementi diagonali non nulli.
\end{thm}
\end{bxthm}

\vspace{10pt}

Nel seguito si fa riferimento alla fattorizzazione di cui nel teorema precedente come alla fattorizzazione del tipo $LL^{T}$.
La fattorizzazione $L{}^tL$ può essere ottenuta attraverso la procedura di fattorizzazione LU senza pivoting con un costo pari a $n^{3}/6$, cioè circa la metà della fattorizzazione LU.
Infatti è possibile organizzare l'algoritmo in modo da sfruttare la simmetria di tutte le matrici attive negli $n-1$ passi.
Vedremo ora come sia possibile ottenere la stessa fattorizzazione attraverso una tecnica "compatta" che porta al metodo di Cholesky e che costa circa $n^{3}/6$ operazioni.

\vspace{10pt}

\paragraph{Costruzione del Metodo di Cholesky}
Posto $A=(a_{i,j})_{i,j\in\{1,\dots,n\}}$ e $L=(l_{i,j})_{i,j\in\{1,\dots,n\}}$, si ha:
$$ a_{i,j} = \sum_{k=1}^{n} l_{i,k}l_{k,j}^{T} = \sum_{k=1}^{n} l_{i,k}l_{j,k} $$
Poiché la matrice è simmetrica, possiamo considerare solo gli elementi di $A$ con $j \le i$. Otteniamo:
$$ a_{i,j} = \sum_{k=1}^{j-1} l_{i,k}l_{j,k} + l_{i,j}l_{j,j}, \quad i\in\{1,\ldots,n\}, \quad j\in\{1,\dots,i-1\} $$
$$ a_{i,i} = \sum_{k=1}^{i-1} l_{i,k}^{2} + l_{i,i}^{2}, \quad i\in\{1,\ldots,n\} $$
Da cui si ricavano le formule per gli elementi di $L$:
$$ l_{1,1} = \sqrt{a_{1,1}} $$
$$ l_{i,j} = \frac{1}{l_{j,j}} \left( a_{i,j} - \sum_{k=1}^{j-1} l_{i,k}l_{j,k} \right), \quad i\in\{2,\dots,n\}, \quad j\in\{1,\dots,i-1\} $$
$$ l_{i,i} = \sqrt{a_{i,i} - \sum_{k=1}^{i-1} l_{i,k}^{2}}, \quad i\in\{2,\dots,n\}.$$
È lecito assumere che l'operazione di radice quadrata sia equivalente (in termini di tempo) alle operazioni di prodotto e divisione.

\vspace{10pt}

\paragraph{Schema Algoritmo}
\begin{verbatim}
l(1,1) = sqrt(a(1,1))                  % 1 operazione
for i = 2:n
    for j = 1:i-1
        l(i,j) = 0;
        for k = 1:j-1
            l(i,j) = l(i,j) + l(i,k) * l(j,k)
        end
        l(i,j) = (a(i,j) - l(i,j)) / l(j,j)    % j-1 operazioni + 1 op.
    end
    l(i,i) = 0;
    for k = 1:i-1
        l(i,i) = l(i,i) + l(i,k)^2
    end
    l(i,i) = sqrt(a(i,i) - l(i,i))             % i-1 operazioni + 1 op.
end
\end{verbatim}

\paragraph{Costo Computazionale}
$$ 1 + \sum_{i=2}^{n} \left[ i + \sum_{j=1}^{i-1} j \right] = \sum_{i=1}^{n} i + \sum_{i=1}^{n} \frac{i(i-1)}{2} $$
$$ = \sum_{i=1}^{n} i + \frac{1}{2}\sum_{i=1}^{n} i^{2} - \frac{1}{2}\sum_{i=1}^{n} i = \frac{1}{2}\sum_{i=1}^{n} i + \frac{1}{2}\sum_{i=1}^{n} i^{2} $$
$$ = \frac{n(n+1)}{4} + \frac{n(n+1)(2n+1)}{12} \simeq \frac{n^{3}}{6} $$

\vspace{10pt}

\begin{lstlisting}[style=matlabCode]
function L = cholDec(A)
%CHOLDEC Esegue la fattorizzazione di Cholesky.
%  L = cholDec(A) decompone la matrice A in L*L'
%  La matrice A deve essere Simmetrica Definita Positiva.
%
%    Input:
%      A - Matrice dei coefficienti (n x n)
%
%    Output:
%      L - Matrice fattorizzata (n x n)

    %% 1. Validazione degli Input
    arguments
        A (:,:) double {mustBeNumeric}
    end
    
    %% 2. Verifica Dimensioni
    [n, m] = size(A);

    if n ~= m
        error('cholDec:nonSquareMatrix', 'La matrice dei coefficienti deve essere quadrata (Size: %dx%d).', n, m);
    end
    
    %% 3. Verifica Simmetria
    if ~isequal(A, A')
        error('cholDec:nonSymmMatrix','La matrice dei coefficienti deve essere simmetrica.')
    end

    %% 2. Metodo di Cholesky
    L = zeros(n);
    
    % Primo elemento
    L(1,1) = sqrt(A(1,1));
    
    for i = 2:n
        % Calcolo elementi fuori diagonale
        for j = 1:i-1
            L(i,j) = 0;    % Inizializza accumulatore
            
            for k = 1:j-1
                L(i,j) = L(i,j) + L(i,k) * L(j,k);
            end 
            
            L(i,j) = (A(i,j) - L(i,j)) / L(j,j); 
        end
        
        % Calcolo elemento diagonale
        L(i,i) = 0;    % Inizializza accumulatore
        for k = 1:i-1
            L(i,i) = L(i,i) + L(i,k)^2;
        end
        
        % Radice quadrata finale per la diagonale
        argRad = A(i,i) - L(i,i);
        
        % Controllo di sicurezza
        if argRad <= 0
            error('cholDec:nonDefPosMatrix','La matrice dei coefficienti non è definita positiva (elemento diagonale <= 0).')
        end
        
        L(i,i) = sqrt(argRad);
    end
end
\end{lstlisting}


\vspace{10pt}

\paragraph{Stabilità e Risoluzione}
È possibile provare che l'algoritmo di Cholesky è stabile. Ricordiamo che, peraltro, anche l'algoritmo di Gauss, senza pivoting, è stabile per le matrici simmetriche definite positive.
Molte routine automatiche calcolano $R=L^{T}$ anziché $L$, ma è evidente che l'algoritmo è lo stesso (con un attento uso degli indici), data la simmetria della matrice di partenza $A$. In tali casi scriveremo:
$$ A = R^{T}R $$
Osserviamo infine che se si vuole risolvere il sistema $Ax=b$ mediante la fattorizzazione di Cholesky basterà, una volta computata la fattorizzazione, e quindi calcolata $L$ (o rispettivamente $R$), risolvere i seguenti due sistemi:
$$ Ly = b, \quad L^{T}x = y $$
(oppure $R^{T}y = b, \quad Rx = y$ rispettivamente).

\vspace{10pt}

Di seguito un esempio in Matlab.

\vspace{10pt}

\begin{lstlisting}[style=matlabCode]
format long e;
n = 5; 

% Generazione dati casuali
A = randi([-10,10], n, n);
A = A + A';             % Rendo A simmetrica
A = A * A' + eye(n);    % Rendo A definita positiva
b = randi([-10,10], n, 1);

% 1. Decomposizione Cholesky
L = cholDec(A);

% 2. Risoluzione dei sistemi triangolari
y = forwardSub(L, b);         % Risolve Ly = b
x = backwardSub(L', y);       % Risolve L'x = y

% 3. Verifica dell'errore
sol_ref = A\b;               % Soluzione di riferimento MATLAB
errore = norm(x - sol_ref);

disp("Errore assoluto:");
disp(errore);
\end{lstlisting}
\begin{verbatim}
Errore assoluto:
     2.220446049250313e-16
\end{verbatim}

\paragraph{Le function Matlab}
Chiudiamo questa carrellata dei principali metodi diretti per la risoluzione di un sistema lineare descrivendo le principali function di Matlab che implementano tali metodi.
Sia allora $A$ una matrice non singolare di ordine $n$ e $b$ un vettore colonna di ordine $n$.

\begin{itemize}
    \item \textbf{Risoluzione dei sistemi diagonali:} Se la matrice dei coefficienti $A$ è diagonale, \texttt{A\textbackslash b} risolve il sistema mediante $n$ operazioni di divisione.
    
    \item \textbf{Risoluzione dei sistemi triangolari:} Se la matrice dei coefficienti $A$ è triangolare (superiore o inferiore), \texttt{A\textbackslash b} risolve il sistema mediante algoritmo di sostituzione (all'indietro o in avanti a seconda della struttura della matrice).
    
    \item \textbf{Risoluzione di sistemi con matrice qualsiasi:} \texttt{A\textbackslash b} risolve il sistema mediante metodo di eliminazione di Gauss (in realtà con la fattorizzazione LU) con pivoting e gli algoritmi di sostituzione.
    
    \item \textbf{Risoluzione di sistemi simmetrici def. positivi:} \texttt{A\textbackslash b} risolve il sistema mediante metodo di Cholesky e gli algoritmi di sostituzione.
    
    \item \textbf{Fattorizzazione LU:} Il comando \texttt{[L,U,P] = lu(A)} calcola i fattori $L, U$ e la matrice di permutazione $P$ tali che $PA=LU$.
    
    \item \textbf{Fattorizzazione di Cholesky:} Se la matrice $A$ è simmetrica e definita positiva, il comando \texttt{R = chol(A)} calcola il fattore triangolare superiore $R$ tale che $A=R^{T}R$.
    
    \item \textbf{Calcolo del determinante:} \texttt{det(A)} calcola il determinante della matrice non singolare $A$, utilizzando la fattorizzazione LU.
    
    \item \textbf{Condizionamento:} Il comando \texttt{cond(A, p)}, con \texttt{p = 1, 2, inf, 'fro'}, calcola il numero di condizionamento rispettivamente in norma 1, 2, infinito e Frobenius.
    
    \item \textbf{Inversa di una matrice:} \texttt{inv(A)} calcola l'inversa della matrice utilizzando la fattorizzazione LU.
\end{itemize}

\vspace{20pt}

\subsection{Stabilità dei Metodi di Gauss}

\vspace{10pt}

\paragraph{Backward Error Analysis}
Con la "Backward Error Analysis (BEA)" (Analisi dell'errore all'indietro), si misura la perturbazione nei dati di ingresso che porta al risultato finale, ipotizzando che le operazioni siano eseguite con precisione infinita.
Si valuta in sostanza quanto è perturbato il dato di input considerando "esatto" il risultato in output.
Essa è stata introdotta da Wilkinson nel 1963 con l'obiettivo di studiare gli errori negli algoritmi di Algebra Lineare ed è particolarmente adatta quando il numero di operazioni è elevato.
L'algoritmo è detto \textit{stabile} se la perturbazione nel dato di ingresso è dell'ordine della precisione in cui si lavora.

\paragraph{Backward Error Analysis nella fattorizzazione LU}
\begin{bxthm}
\begin{thm}
Sia $A$ una matrice di ordine $n$ i cui elementi sono numeri di macchina. Siano $L$ ed $U$ le matrici della fattorizzazione ottenute senza la tecnica del pivoting. Esiste allora una matrice $E$ per cui:
$$ (A+E) = LU $$
con $E$ tale che:
$$ ||E|| \le n u ||L|| ||U|| $$
essendo $u = \frac{\text{eps}}{2}$.    
\end{thm}
\end{bxthm}

Poiché $U = A^{(n-1)}$ la stabilità del metodo di eliminazione di Gauss può essere meglio compresa misurando la crescita degli elementi delle matrici $A^{(k)}$ e, quindi, misurando il fattore di crescita.

\paragraph{Fattore di Crescita}
\begin{bxthm}
\begin{defn}
Si definisce fattore di crescita la seguente quantità:
$$ \rho = \frac{\max(\alpha_{1}, \alpha_{2}, \dots, \alpha_{n})}{\alpha_{1}} $$
con:
$$ \alpha_{1} = \max_{1 \le i,j \le n} |a_{i,j}| $$
$$ \alpha_{k} = \max_{1 \le i,j \le n} |a_{i,j}^{(k)}|, \quad k=2,\dots,n $$
essendo $A^{(k)} = (a_{i,j}^{(k)})_{i,j=1,\dots,n}$ la matrice ottenuta al termine del passo $k$-esimo della fattorizzazione LU.    
\end{defn}
\end{bxthm}

La definizione vale sia con pivoting che senza pivoting.
Osserviamo che, sebbene il pivoting faccia in modo che i moltiplicatori siano tutti più piccoli di 1, gli elementi delle matrici $A^{(k)}$ possono ancora crescere arbitrariamente.

\paragraph{BEA nella fattorizzazione LU con pivoting}
\begin{bxthm}
\begin{thm}
Siano $L, U$ le matrici della fattorizzazione con pivoting parziale. Si ha:
$$ P(A+E) = LU $$
con $E$ tale che:
$$ ||E|| \le n^{3} u \rho ||A||_{\infty} $$
essendo $u = \frac{\text{eps}}{2}$.    
\end{thm}
\end{bxthm}

Ci domandiamo: quanto grande può essere $\rho$?

\paragraph{Esempio senza pivoting}
Consideriamo il sistema lineare:
$$ \begin{cases} \varepsilon x_{1} + x_{2} = 1 + \varepsilon \\ x_{1} = 1 \end{cases} $$
Se scegliamo $\varepsilon = 0.5 \cdot 10^{-5}$ si ha:
$$ L = \begin{pmatrix} 1 & 0 \\ 2 \cdot 10^{5} & 1 \end{pmatrix}, \quad U = \begin{pmatrix} 0.5 \cdot 10^{-5} & 1 \\ 0 & -2 \cdot 10^{5} \end{pmatrix} $$
Poiché $||L||_{\infty} = (1 + 2 \cdot 10^{5})$ e $||U||_{\infty} = 2 \cdot 10^{5}$, per il Teorema 1 si ha:
$$ ||E||_{\infty} \le n u ||L||_{\infty} ||U||_{\infty} = \text{eps}(1 + 2 \cdot 10^{5}) 2 \cdot 10^{5} \sim 0.9 \cdot 10^{-5} < 0.5 \cdot 10^{-4} $$
Inoltre il fattore di crescita è:
$$ \rho = \frac{\max(1, 2 \cdot 10^{5})}{1} = 2 \cdot 10^{5} $$

\paragraph{Esempio con pivoting}
Applicando il pivoting si ha:
$$ L = \begin{pmatrix} 1 & 0 \\ 5.00 \cdot 10^{-6} & 1 \end{pmatrix}, \quad U = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} $$
Poiché il fattore di crescita è $\rho = 1$ e $||A||_{\infty} = 1 + 0.5 \cdot 10^{-5}$, per il Teorema 2 si ha:
$$ ||E||_{\infty} \le 8 \cdot u \cdot 1 \cdot (1 + 0.5 \cdot 10^{-5}) \sim 0.9 \cdot 10^{-15} < 0.5 \cdot 10^{-14} $$

\paragraph{Crescita del fattore $\rho$ nel pivoting}
\begin{bxthm}
\begin{thm}
Per la fattorizzazione LU con pivoting parziale (e per l'eliminazione di Gauss con pivoting), per ogni matrice $A$ di ordine $n$ si ha:
$$ \rho \le 2^{n-1} $$    
\end{thm}
\end{bxthm}

Sfortunatamente esistono matrici che hanno fattore di crescita pari a $2^{n-1}$. Una di esse è la matrice di Wilkinson:
$$ a_{ij} = \begin{cases} 1, & i=j \\ -1, & i>j \\ 1, & j=n \\ 0 & \text{altrove} \end{cases} $$

\paragraph{Il pivoting totale}
Si sceglie una coppia $(r, s)$, con $r, s \ge k$ tale che:
$$ |a_{rs}^{(k)}| = \max_{k \le i,j \le n} |a_{ij}^{(k)}| $$
e si scambia l'equazione $k$-esima con la $r$-esima e l'incognita $k$-esima (con il suo coefficiente) con la $s$-esima.
Nella fattorizzazione LU con pivoting totale si costruiscono due matrici di permutazione $P$ e $Q$ tali che:
$$ PAQ = LU $$
Il sistema lineare $Ax=b$ diventa equivalente a $(Q^{-1}=Q^{T})$:
$$ PAQQ^{T}x = Pb $$
Posto $Q^{T}x = y$ si ha $PAQy = Pb$ ed usando la fattorizzazione LU si ha $LUy = Pb$.
Posto $Uy = z$ si calcola il vettore soluzione di $Lz = Pb$ e poi il vettore soluzione di $Uy = z$. Infine, si calcola la soluzione $x$ del sistema iniziale $x = Qy$.
Nella fattorizzazione LU con pivoting totale il fattore di crescita $\rho$ cresce molto lentamente, si prova infatti che:
$$ \rho \le (n \cdot 2^{1} \cdot 3^{\frac{1}{2}} \cdot 4^{\frac{1}{3}} \cdots n^{\frac{1}{n-1}})^{\frac{1}{2}} $$

\paragraph{Conclusioni sulla stabilità}
Il metodo di eliminazione di Gauss senza pivoting ha fattore di crescita pari a 1 per matrici simmetriche definite positive.
Il metodo di eliminazione di Gauss senza pivoting ha fattore di crescita $\le 2$ per matrici a diagonale dominante per righe o per colonne.
Da cui segue la stabilità del metodo di Gauss senza pivoting quando viene applicato a matrici simmetriche definite positive o a diagonale dominante per righe o per colonne.

\textit{Tabella riassuntiva:}
\begin{itemize}
    \item \textbf{GE} (o $A=LU$): non stabile in generale (stabile se la matrice è simmetrica definita positiva oppure a diagonale dominante).
    \item \textbf{GEPP} (o $PA=LU$): quasi sempre stabile.
    \item \textbf{GETP} (o $PAQ=LU$): stabile.
\end{itemize}
(GE: Gaussian Elimination, GEPP: Partial Pivoting, GETP: Total Pivoting)

\paragraph{Residuo e sua valutazione}
In riferimento ai sistemi lineari quadrati, sia $x$ la soluzione esatta ed $x^{*}$ la soluzione calcolata.
Dimostriamo che vale la seguente stima dell'errore relativo:
$$ \frac{||x-x^{*}||}{||x||} \le K(A) \frac{||Ax^{*}-b||}{||b||} $$
\textit{Dimostrazione.} Si ha:
$$ ||x-x^{*}|| = ||A^{-1}(Ax-Ax^{*})|| \le ||A^{-1}|| ||b-Ax^{*}|| = ||A^{-1}|| ||b|| \frac{||b-Ax^{*}||}{||b||} $$
Inoltre, poiché $||b|| \le ||A|| ||x||$, si ottiene:
$$ \frac{||x-x^{*}||}{||x||} \le ||A|| ||A^{-1}|| \frac{||Ax^{*}-b||}{||b||} $$
Dalla stima precedente si deduce che la norma del residuo $||Ax^{*}-b||$, pur essendo piccola (magari dell'ordine di eps), non garantisce un errore relativo piccolo, in particolare per matrici con elevato indice di condizionamento.
Quindi la quantità $||Ax^{*}-b||$ deve essere usata con cautela come stimatore a posteriori dell'errore.

\newpage

\section{Metodi Numerici per la Risoluzione di un Sistema Lineare Rettangolare nel Senso dei Minimi Quadrati}

\paragraph{Il problema dei minimi quadrati lineari}
Consideriamo un sistema lineare del tipo
$$ Ax=b $$
dove $A$ è una matrice di ordine $m \times n$, con $m \neq n$, $x$ è un vettore di lunghezza $n$ e $b$ un vettore di lunghezza $m$.
Molte situazioni reali, per esempio nelle applicazioni statistiche, nei problemi di teoria dei segnali, in astronomia, possono essere modellizzate mediante un sistema lineare nel quale la matrice dei coefficienti è rettangolare (o quadrata ma singolare).

In tali casi la soluzione può non esistere affatto oppure possono esserci infinite soluzioni.
\begin{itemize}
    \item Se $m > n$, se cioè il numero delle equazioni è superiore a quello delle incognite, il sistema è detto \textit{sovradeterminato} e non ammette soluzioni.
    \item Se invece $m < n$ il sistema è detto \textit{sottodeterminato} e ammette infinite soluzioni.
\end{itemize}

In questi casi, invece di cercare di determinare il vettore $x$ che verifichi l'identità $Ax=b$, si cerca, se esiste, un vettore $x$ tale che la distanza di $Ax$ da $b$ sia la minima possibile.
In altri termini il problema viene ricondotto ad un problema di minimo: determinare un $\overline{x}$ di lunghezza $n$ tale che, posto $r(x)=Ax-b$ (detto residuo) risulti
$$ ||r(\overline{x})|| = \min_{x} ||r(x)|| = \min_{x} ||Ax-b|| $$
Se la norma utilizzata è quella euclidea, ovvero la norma 2,
$$ ||x||_{2} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}} = \sqrt{x^{T}x} $$
il problema è detto \textit{problema dei minimi quadrati lineari} (PMQ).

Il nome è dovuto al fatto che si cerca il minimo della quantità
$$ \min_{x=(x_{1},\dots,x_{n})} ||Ax-b||_{2} = \min_{x=(x_{1},\dots,x_{n})} \sqrt{\sum_{i=1}^{m} \left[ b_{i} - \sum_{j=1}^{n} a_{ij}x_{j} \right]^{2}} $$
cioè si cerca di minimizzare una somma di quadrati.

Anche il problema dei minimi quadrati può avere infinite soluzioni. In questo caso si definisce \textit{soluzione di minima norma} quella per cui accade che
$$ ||x_{min}||_{2} \le ||x||_{2} $$
se $x$ è una qualsiasi soluzione.

D'ora in avanti ci occuperemo solo del caso dei sistemi sovradeterminati, cioè con $m \ge n$, che è anche il più significativo dal punto di vista delle applicazioni. Una delle più significative applicazioni del problema dei minimi quadrati sovradeterminati è quella relativa all'approssimazione di dati, detta anche \textit{best fitting}.

\paragraph{Risolubilità del problema dei minimi quadrati}
Vediamo ora in quali ipotesi il problema dei minimi quadrati ammette soluzione.

\begin{bxthm}
\begin{thm}
Se $A$ è una matrice $m \times n$ con $m > n$ allora esiste sempre una soluzione del PMQ. Inoltre tale soluzione è unica se e solo se il rango di $A$ è massimo, cioè se $\text{rank}(A) = n$.    
\end{thm}
\end{bxthm}

Omettiamo la dimostrazione di tale teorema mettendo però in evidenza che tale dimostrazione è basata sulla seguente equivalenza:
$$ x \text{ è soluzione del PMQ} \iff x \text{ è soluzione del sistema } A^{T}Ax = A^{T}b $$

\paragraph{Il sistema delle equazioni normali}
Il sistema
$$ A^{T}Ax = A^{T}b $$
è detto \textit{sistema delle equazioni normali}.
Osserviamo innanzitutto che la matrice $A^{T}A$ è una matrice quadrata di ordine $n$. Inoltre è simmetrica in quanto
$$ (A^{T}A)^{T} = A^{T}(A^{T})^{T} = A^{T}A $$
cioè coincide con la sua trasposta.
Infine, se $A$ ha rango massimo, è definita positiva poiché se $y$ è un qualunque vettore di lunghezza $n$ non nullo si ha
$$ y^{T}(A^{T}A)y = (y^{T}A^{T})(Ay) = (Ay)^{T}(Ay) = ||Ay||_{2}^{2} > 0 $$
(se il rango non è massimo può accadere che $Ay=0$, con $y \neq 0$).

\paragraph{Interpretazione geometrica}
Vogliamo ora dare un'interpretazione geometrica del problema dei minimi quadrati.
Se $A$ è di ordine $m \times n$ allora può essere pensata come un'applicazione lineare di $\mathbb{R}^{n} \to R(A) \subseteq \mathbb{R}^{m}$.
Il sottospazio $R(A)$ delle immagini è l'insieme dei vettori $u$ tali che esiste un $y \in \mathbb{R}^{n}$ tale che $u=Ay$.
Sia ora $b \in \mathbb{R}^{m}$. Poiché la norma 2 è quella euclidea, $||b-Ax||_{2}$ rappresenta la distanza tra i vettori $b$ e $Ax$.

È evidente allora che affinché tale distanza sia minima deve accadere che $b-Ax$ sia ortogonale a $R(A)$.
Ora, ogni vettore di $R(A)$ può essere visto come combinazione lineare delle colonne di $A$. Infatti se $u \in R(A)$ è tale che $u=Ay$, ciò significa che
$$ u = \begin{pmatrix} u_{1} \\ \vdots \\ u_{m} \end{pmatrix} = \begin{pmatrix} a_{11}y_{1} + \dots + a_{1n}y_{n} \\ \vdots \\ a_{m1}y_{1} + \dots + a_{mn}y_{n} \end{pmatrix} $$
cioè
$$ u = y_{1}a_{1} + y_{2}a_{2} + \dots + y_{n}a_{n} $$
dove $a_{j}$ denota la colonna $j$-esima di $A$.

Dunque, affinché $b-Ax$ risulti ortogonale a $R(A)$, basta che sia ortogonale a tutte le colonne di $A$, cioè
$$ a_{j}^{T}(b-Ax) = 0, \quad j=1,\dots,n \iff A^{T}(b-Ax) = 0 \iff A^{T}Ax = A^{T}b $$
Si perviene così, anche per via geometrica, all'equivalenza tra la soluzione del problema dei minimi quadrati e quello del sistema delle equazioni normali.

\paragraph{La pseudoinversa}
\begin{bxthm}
\begin{defn}
Sia $A$ una matrice $m \times n$, $m \ge n$ e con rango massimo ($\text{rank}(A)=n$). Si definisce \textit{pseudoinversa} o inversa generalizzata di Moore-Penrose di $A$ la matrice
$$ A^{+} = (A^{T}A)^{-1}A^{T} .$$
\end{defn}
\end{bxthm}

La pseudoinversa generalizza il concetto di inversa di una matrice nel caso delle matrici rettangolari. Infatti
$$ A^{+}A = (A^{T}A)^{-1}A^{T}A = I $$
D'altra parte se $A$ è quadrata e non singolare si ha
$$ A^{+} = (A^{T}A)^{-1}A^{T} = A^{-1}(A^{T})^{-1}A^{T} = A^{-1} $$
cioè inversa e pseudoinversa coincidono.

Il legame tra pseudoinversa e problema dei minimi quadrati è fornito dalla seguente

\begin{bxthm}
\begin{prop}
Sia $A$ di ordine $m \times n$, $m \ge n$ e con rango massimo ($\text{rank}(A)=n$) e $b$ un vettore di lunghezza $m$. L'unica soluzione del problema dei minimi quadrati relativo a tali dati è data da
$$ x = A^{+}b \equiv (A^{T}A)^{-1}A^{T}b.$$    
\end{prop}
\end{bxthm}

La Proposizione è immediata conseguenza del teorema di esistenza e unicità. Infatti abbiamo già detto che la soluzione del PMQ è quella del sistema delle equazioni normali
$$ A^{T}Ax = A^{T}b \implies x = (A^{T}A)^{-1}A^{T}b $$

La matrice pseudoinversa ha diverse e utili applicazioni, non solo nella soluzione del problema dei minimi quadrati.
Un'importante informazione che si può dedurre dalla conoscenza di $A^{+}$ è il condizionamento del problema dei minimi quadrati.
È infatti possibile dimostrare che, se si introducono delle perturbazioni su $b$ ed $A$, allora la corrispondente perturbazione sul vettore $x$, soluzione del problema dei minimi quadrati, è proporzionale a quelle introdotte sui dati mediante la quantità
$$ pcond(A) = ||A|| ||A^{+}|| $$

\paragraph{Risoluzione del sistema delle equazioni normali}
Abbiamo visto come la soluzione del problema dei minimi quadrati coincida, nel caso sovradeterminato e di rango massimo, con la soluzione del sistema delle equazioni normali:
$$ A^{T}Ax = A^{T}b $$
Dunque per calcolare numericamente la soluzione del PMQ è possibile risolvere numericamente il sistema delle equazioni normali.
Poiché la matrice dei coefficienti è simmetrica e definita positiva, possiamo usare il metodo di eliminazione di Gauss senza pivoting o, con un costo computazionale dimezzato, il metodo di Cholesky.

Dunque il problema dei minimi quadrati può essere risolto nel seguente modo:
\begin{itemize}
    \item si forma la matrice $A^{T}A$;
    \item si fattorizza $A^{T}A$ mediante fattorizzazione di Cholesky, ovvero si computa $L$ tale che $A^{T}A = LL^{T}$;
    \item si forma il vettore $c = A^{T}b$;
    \item si risolve la coppia di sistemi lineari triangolari:
    $$ \begin{cases} Ly = c \\ L^{T}x = y \end{cases} $$
\end{itemize}

\paragraph{Costo computazionale e stabilità}
Il costo computazionale complessivo della procedura è di circa
$$ m\frac{n^{2}}{2} + \frac{n^{3}}{6} $$
operazioni, in quanto ne occorrono circa $m\frac{n^{2}}{2}$ per formare $A^{T}A$ (che si costruisce per simmetria) e $\frac{n^{3}}{6}$ per la fattorizzazione di Cholesky.
Le sostituzioni all'indietro e in avanti costano complessivamente $n^{2}$ e sono trascurate.
Dunque la procedura è relativamente economica, in quanto l'operazione più costosa (la fattorizzazione di Cholesky) viene effettuata sulla matrice di ordine $n$ (che in genere è molto minore di $m$).

Sfortunatamente la procedura appena descritta è complessivamente instabile.
Infatti il primo problema è la costruzione della matrice $A^{T}A$. Può accadere che vi sia una perdita di cifre significative nella rappresentazione di macchina di $A^{T}A$.
Nei casi più patologici può verificarsi che la matrice "di macchina" non sia più definita positiva o addirittura che risulti singolare.
È stato infatti provato che se $pcond(A) > 10^{\frac{t}{2}}$, dove $t$ è la precisione della rappresentazione floating point, non è più assicurato che $fl(A^{T}A)$ sia non singolare.

\paragraph{Esempio di instabilità}
Consideriamo la matrice
$$ A = \begin{pmatrix} 1 & 1 \\ 10^{-4} & 0 \\ 0 & 10^{-4} \end{pmatrix} $$
e assumiamo di lavorare in precisione $t=8$. Le colonne di $A$ sono linearmente indipendenti e quindi il rango è 2.
Computiamo ora $A^{T}A$.
$$ A^{T}A = \begin{pmatrix} 1+10^{-8} & 1 \\ 1 & 1+10^{-8} \end{pmatrix} $$
Poiché $t=8$, allora $fl(A^{T}A) = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$ che è singolare!
Si noti che $pcond(A) = 1.4142... \cdot 10^{4} > 10^{4}$.

Anche se non si incorre in questi casi "patologici" tuttavia la procedura è generalmente poco stabile in quanto, mentre il PMQ ha un condizionamento che è regolato da $pcond(A)$, il sistema delle equazioni normali ha un condizionamento che dipende da $cond(A^{T}A)$.
Si può provare che (in norma 2):
$$ cond(A^{T}A) = pcond(A)^{2} $$
Concludendo possiamo dire che alla risoluzione mediante equazioni normali, benché economica, si deve ricorrere solo quando si hanno informazioni sul buon condizionamento della matrice $A^{T}A$. Nei casi in cui si è incerti sul condizionamento del problema conviene utilizzare altri metodi, più costosi, ma stabili.

\paragraph{Metodo mediante fattorizzazione QR}
\paragraph{La fattorizzazione QR}
È noto che le fattorizzazioni di matrici costituiscono un efficace strumento in algebra lineare. Le più note sono la fattorizzazione LU e la fattorizzazione $LL^{T}$ di Cholesky (per matrici simmetriche e definite positive). Entrambe queste fattorizzazioni sono per matrici quadrate.
Il costo computazionale per la costruzione di $L$ e $U$ è di circa $\frac{m^{3}}{3}$ operazioni, mentre la fattorizzazione di Cholesky costa $\frac{m^{3}}{6}$ operazioni.

Illustriamo ora un'altra fattorizzazione che prende il nome di fattorizzazione QR e che può essere costruita anche per matrici rettangolari.
Data una matrice $A$ di ordine $m \times n$ esistono due matrici, $Q$ quadrata di ordine $m$ e ortogonale (cioè tale che $Q^{-1}=Q^{T}$) e $R$ di ordine $m \times n$, del tipo:
$$ R = \begin{pmatrix} R_{1} \\ 0 \end{pmatrix} $$
con $R_{1}$ triangolare superiore di ordine $n$, se $m > n$, oppure del tipo $(R_{1} \quad 0)$ con $R_{1}$ triangolare superiore di ordine $m$, se $m < n$.

Nel caso $A$ sia quadrata i fattori $Q$ ed $R$ sono entrambi quadrati e $R$ è triangolare superiore.
Esistono vari algoritmi per la costruzione della fattorizzazione QR, il più utilizzato dei quali (quello che adopera le matrici elementari di Householder) ha un costo computazionale di $\frac{2}{3}m^{3}$, nel caso della matrice quadrata e piena.
Nel caso rettangolare con $m > n$ invece il costo è dell'ordine di:
$$ n^{2}(m - \frac{n}{3}) $$

Confrontando dunque su matrici quadrate, la fattorizzazione QR "costa di più" della fattorizzazione LU. Il valore aggiunto però sta nel fatto che il fattore $Q$ è ortogonale.
Le matrici ortogonali rivestono un'importanza rilevante per le loro numerose proprietà:
\begin{itemize}
    \item Non è necessario costruire le inverse, che sono disponibili con la sola trasposizione ($Q^{-1} \equiv Q^{T}$), il che implica $Q^{T}Q = QQ^{T} = I$.
    \item Se $y$ è un qualsiasi vettore di lunghezza $m$ si ha:
    $$ ||Qy||_{2} = \sqrt{(Qy)^{T}(Qy)} = \sqrt{y^{T}Q^{T}Qy} = \sqrt{y^{T}y} = ||y||_{2} $$
    cioè la moltiplicazione per matrici ortogonali lascia invariata la norma del vettore (lo stesso vale per $Q^{T}$).
    \item Da quanto sopra si ha $||Q||_{2}=1$, $||Q^{T}||_{2}=1$, da cui $cond(Q)=1$, cioè le matrici ortogonali sono perfettamente condizionate.
\end{itemize}

\paragraph{Il metodo per il PMQ basato sulla fattorizzazione QR}
Ricordiamo che il problema dei minimi quadrati lineari consiste nel cercare il minimo, su tutti i vettori di lunghezza $n$, della quantità $||Ax-b||_{2}$ dove $A$ è una matrice di ordine $m \times n$, con $m \ge n$, $rank(A)=n$ e $b$ è di dimensione $m$.
Un metodo numerico alternativo è basato sull'utilizzo della fattorizzazione QR di $A$.

Siano infatti $Q$ e $R$ i fattori di $A$, si ha:
$$ Q^{T}A = \begin{pmatrix} R_{1} \\ 0 \end{pmatrix} $$
dove $R_{1}$ è quadrata, di ordine $n$ e triangolare superiore.
Poiché $Q^{T}$ è ortogonale, e quindi preserva la norma 2 del vettore, si ha:
$$ ||Ax-b||_{2}^{2} = ||Q^{T}(Ax-b)||_{2}^{2} = ||Q^{T}Ax - Q^{T}b||_{2}^{2} $$
Ponendo ora $Q^{T}b = \begin{pmatrix} c \\ d \end{pmatrix}$ con $c$ vettore di lunghezza $n$, si ha:
$$ ||Ax-b||_{2}^{2} = ||\begin{pmatrix} R_{1}x \\ 0 \end{pmatrix} - \begin{pmatrix} c \\ d \end{pmatrix}||_{2}^{2} = ||\begin{pmatrix} R_{1}x - c \\ -d \end{pmatrix}||_{2}^{2} $$

Osserviamo ora che se un vettore $y$ di lunghezza $m$ è decomposto in due sottovettori, per esempio $y=(z,t)^{T}$ con $z$ di lunghezza $n$ e $t$ di lunghezza $m-n$, si ha che $||y||_{2}^{2} = ||z||_{2}^{2} + ||t||_{2}^{2}$.
Dunque:
$$ ||Ax-b||_{2}^{2} = ||\begin{pmatrix} R_{1}x - c \\ -d \end{pmatrix}||_{2}^{2} = ||R_{1}x - c||_{2}^{2} + ||d||_{2}^{2} $$
Poiché il secondo termine della somma non dipende da $x$ ed è sempre positivo, il minimo del primo membro sarà assunto se $||R_{1}x - c||_{2} = 0$, se cioè $R_{1}x = c$, ovvero se $x$ è soluzione del sistema triangolare con $R_{1}$ come matrice dei coefficienti e $c$ come termine noto.

\paragraph{Procedura e Costo}
Dunque la procedura proposta può essere riassunta come segue:
\begin{itemize}
    \item si effettua la fattorizzazione QR della matrice $A$ e quindi, in particolare, si determina la matrice $R_{1}$;
    \item si forma il vettore $c$ costituito dai primi $n$ elementi del vettore $Q^{T}b$;
    \item si risolve il sistema triangolare $R_{1}x = c$ (mediante sostituzione all'indietro).
\end{itemize}
La quantità $||d||_{2}$ rappresenta la norma 2 del residuo.
L'algoritmo ha un costo computazionale che è essenzialmente quello della fattorizzazione QR e pertanto è $n^{2}(m-\frac{n}{3})$. La procedura descritta risulta inoltre stabile.

\paragraph{Conclusioni}
Abbiamo proposto due possibili strategie numeriche per la risoluzione del problema dei minimi quadrati lineari, nel caso sia $A$ di dimensioni $m \times n$, con $m \ge n$ e $rank(A)=n$:
\begin{enumerate}
    \item la risoluzione del sistema delle equazioni normali $A^{T}Ax = A^{T}b$;
    \item il metodo che utilizza la fattorizzazione QR della matrice $A$.
\end{enumerate}

\paragraph{Confronto Schemi}
\textbf{Metodo 1 (Equazioni Normali):}
\begin{itemize}
    \item formare la matrice $A^{T}A$;
    \item calcolare la fattorizzazione di Cholesky $A^{T}A = LL^{T}$;
    \item risolvere la coppia di sistemi triangolari $\begin{cases} Ly = A^{T}b \\ L^{T}x = y \end{cases}$;
    \item costo computazionale: $m\frac{n^{2}}{2} + \frac{n^{3}}{6}$;
    \item stabilità: assicurata solo se $cond(A^{T}A) = pcond(A)^{2}$ non è grande.
\end{itemize}

\textbf{Metodo 2 (Fattorizzazione QR):}
\begin{itemize}
    \item calcolare la fattorizzazione QR di $A$;
    \item calcolare $Q^{T}b = \begin{pmatrix} c \\ d \end{pmatrix}$, dove $c$ ha lunghezza $n$;
    \item posto $R = \begin{pmatrix} R_{1} \\ 0 \end{pmatrix}$, risolvere il sistema $R_{1}x = c$;
    \item la norma del residuo è data da $||d||_{2}$;
    \item costo computazionale: $n^{2}(m - \frac{n}{3})$;
    \item stabilità: assicurata sempre.
\end{itemize}

\paragraph{Functions del Matlab}
Le funzioni Matlab coinvolte nella risoluzione del PMQ sono le seguenti:
\begin{itemize}
    \item \texttt{A\textbackslash b}: implementa, nel caso $A$ sia rettangolare, il metodo che utilizza la fattorizzazione QR, cioè il metodo 2;
    \item \texttt{R = chol(B)}: calcola il fattore di Cholesky della matrice simmetrica e definita positiva $B$ ($B={}^tRR$);
    \item \texttt{D = pinv(A)}: calcola la pseudoinversa (sinistra) della matrice $A$;
    \item \texttt{[Q,R] = qr(A)}: calcola la fattorizzazione QR della matrice $A$ (quadrata o rettangolare) mediante trasformazioni di Householder.
\end{itemize}

\newpage

\section{Metodi Numerici per la Risoluzione di una Equazione non Lineare}

\vspace{20pt}

\subsection{Risoluzione di Equazioni non Lineari}

\vspace{10pt}

\paragraph{Introduzione}
Sia $F \in C^{0}([a,b])$, cioè $F$ è una funzione continua in un intervallo $[a,b] \subset \mathbb{R}$, tale che $F(a)F(b) < 0$.
Vogliamo trovare le radici dell'equazione non lineare
$$ F(x) = 0 $$
La condizione $F(a)F(b) < 0$ è una condizione sufficiente per l'esistenza di almeno una radice di $F$ in $[a,b]$.

\vspace{10pt}

\paragraph{Metodo di Bisezione}
Questo metodo consiste nel costruire, a partire dall'intervallo $[a,b]$, una successione di intervalli incapsulati
$$ [a,b] = [a_{0}, b_{0}] \supset [a_{1}, b_{1}] \supset \dots \supset [a_{n}, b_{n}] $$
tutti contenenti la radice $\overline{x}$ di $F$, tale che
$$ b_{n} - a_{n} \rightarrow 0 \quad \text{per } n \rightarrow +\infty $$

Gli intervalli $[a_{k}, b_{k}]$, $k=1,\dots,n$, della successione vengono determinati come segue:
dato $[a_{k-1}, b_{k-1}]$, determiniamo il punto medio
$$ m_{k} = \frac{a_{k-1} + b_{k-1}}{2} $$
\begin{itemize}
    \item se $F(m_{k}) = 0$ allora $\overline{x} = m_{k}$ e abbiamo terminato;
    \item se $F(m_{k}) \neq 0$ poniamo
    $$ [a_{k}, b_{k}] = \begin{cases} [a_{k-1}, m_{k}] & \text{se } F(a_{k-1})F(m_{k}) < 0 \\ [m_{k}, b_{k-1}] & \text{se } F(b_{k-1})F(m_{k}) < 0 \end{cases} $$
\end{itemize}

Dopo $n$ passi si giunge all'intervallo $[a_{n}, b_{n}]$ contenente la radice $\overline{x}$ e di ampiezza
$$ b_{n} - a_{n} = \frac{b_{n-1} - a_{n-1}}{2} = \frac{b_{n-2} - a_{n-2}}{2^{2}} = \dots = \frac{b-a}{2^{n}} $$
Come stima di $\overline{x}$ scegliamo
$$ x_{n+1} = m_{n+1} = \frac{a_{n} + b_{n}}{2} $$
così che
$$ \overline{x} = x_{n+1} \pm \varepsilon_{n+1} $$
dove
$$ \varepsilon_{n+1} = |\overline{x} - x_{n+1}| < \frac{b-a}{2^{n+1}} $$
è l'errore assoluto di approssimazione della radice $\overline{x}$ di $F$.

Fissata una tolleranza TOLL, è possibile determinare il numero di iterazioni $k$ necessarie per approssimare la radice $\overline{x}$ con precisione TOLL imponendo che
$$ |\overline{x} - x_{n}| < \frac{b-a}{2^{n}} < \text{TOLL} $$
Infatti, possiamo scrivere
$$ \frac{b-a}{\text{TOLL}} < 2^{n} $$
$$ \log\left(\frac{b-a}{\text{TOLL}}\right) < \log(2^{n}) = n \log 2 $$
$$ n > \frac{\log\left(\frac{b-a}{\text{TOLL}}\right)}{\log 2} $$
Dunque prendiamo
$$ n = \left\lfloor \frac{\log\left(\frac{b-a}{\text{TOLL}}\right)}{\log 2} \right\rfloor + 1 $$
dove $\lfloor a \rfloor$ denota la parte intera inferiore di $a$.

\paragraph{Ordine di convergenza}
Introduciamo ora una misura della velocità di convergenza di una successione numerica.

\begin{bxthm}
\begin{defn}
Sia $\{x_{n}\}_{n}$ una successione convergente al valore $\overline{x}$. Poniamo $\varepsilon_{n} = |\overline{x} - x_{n}|$. Se esiste un numero reale $p \ge 1$ e una costante reale positiva $C$ tale che
$$ \lim_{n \to +\infty} \frac{\varepsilon_{n+1}}{\varepsilon_{n}^{p}} = C $$
allora diciamo che la successione $\{x_{n}\}_{n}$ ha ordine di convergenza $p$.    
\end{defn}
\end{bxthm}

Poiché per il metodo di bisezione si ha
$$ \frac{\varepsilon_{n+1}}{\varepsilon_{n}} \simeq \frac{1}{2} $$
ovvero l'errore si dimezza (più o meno) ad ogni passo, il suo ordine di convergenza è 1, cioè converge molto lentamente.
Va però osservato che il metodo richiede solo la continuità della funzione $F$ e la conoscenza del segno di $F$ negli estremi dell'intervallo $[a, b]$.

\textit{Osservazione:} Per ogni $n \ge 0$ supponiamo che l'approssimazione $x_{n}$ di $\overline{x}$ abbia $t_{n} \in \mathbb{R}$ cifre decimali corrette, cioè
$$ \varepsilon_{n} = \frac{1}{2} 10^{-t_{n}} $$
Allora, poiché ad ogni passo l'errore si dimezza,
$$ \frac{1}{2} 10^{-t_{n+1}} = \varepsilon_{n+1} = \frac{1}{2} \varepsilon_{n} = \frac{1}{4} 10^{-t_{n}} $$
ovvero
$$ 10^{-t_{n+1}} = \frac{1}{2} 10^{-t_{n}} $$
$$ t_{n+1} = t_{n} + \log_{10} 2 \approx t_{n} + 0.3010, \quad n \ge 0 $$
Dunque $t_{n}$ aumenta di una unità dopo almeno 4 iterazioni. 3 iterazioni non sono sufficienti per ottenere un'approssimazione di $\overline{x}$ con una cifra decimale corretta in più.

\paragraph{Metodi Iterativi}
Partendo dal punto iniziale $x_{0}$, generiamo i valori $x_{1}, x_{2}, \dots, x_{n}$ nel seguente modo:
\begin{itemize}
    \item Conduciamo dal punto $(x_{0}, F(x_{0}))$ una retta con pendenza $m_{0}$ e scegliamo come approssimazione $x_{1}$ l'intersezione di tale retta con l'asse delle x;
    \item Conduciamo dal punto $(x_{1}, F(x_{1}))$ una retta con pendenza $m_{1}$ e scegliamo come approssimazione $x_{2}$ l'intersezione di tale retta con l'asse delle x;
\end{itemize}
Dunque ad ogni passo scegliamo come approssimazione della radice $\overline{x}$ di $F$, la radice dell'equazione lineare
$$ F(x_{n}) + m_{n}(x - x_{n}) = 0 $$
cioè
$$ x_{n+1} = x_{n} - \frac{F(x_{n})}{m_{n}} \quad (1) $$
Una formula del tipo (1) viene detta \textit{formula iterativa} e si dice che la successione $x_{1}, x_{2}, \dots, x_{n}, \dots$ viene costruita mediante un procedimento iterativo.

La funzione
$$ g(x) = x - \frac{F(x)}{m_{n}} $$
tale che $x_{n+1} = g(x_{n})$ viene detta \textit{funzione di iterazione semplice} perché per la costruzione della successione $\{x_{n}\}_{n}$ utilizza solo il punto $x_{n}$.
Si parla di procedimenti iterativi multipli quando
$$ x_{n+k} = g(x_{n+k-1}, x_{n+k-2}, \dots, x_{n}), \quad k > 1 $$
cioè la successione $\{x_{n}\}_{n}$ è costruita a partire dai punti $x_{0}, x_{1}, \dots, x_{k-1}$.

\paragraph{Criteri di Arresto}
Quando si implementa in maniera automatica un procedimento iterativo del tipo $x_{n+1} = g(x_{n})$, $n=0,1,\dots$, occorrono uno o più criteri per arrestare tale procedimento.
Fissata una tolleranza TOLL arbitrariamente piccola, i criteri di arresto usualmente utilizzati sono:
1. $|F(x_{n})| < \text{TOLL}$ (tanto più è piccolo TOLL tanto più $x_{n}$ è vicino ad $\overline{x}$);
2. $|x_{n+1} - x_{n}| < \text{TOLL}$ oppure $\frac{|x_{n+1} - x_{n}|}{|x_{n+1}|} < \text{TOLL}$ (tanto più è piccolo TOLL tanto più $x_{n}$ è vicino al limite della successione $\overline{x}$);
3. numero delle iterazioni < ITMAX, dove ITMAX è una variabile intera fissata.

Il criterio 3 entra in gioco quando i primi due falliscono. Per questo motivo la variabile ITMAX viene anche detta variabile tappo. In generale è preferibile utilizzare tutti e tre i criteri, perché per particolari funzioni si può verificare che la convergenza sia troppo lenta oppure che i criteri 1 e/o 2 non vengano mai soddisfatti numericamente.

\paragraph{Metodo di Newton o delle tangenti}
Nella formula
$$ x_{n+1} = x_{n} - \frac{F(x_{n})}{m_{n}} $$
le direzioni $m_{n}$ possono essere scelte in vari modi. In questo metodo la direzione scelta ad ogni passo è quella della tangente alla curva $y=F(x)$ nel punto $x_{n}$, cioè
$$ m_{n} = F'(x_{n}) $$
Dunque il metodo è dato da
$$ x_{n+1} = x_{n} - \frac{F(x_{n})}{F'(x_{n})}, \quad n=0,1,\dots $$
La funzione di iterazione è quindi
$$ g(x) = x - \frac{F(x)}{F'(x)} $$
Appare evidente che il metodo perde di significato se per qualche $n$ risulta $F'(x_{n}) = 0$. Dunque in generale si suppone che $F'(x) \neq 0$, $\forall x \in [a,b]$.

La formula iterativa ci permette di calcolare la successione $x_{1}, x_{2}, \dots$ a partire dal punto $x_{0}$.
\textit{Domanda:} Come scegliamo il punto $x_{0}$?

\paragraph{Teorema di Convergenza}
Il seguente teorema stabilisce delle condizioni per la determinazione di un intervallo $[a,b]$ per il quale il metodo di Newton converge per ogni scelta di $x_{0} \in [a,b]$.

\begin{bxthm}
\begin{thm}
Sia $F(x) \in C^{2}([a,b])$ con $[a,b]$ chiuso e limitato. Se:
\begin{itemize}
    \item $F(a)F(b) < 0$;
    \item $F'(x) \neq 0$, $\forall x \in [a,b]$;
    \item $F(x) \ge 0$ oppure $F(x) \le 0$, $\forall x \in [a,b]$;
    \item $|\frac{F(a)}{F'(a)}| < (b-a)$ e $|\frac{F(b)}{F'(b)}| < (b-a)$;
\end{itemize}
allora il Metodo di Newton converge all'unica soluzione $\overline{x} \in [a,b]$ per ogni scelta di $x_{0} \in [a,b]$.    
\end{thm}
\end{bxthm}
\begin{proof}
Dalle condizioni 1-3 segue l'unicità della soluzione dell'equazione $F(x)=0$ nell'intervallo $[a,b]$.
Facciamo prima vedere che la condizione 4 implica che la tangente alla curva di equazione $y=F(x)$ negli estremi dell'intervallo interseca l'asse $x$ all'interno di $[a,b]$. Conseguentemente, se prendiamo come punto iniziale $x_{0}$ un punto appartenente all'intervallo $[a,b]$ (a e b inclusi) tutti i punti della successione $\{x_{k}\}_{k=1,2,\dots}$ appartengono all'intervallo $[a,b]$.

La retta tangente in $(a, F(a))$ ha equazione
$$ y = F'(a)(x-a) + F(a) $$
ed ha dunque intersezione con l'asse $x$ in
$$ x = a - \frac{F(a)}{F'(a)} $$
Poiché $\frac{F(a)}{F'(a)}$ sotto le condizioni 1-3 è sempre negativa e, usando l'ipotesi 4, risulta
$$ |x-a| = \left|\frac{F(a)}{F'(a)}\right| < b-a $$
otteniamo $a < x < b$. Analogamente si ragiona per l'altro estremo.

Facciamo ora vedere che la successione $\{x_{k}\}_{k=1,2,\dots}$ che si costruisce a partire da $x_{0} \in [a,b]$ è monotona e limitata. Conduciamo la dimostrazione in una delle possibili quattro situazioni che si possono verificare nelle ipotesi 1-3 (la dimostrazione negli altri casi è simile).
Supponiamo allora che sia $F' > 0$, $F \le 0$, $F(a) < 0$, $F(b) > 0$.
Dimostriamo dunque che $x_{k} \le \overline{x}$ e $x_{k+1} \ge x_{k}$.
Incominciamo per $k=0$ e poi procederemo per induzione.
Senza ledere la generalità, prendiamo $x_{0}$ nell'intervallo $[a, \overline{x}]$ e quindi tale che $F(x_{0}) \le 0$. Si ha allora
$$ x_{1} = x_{0} - \frac{F(x_{0})}{F'(x_{0})} \ge x_{0} $$
Dunque $x_{0} \le \overline{x}$ e $x_{1} \ge x_{0}$.

Poiché la proprietà è vera per $k=0$, dimostriamo che, supposta vera per $k$, essa è verificata per $k+1$.
Si ha
$$ x_{k+1} = x_{k} - \frac{F(x_{k})}{F'(x_{k})} $$
e, per il Teorema di Lagrange,
$$ -F(x_{k}) = F(\overline{x}) - F(x_{k}) = F'(\xi_{k})(\overline{x} - x_{k}), \quad x_{k} \le \xi_{k} \le \overline{x} $$
Dal momento che per ipotesi $F(x) \le 0$, si ha che $F'$ è decrescente e quindi $F'(\xi_{k}) \le F'(x_{k})$, da cui
$$ -F(x_{k}) \le F'(x_{k})(\overline{x} - x_{k}) $$
Di conseguenza
$$ x_{k+1} = x_{k} - \frac{F(x_{k})}{F'(x_{k})} \le x_{k} + (\overline{x} - x_{k}) = \overline{x} $$
$$ F(x_{k+1}) \le 0 \implies x_{k+2} = x_{k+1} - \frac{F(x_{k+1})}{F'(x_{k+1})} \ge x_{k+1} $$

Per completare la dimostrazione, basta osservare che la successione $\{x_{k}\}_{k=1,2,\dots}$ essendo monotona e limitata, è convergente. Inoltre passando al limite nella formula iterativa si deduce immediatamente che essa converge ad $\overline{x}$.
Infatti, detto $\beta = \lim_{k \to \infty} x_{k}$, si ha
$$ \beta = \lim_{k \to \infty} x_{k+1} = \lim_{k \to \infty} x_{k} - \lim_{k \to \infty} \frac{F(x_{k})}{F'(x_{k})} = \beta - \frac{F(\beta)}{F'(\beta)} $$
da cui $F(\beta) = 0$ e quindi $\beta = \overline{x}$ essendo, date le nostre ipotesi, $\overline{x}$ l'unico zero di $F$ nell'intervallo $[a,b]$.    
\end{proof}

\paragraph{Ordine di convergenza del Metodo di Newton}
Per determinare l'ordine di convergenza utilizziamo il seguente
\begin{bxthm}
\begin{thm}
Condizione necessaria e sufficiente affinché un procedimento iterativo semplice e convergente ad $\overline{x}$ abbia ordine di convergenza $p$ è che, se la funzione di iterazione $g$ è dotata di derivata $p$-esima continua per $x=\overline{x}$ risulti
$$ g'(\overline{x}) = g(\overline{x}) = \dots = g^{(p-1)}(\overline{x}) = 0 \quad \text{e} \quad g^{(p)}(\overline{x}) \neq 0 $$    
\end{thm}
\end{bxthm}

Per il Metodo di Newton si ha:
$$ g'(x) = 1 - \frac{(F'(x))^{2} - F(x)F(x)}{(F'(x))^{2}} = \frac{F(x)F(x)}{(F'(x))^{2}} $$
ma, poiché assumiamo che $F(\overline{x}) = 0$ si ha
$$ g'(\overline{x}) = 0 $$
e quindi
$$ g(x) = \frac{(F'(x)F(x) + F(x)F'(x))(F'(x))^{2} - 2F'(x)F(x)(F(x))^{2}}{(F'(x))^{4}} $$
$$ g(\overline{x}) = \frac{F(\overline{x})}{F'(\overline{x})} $$
che, in generale, è diverso da 0. Dunque, in generale, il metodo ha ordine di convergenza 2.

\textit{Domanda:} Cosa succede se $\overline{x}$ è uno zero doppio, cioè $F(\overline{x}) = F'(\overline{x}) = 0$ e $F(\overline{x}) \neq 0$?
In questo caso non è possibile calcolare né $g$ né $g'$ in $\overline{x}$, ma è possibile definirli per continuità:
$$ g(\overline{x}) = \overline{x} - \lim_{x \to \overline{x}} \frac{F(x)}{F'(x)} = \overline{x} - \lim_{x \to \overline{x}} \frac{F'(x)}{F(x)} = \overline{x} $$
$$ g'(\overline{x}) = F(\overline{x}) \lim_{x \to \overline{x}} \frac{F(x)}{(F'(x))^{2}} = F(\overline{x}) \lim_{x \to \overline{x}} \frac{F'(x)}{2F'(x)F(x)} = \frac{1}{2} $$
Dunque se $\overline{x}$ è uno zero doppio, essendo $g'(\overline{x}) \neq 0$, il metodo di Newton ha ordine di convergenza 1.
Più in generale, se $\overline{x}$ è uno zero d'ordine $r$, cioè
$$ F(\overline{x}) = F'(\overline{x}) = \dots = F^{(r-1)}(\overline{x}) = 0 \quad \text{e} \quad F^{(r)}(\overline{x}) \neq 0 $$
allora
$$ g'(\overline{x}) = 1 - \frac{1}{r} \neq 0 $$
dunque il metodo di Newton ha ancora ordine di convergenza 1.

\paragraph{Metodo di Newton per radici multiple}
Quando $\overline{x}$ è uno zero multiplo è possibile modificare leggermente il metodo di Newton per "recuperare" l'ordine 2.
Si considera la seguente formula iterativa:
$$ x_{n+1} = x_{n} - 2\frac{F(x_{n})}{F'(x_{n})} $$
la cui funzione di iterazione è
$$ g(x) = x - 2\frac{F(x)}{F'(x)} $$
La precedente formula iterativa ha ordine di convergenza 2, infatti:
$$ g'(x) = 1 - 2\frac{(F'(x))^{2} - F(x)F(x)}{(F'(x))^{2}} = 2\frac{F(x)F(x)}{(F'(x))^{2}} - 1 $$
otteniamo
$$ g'(\overline{x}) = 2F(\overline{x}) \lim_{x \to \overline{x}} \frac{F(x)}{(F'(x))^{2}} - 1 = 1 - 1 = 0 $$

In generale se $\overline{x}$ è una radice multipla di ordine $r$ si utilizza la seguente formula iterativa
$$ x_{n+1} = x_{n} - r\frac{F(x_{n})}{F'(x_{n})}, \quad n=0,1,\dots $$
il cui ordine di convergenza è ancora 2.
Esistono opportune modifiche anche nei casi in cui la molteplicità della radice è $>1$ ma non è nota e nei casi in cui la molteplicità è infinita. Esistono delle varianti del Metodo di Newton che hanno ordine di convergenza $>$ 2.

\textit{Osservazione:} Quando la funzione $F$ ha radici multiple, abbiamo visto che da un punto di vista teorico il Metodo di Newton converge. Tuttavia, spesso si verifica che esso numericamente non converge. La stessa cosa certe volte accade quando $F$ ha radici "quasi multiple".

\paragraph{Esempi}

\textit{Esempio 1:} Data l'equazione
$$ \cos x - 4x = 0 \quad (2) $$
determinare un intervallo dell'asse reale che contenga l'unico zero di tale equazione e sia tale che il Metodo di Newton risulti convergente.
\textit{Svolgimento:} Localizziamo graficamente lo zero dell'equazione (2) trovando il punto di intersezione delle curve $y=\cos x$ e $y=4x$. Si vede che lo zero $\overline{x}$ cade nell'intervallo $[0, \frac{1}{2}]$.
Vediamo se tale intervallo è il giusto candidato per la convergenza del Metodo di Newton.
Prima verifichiamo le condizioni agli estremi. Si ha
$$ F(0)F(1/2) = 1 \cdot (-1.12241) < 0 $$
Ponendo $F(x) = \cos x - 4x$ si ha
$$ F'(x) = -\sin x - 4 \Rightarrow F'(x) < 0 \forall x \in \mathbb{R} $$
$$ F(x) = -\cos x \Rightarrow F(x) \le 0 \text{ se } \cos x \ge 0 $$
$$ F(x) \le 0 \quad \forall x \in [0, 1/2] $$
Inoltre
$$ \left|\frac{F(0)}{F'(0)}\right| = \left|\frac{1}{-4}\right| = \frac{1}{4} < \frac{1}{2} $$
$$ \left|\frac{F(1/2)}{F'(1/2)}\right| = \left|\frac{-1.12241}{-4.47942}\right| = 0.250571\dots < \frac{1}{2} $$
Dunque il Metodo di Newton converge $\forall x_{0} \in [0, 1/2]$.

\textit{Esempio 2:} Data l'equazione
$$ e^{x} + \frac{x}{10} = 0 \quad (3) $$
determinare un intervallo dell'asse reale che contenga l'unico zero di tale equazione e sia tale che il Metodo di Newton risulti convergente.
\textit{Svolgimento:} Localizziamo graficamente lo zero dell'equazione (3) trovando il punto di intersezione delle curve $y=e^{x}$ e $y=-x/10$. Si vede che lo zero $\overline{x}$ cade nell'intervallo $[-2, -3/2]$.
Vediamo se tale intervallo è il giusto candidato per la convergenza del Metodo di Newton.
Prima verifichiamo le condizioni agli estremi. Si ha
$$ F(-2)F(-3/2) = (-0.64665e-003) \cdot (0.73130e-003) < 0 $$
Ponendo $F(x) = e^{x} + x/10$ si ha
$$ F'(x) = e^{x} + 1/10 \Rightarrow F'(x) > 0 \forall x \in \mathbb{R} $$
$$ F(x) = e^{x} \Rightarrow F(x) > 0 \forall x \in \mathbb{R} $$
Inoltre
$$ \left|\frac{F(-2)}{F'(-2)}\right| = \left|\frac{-0.64665e-003}{0.23534}\right| = 0.27478 < \frac{1}{2} $$
$$ \left|\frac{F(-3/2)}{F'(-3/2)}\right| = \left|\frac{0.73130e-003}{0.32313e-002}\right| = 0.22632e-001\dots < \frac{1}{2} $$
Dunque il Metodo di Newton converge $\forall x_{0} \in [-2, -3/2]$.

\textit{Esempio 3:} Data l'equazione
$$ x + \log x^{3} = 0 \quad (4) $$
determinare un intervallo dell'asse reale che contenga l'unico zero di tale equazione e sia tale che il Metodo di Newton risulti convergente.
\textit{Svolgimento:} Localizziamo graficamente lo zero dell'equazione (4) trovando il punto di intersezione delle curve $y=\log x$ e $y=-x/3$. 
Si vede che lo zero $\overline{x}$ cade nell'intervallo $[1/2, 1]$.
Vediamo se tale intervallo è il giusto candidato per la convergenza del Metodo di Newton.
Prima verifichiamo le condizioni agli estremi. Si ha
$$ F(1/2)F(1) = (-1.57944) \cdot 1 < 0 $$
Ponendo $F(x) = x + 3 \log x$ si ha
$$ F'(x) = 1 + \frac{3}{x} \Rightarrow F'(x) > 0 \forall x > 0 $$
$$ F(x) = -\frac{3}{x^{2}} \Rightarrow F(x) < 0 \forall x \in [1/2, 1] $$
Inoltre
$$ \left|\frac{F(1/2)}{F'(1/2)}\right| = \left|\frac{-1.57944}{7}\right| = 0.22563 < \frac{1}{2} $$
$$ \left|\frac{F(1)}{F'(1)}\right| = \left|\frac{1}{4}\right| < \frac{1}{2} $$
Dunque il Metodo di Newton converge $\forall x_{0} \in [1/2, 1]$.

\paragraph{Osservazione}
La convergenza del Metodo di Newton è garantita quando, supposte soddisfatte le ipotesi del teorema, l'approssimazione iniziale $x_{0}$ è "sufficientemente" vicina alla radice $\overline{x}$.
Pertanto tale metodo si rivela spesso efficiente soprattutto per migliorare un'approssimazione sufficientemente buona ottenuta con un metodo di ordine più basso la cui convergenza è assicurata.
In generale, anziché verificare le ipotesi del teorema, è preferibile utilizzare il Metodo di bisezione per determinare un'approssimazione della radice con 1 o 2 cifre decimali corrette, e poi applicare il Metodo di Newton per ottenere con pochissime iterazioni la precisione desiderata.

\vspace{20pt}

\subsection{Risoluzione di Equazioni Algebriche}

\vspace{10pt}

\paragraph{Valutazione di un polinomio in un punto}
Le equazioni algebriche sono equazioni del tipo
$$ P(x)=0 $$
dove $P$ è un polinomio di grado $n$, cioè
$$ P(x)=a_{1}x^{n}+a_{2}x^{n-1}+\dots+a_{n}x+a_{n+1}, \quad a_{1} \ne 0, \quad a_{k} \in \mathbb{R} $$

Le seguenti proprietà sono ben note:
\begin{itemize}
    \item Un polinomio di grado $n$ ha esattamente $n$ radici, che possono essere reali o complesse, ciascuna contata con la sua moltiplicità (Teorema Fondamentale dell'Algebra);
    \item Se un polinomio $P$ ha una radice complessa $\alpha=a+ib$ allora la sua coniugata $\overline{\alpha}=a-ib$ è anche radice di $P$;
    \item Un polinomio di grado dispari ha almeno una radice reale;
    \item Per $n \ge 5$ non esiste alcuna forma esplicita per le radici di $P$.
\end{itemize}

Per determinare un'approssimazione delle radici reali di un'equazione algebrica è possibile utilizzare i metodi visti per le equazioni non lineari, dopo aver individuato per ciascuna radice reale un intervallo che la contenga.
Quando $n$ è molto grande il problema di individuare tali intervalli può risultare non immediato.
In generale occorre effettuare delle operazioni preliminari:
\begin{itemize}
    \item \textit{localizzazione delle radici}: serve a determinare un cerchio del piano complesso che contenga tutte le radici (sia reali che complesse);
    \item \textit{numerazione delle radici}: consiste nel determinare il numero delle radici reali.
\end{itemize}
A tale scopo sono utili i seguenti risultati.

\paragraph{Teorema di Cauchy}
\begin{bxthm}
\begin{thm}
Tutti gli zeri di
$$ P(x)=a_{1}x^{n}+a_{2}x^{n-1}+\dots+a_{n}x+a_{n+1} $$
sono inclusi nel cerchio del piano di centro l'origine e raggio
$$ r=1+\max_{2 \le k \le n+1} \left| \frac{a_{k}}{a_{1}} \right| $$    
\end{thm}
\end{bxthm}

Dunque, in particolare le radici reali si trovano nell'intervallo $[-r, r]$.

\paragraph{Regola dei segni di Cartesio}
Sia
$$ P(x)=a_{1}x^{n}+a_{2}x^{n-1}+\dots+a_{n}x+a_{n+1} $$
Consideriamo l'insieme ordinato dei coefficienti
$$ A=\{a_{1}, a_{2}, \dots, a_{n}, a_{n+1}\} $$
Se denotiamo con $\nu$ il numero di variazioni di segno nell'insieme $A$ (gli eventuali zeri non si contano) si ha che il numero $k$ delle radici reali positive di $P$ soddisfa le seguenti proprietà:
$$ k \le \nu \quad \text{e} \quad \nu-k \text{ è un numero pari.} $$
Se applichiamo la regola di Cartesio al polinomio
$$ Q(x)=P(-x) $$
otteniamo il numero delle radici reali positive di $Q$ e quindi il numero delle radici reali negative di $P$.

Applicando i precedenti risultati e disegnando il grafico di $P$ sull'intervallo $[-r, r]$ si stabilisce con certezza il numero delle radici reali di $P$ e si determinano tanti intervallini quante sono le radici di $P$. Si può così procedere al calcolo approssimato delle radici reali di $P$ utilizzando i metodi visti per le equazioni non lineari.

\paragraph{Osservazioni}
È possibile utilizzare il metodo di Newton anche per approssimare le radici complesse di un polinomio scegliendo opportunamente il punto $x_{0}$.
Esistono delle varianti del Metodo di Newton che permettono di approssimare contemporaneamente tutte le radici del polinomio, sia quelle reali che quelle complesse.

\paragraph{Esempio 1}
Sia
$$ P(x)=x^{6}-x-1 $$
Poiché
$$ \max_{2 \le k \le n+1} \left| \frac{a_{k}}{a_{1}} \right| = 1 $$
applicando il Teorema di Cauchy, tutte le radici reali di $P$ appartengono all'intervallo $[-2, 2]$.
Poiché
$$ A=\{1, 0, 0, 0, 0, -1, -1\} $$
applicando la regola di Cartesio si ha $\nu=1$.
Pertanto $k$ deve essere necessariamente 1, cioè $P$ ha una sola radice positiva.
Infine, posto
$$ Q(x):=P(-x)=x^{6}+x-1 $$
si ha
$$ A=\{1, 0, 0, 0, 0, 1, -1\} $$
e, applicando ancora la regola di Cartesio, otteniamo anche in questo caso $\nu=1$, cioè il numero delle radici negative di $P$ è $k=1$.

Possiamo concludere che $P(x)=x^{6}-x-1$ ha due radici reali e due coppie di radici complesse coniugate appartenenti alla circonferenza del piano complesso di centro 0 e raggio 2.
In particolare:
\begin{itemize}
    \item la radice positiva appartiene all'intervallo $[0, 2]$;
    \item la radice negativa appartiene all'intervallo $[-2, 0]$.
\end{itemize}

\paragraph{Esempio 2}
Sia
$$ P(x) = x^{9} + 2x^{8} - 3x^{7} + x^{6} + x^{4} - 2x^{2} + x - 1 $$
Poiché
$$ \max_{2 \le k \le n+1} \left| \frac{a_{k}}{a_{1}} \right| = 3 $$
applicando il Teorema di Cauchy, tutte le radici reali di $P$ appartengono all'intervallo $[-4, 4]$.

Poiché l'insieme dei coefficienti è
$$ A = \{1, 2, -3, 1, 0, 1, 0, -2, 1, -1\} $$
applicando la regola di Cartesio si ha $\nu = 5$.
Pertanto $k$, il numero di radici positive, può essere 1, 3 o 5, cioè $P$ ha 1, 3 o 5 radici positive.

Infine, posto
$$ Q(x) := P(-x) = -x^{9} + 2x^{8} + 3x^{7} + x^{6} + x^{4} - 2x^{2} - x - 1 $$
si ha l'insieme dei coefficienti di $Q$:
$$ A = \{-1, 2, 3, 1, 0, 1, 0, -2, -1, -1\} $$
e, applicando ancora la regola di Cartesio, otteniamo $\nu = 2$.
Cioè il numero delle radici negative di $P$ è $k=0$ o $k=2$.

Possiamo concludere che tutte le radici di $P(x)$ appartengono alla circonferenza del piano complesso di centro 0 e raggio 4. In particolare, i seguenti casi sono possibili:
\begin{itemize}
    \item $P$ ha 5 radici positive appartenenti all'intervallo $[0, 4]$ e due coppie di radici complesse coniugate;
    \item $P$ ha 3 radici positive appartenenti all'intervallo $[0, 4]$ e tre coppie di radici complesse coniugate;
    \item $P$ ha 1 radice positiva appartenenti all'intervallo $[0, 4]$ e quattro coppie di radici complesse coniugate;
    \item $P$ ha 2 radici negative appartenenti all'intervallo $[-4, 0]$, 5 radici positive appartenenti all'intervallo $[0, 4]$ e una coppia di radici complesse coniugate;
    \item $P$ ha 2 radici negative appartenenti all'intervallo $[-4, 0]$, 3 radici positive appartenenti all'intervallo $[0, 4]$ e due coppie di radici complesse coniugate;
    \item $P$ ha 2 radici negative appartenenti all'intervallo $[-4, 0]$, 1 radice positiva appartenenti all'intervallo $[0, 4]$ e tre coppie di radici complesse coniugate.
\end{itemize}

Zoommando si può vedere che $P$ ha 3 radici reali di cui:
\begin{itemize}
    \item 1 radice negativa appartenente all'intervallo $[-\frac{7}{2}, -3]$;
    \item 1 radice negativa appartenente all'intervallo $[-1, -\frac{1}{2}]$;
    \item 1 radice positiva appartenente all'intervallo $[\frac{1}{2}, \frac{3}{2}]$.
\end{itemize}

\paragraph{Valutazione di un polinomio in un punto}
Consideriamo un polinomio di grado $n$ a coefficienti reali:
$$ P(x) = a_{1}x^{n} + a_{2}x^{n-1} + \dots + a_{n}x + a_{n+1} $$
Il nostro obiettivo è costruire l'algoritmo più efficiente per valutare $P$ in un punto fissato $\overline{x}$.

\textbf{Algoritmo 1: Metodo Immediato} \\
L'approccio più diretto consiste nel calcolare ogni termine della sommatoria singolarmente:
$$ P(x) = \sum_{i=0}^{n} a_{i+1}x^{n-i} $$
In questo metodo, ogni potenza $x^{k}$ viene calcolata da zero moltiplicando $x$ per se stesso $k-1$ volte. Questo comporta un costo computazionale elevato per gradi $n$ grandi.
\begin{itemize}
    \item \textit{Operazioni moltiplicative:} $\sum_{i=1}^{n} i = \frac{n(n+1)}{2}$
    \item \textit{Operazioni additive:} $n$
\end{itemize}

\textbf{Algoritmo 2: Metodo con Accumulazione delle Potenze} \\
È possibile migliorare l'efficienza evitando di ricalcolare le potenze ogni volta. Riscriviamo il polinomio come:
$$ P(x) = a_{n+1} + \sum_{i=0}^{n-1} a_{n-i}x^{i+1} $$
L'idea chiave è quella di accumulare la potenza di $x$: invece di calcolare $x^{i+1}$ da capo, si utilizza il valore della potenza precedente $x^{i}$ e lo si moltiplica per $x$ (cioè $x^{i+1} = x \cdot x^{i}$).
In questo modo, il numero di moltiplicazioni necessarie diminuisce drasticamente, passando da una crescita quadratica a una lineare.
\begin{itemize}
    \item \textit{Operazioni moltiplicative:} $2n$ (una per aggiornare la potenza e una per il coefficiente ad ogni passo)
    \item \textit{Operazioni additive:} $n$
\end{itemize}
Esiste un algoritmo, detto \textbf{Algoritmo di Horner}, che permette di valutare $P$ in un punto con un costo computazionale pari a $n$ operazioni moltiplicative e $n$ operazioni additive.

\paragraph{Algoritmo di Horner}
Per semplificare, supponiamo che $P$ sia di grado 3:
$$ P(x)=a_{4}+a_{3}x+a_{2}x^{2}+a_{1}x^{3} $$
Mettendo in evidenza la $x$ otteniamo:
$$ P(x)=a_{4}+x(a_{3}+x(a_{2}+xa_{1})) $$
Riepilogando, ponendo
$$ b_{1}:=a_{1} $$
$$ b_{2}:=a_{2}+\overline{x}b_{1} $$
$$ b_{3}:=a_{3}+\overline{x}b_{2} $$
$$ b_{4}:=a_{4}+\overline{x}b_{3} $$
si ottiene $P(\overline{x})=b_{4}$.
In generale, per valutare il polinomio $P(x)$ in un punto $\overline{x}$, ponendo:
$$ b_{1}:=a_{1} $$
$$ b_{i}:=a_{i}+\overline{x}b_{i-1}, \quad i=2, 3, \dots, n+1 $$
si ha
$$ P(\overline{x})=b_{n+1} $$

\paragraph{Osservazione}
I numeri $b_{i}, i=1, \dots, n$ non sono altro che i coefficienti del polinomio $Q(x)$ ottenuto come quoziente della divisione del polinomio $P(x)$ per il binomio $(x-\overline{x})$ e $P(\overline{x})$ non è altro che il resto, cioè
$$ P(x)=Q(x)(x-\overline{x})+P(\overline{x}) $$
con
$$ Q(x)=b_{1}x^{n-1}+b_{2}x^{n-2}+\dots+b_{n-1}x+b_{n} $$
Dunque le formule dell'algoritmo di Horner permettono di calcolare, senza eseguire la divisione, i coefficienti di $Q(x)$ nonché il resto $P(\overline{x})$ secondo lo schema di Ruffini:
$$
\begin{array}{c|cccccc|c}
 & a_{1} & a_{2} & a_{3} & \dots & a_{n-1} & a_{n} & a_{n+1} \\
\overline{x} & & \overline{x}b_{1} & \overline{x}b_{2} & \dots & \overline{x}b_{n-2} & \overline{x}b_{n-1} & \overline{x}b_{n} \\
\hline
 & b_{1} & b_{2} & b_{3} & \dots & b_{n-1} & b_{n} & b_{n+1}
\end{array}
$$

\paragraph{Schema Algoritmo di Horner}
\begin{verbatim}
START main
in n, a, x
b(1) = a(1)
for i = 2; i <= n; i++
    b(i) = a(i) + x * b(i-1)
px = a(n+1) + x * b(n)
out px, b
END
\end{verbatim}
Costo: $n$ operazioni moltiplicative e $n$ operazioni additive.

\paragraph{Esempio}
Valutare il polinomio
$$ P(x)=2x^{9}+8x^{4}-x^{3}-1 $$
nel punto $\overline{x}=1$.
Usando lo schema di Ruffini:
$$
\begin{array}{c|ccccccccc|cc}
 & 2 & 0 & 0 & 0 & 0 & 8 & -1 & 0 & 0 & -1 \\
1 & & 2 & 2 & 2 & 2 & 2 & 10 & 9 & 9 & 9 \\
\hline
 & 2 & 2 & 2 & 2 & 2 & 10 & 9 & 9 & 9 & 8
\end{array}
$$
Dunque $P(1)=8$ e il quoziente è:
$$ Q(x)=2x^{8}+2x^{7}+2x^{6}+2x^{5}+2x^{4}+10x^{3}+9x^{2}+9x+9 $$

\paragraph{Esercizio in Octave}
\begin{verbatim}
>> a = [2 0 0 0 0 8 -1 0 0 -1];
>> [px, b] = horner(a, 1)
px = 8
b = 2 2 2 2 2 10 9 9 9
\end{verbatim}

\paragraph{Calcolo della derivata}
Vediamo ora che, contestualmente al calcolo di $P(\xi)$ è possibile calcolare anche $P'(\xi)$.
Si può dimostrare facilmente che
$$ P(x)=Q(x)(x-\xi)+P(\xi) $$
dove
$$ Q(x)=b_{1}x^{n-1}+b_{2}x^{n-2}+\dots+b_{n-1}x+b_{n} $$
Ricordando la formula di Taylor:
$$ P(x)=P(\xi)+P'(\xi)(x-\xi)+\frac{(x-\xi)^{2}}{2!}P(\xi)+\dots+\frac{(x-\xi)^{n}}{n!}P^{(n)}(\xi) $$
possiamo scrivere
$$ Q(x)=\frac{P(x)-P(\xi)}{x-\xi}=P'(\xi)+\frac{(x-\xi)}{2!}P(\xi)+\dots $$
Facendo il limite per $x \to \xi$ otteniamo
$$ Q(\xi)=\lim_{x \to \xi}\frac{P(x)-P(\xi)}{x-\xi}=P'(\xi) $$
Dunque per calcolare $P'(\xi)$ basta calcolare $Q(\xi)$.
Lo possiamo fare utilizzando ancora l'algoritmo di Horner sui coefficienti $b_{i}$.
Infatti, ricordando la definizione di $Q(x)$ e ponendo:
$$
\begin{cases}
c_{1}:=b_{1} \\
c_{i}=c_{i-1}\xi+b_{i}, \quad i=2,\dots,n
\end{cases}
$$
si ha
$$ P'(\xi)=Q(\xi)=c_{n} $$

L'algoritmo per calcolare $P(\xi)$ e $P'(\xi)$ è dunque:
\begin{verbatim}
p = a(1);
dp = p;
for i = 2:n
    p = p * x + a(i);
    dp = dp * x + p;
end
p = p * x + a(n+1);
\end{verbatim}
Il suo costo computazionale complessivo è $2n$.

\paragraph{Condizionamento delle radici di un polinomio}
Sia
$$ P(x) = a_{1}x^{n} + a_{2}x^{n-1} + \dots + a_{n}x + a_{n+1} $$
un polinomio di grado $n$ e siano $\alpha_{1}, \dots, \alpha_{n}$ le sue radici.
Per effetto della rappresentazione finita, i dati in ingresso del problema (ovvero i coefficienti) sono affetti da un errore che al massimo è l'epsilon macchina e di conseguenza le radici effettivamente calcolate saranno altre.

Obiettivo dello studio del condizionamento è misurare, a posteriori, la sensibilità del problema posto.
Nello stesso polinomio ci possono essere radici ben condizionate ed altre mal condizionate, cioè lo studio non è globale (tutte le radici) ma puntuale (una sola radice alla volta).

\paragraph{Condizionamento di una radice semplice}
Sia $\alpha$ una generica radice semplice del polinomio $P(x)$ e sia $\beta$ la corrispondente radice del polinomio che si ottiene perturbando il $k$-esimo coefficiente. Si dimostra che:
$$ \frac{|\alpha-\beta|}{|\alpha|} \le \frac{\varepsilon}{|\alpha|} \max_{k=1,\dots,n+1} \left| \frac{a_{k}\alpha^{n-k+1}}{P'(\alpha)} \right| $$
La quantità
$$ \frac{1}{|\alpha|} \max_{k=1,\dots,n+1} \left| \frac{a_{k}\alpha^{n-k+1}}{P'(\alpha)} \right| $$
rappresenta l'indice di condizionamento della radice.
Si evince che l'errore si amplifica o perché la derivata prima del polinomio computato nella radice è molto piccola (radici quasi doppie) o perché i coefficienti del polinomio sono molto grandi.

\paragraph{Condizionamento di una radice multipla}
Sia $\alpha$ una generica radice di molteplicità $r>1$. Si dimostra che:
$$ \frac{|\alpha-\beta|}{|\alpha|} \le \frac{\varepsilon^{1/r}}{|\alpha|} \max_{k=1,\dots,n+1} \left| \frac{r! a_{k}\alpha^{n-k+1}}{P^{(r)}(\alpha)} \right|^{1/r} $$
Quindi il problema è in generale mal condizionato.

\paragraph{Problema del calcolo simultaneo di tutti gli zeri}
Esistono diversi approcci:
\begin{enumerate}
    \item \textbf{Deflazione:} Si calcola una radice $x_{1}$ e si costruisce il quoziente $P_{1}(x)=P(x)/(x-x_{1})$. Si itera il procedimento.
    \textit{Difetti:} L'errore di arrotondamento si accumula in modo inaccettabile fornendo risultati fasulli dopo le prime radici.

    \item \textbf{Matrice Companion:} Si riduce il calcolo degli zeri di $P$ al calcolo degli autovalori della matrice companion di $P$, che ha $P$ come polinomio caratteristico.
    $$ P_{n}(x)=a_{1}x^{n}+\dots+a_{n}x+a_{n+1} $$
    $$ A=\begin{pmatrix}
    -\frac{a_{2}}{a_{1}} & -\frac{a_{3}}{a_{1}} & \dots & -\frac{a_{n}}{a_{1}} & -\frac{a_{n+1}}{a_{1}} \\
    1 & 0 & \dots & 0 & 0 \\
    0 & 1 & \dots & 0 & 0 \\
    \vdots & & \ddots & & \vdots \\
    0 & 0 & \dots & 1 & 0
    \end{pmatrix} $$
    È quello che fa la function \texttt{roots} del Matlab.
    \textit{Difetti:} Il metodo calcola con precisione gli autovalori della matrice, ma ciò non implica una soddisfacente approssimazione degli zeri di $P$, poiché i due problemi hanno un differente condizionamento.
\end{enumerate}

\newpage

\section{Esercizi}

\vspace{20pt}

\subsection{Risoluzione di un Sistema Lineare Quadrato}

\vspace{10pt}

\begin{enumerate}
\item Scrivere una function Matlab che verifichi che una matrice è simmetrica.
\item Scrivere una function Matlab che verifichi che una matrice simmetrica è definita positiva usando il criterio di Sylvester.
\item Scrivere una function Matlab che verifichi che una matrice è a diagonale dominante per righe.
\item Si consideri la matrice
\[
A = rand(10), \quad A = A * {}^tA.
\]
\begin{itemize}
\item Verificare se è simmetrica;
\item Verificare se è simmetrica e definita positiva;
\end{itemize}
\item Si consideri la matrice
\[
A = rand(10) + 100 * diag(ones(1, 10)).
\]
Verificare se è a diagonale dominante per righe.
\item Scrivere una function Matlab che verifichi che una matrice è a diagonale dominante per colonne.
\item Si consideri la matrice
\[
A(i, j) = \begin{cases}
-1 & i > j \\
0 & i < j, \qquad i, j \in\{1, \dots, n\} \\
100 & i = j
\end{cases}
\]
con $n = 15$.
\begin{itemize}
\item Verificare se è a diagonale dominante per colonne;
\item Verificare se è a diagonale dominante per righe.
\end{itemize}
\item Scrivere una function Matlab che implementi il metodo di sostituzione in avanti.
\item Scrivere una function Matlab che implementi il metodo di sostituzione all'indietro.
\item Scrivere una function Matlab che implementi opportunamente il metodo di sostituzione all'indietro per calcolare l'inversa di una matrice triangolare superiore
\item Si consideri il sistema lineare $A\mathbf{x} = \mathbf{b}$ con
\[
A = tril(rand(10)), \quad \mathbf{b} = sum(A, 2).
\]
Risolvere il sistema con il metodo di sostituzione in avanti.
\item Si consideri il sistema lineare $A\mathbf{x} = \mathbf{b}$ con
\[
A = triu(rand(10)), \quad \mathbf{b} = sum(A, 2).
\]
\begin{itemize}
\item Risolvere il sistema con il metodo di sostituzione all'indietro;
\item calcolare l'inversa di $A$.
\end{itemize}
\item Scrivere una function Matlab che risolvi un sistema diagonale.
\item Risolvere il sistema lineare $A\mathbf{x} = \mathbf{b}$ con
\[
A = diag(diag(rand(10))), \quad \mathbf{b} = sum(A, 2).
\]
\item Scrivere una function Matlab che implementi opportunamente il metodo di sostituzione in avanti per calcolare l'inversa di una matrice triangolare inferiore
\item Calcolare l'inversa della matrice
\[
A = tril(rand(20)).
\]
\item Scrivere una function Matlab che implementi il metodo di eliminazione di Gauss.
\item Si consideri il sistema lineare $A\mathbf{x} = \mathbf{b}$ con
\[
A = rand(100), \quad \mathbf{b} = sum(A, 2).
\]
Risolvere il sistema con il metodo di Gauss.
\item Si consideri il sistema di equazioni lineari $Ax=b$ di ordine $n=15$, con
\[
A(i, j) = \begin{cases}
-1 & i > j \\
0 & i < j, \qquad i, j \in \{1, \dots, n\} \\
100 & i = j
\end{cases}
\]
e
\[
\mathbf{b} = {}^t(1, 1, \dots, 1).
\]
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero massimo di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare il vettore soluzione con il metodo di sostituzione in avanti e riportarne le prime due componenti con le cifre significative che si possono certamente ritenere corrette.
\item Confrontare la soluzione ottenuta (vettore x) con la soluzione che si ottiene usando la function predefinita del Matlab (vettore y). Di quanto differiscono al massimo?
\end{itemize}
\item Si consideri il sistema di equazioni lineari $Ax=b$ di ordine $n=10$, con
\[
A = \begin{pmatrix}
\frac{1}{2} & 2 & 4 & 4 & \dots & 4 & 4 \\
1 & \frac{1}{3} & 2 & 4 & \dots & & 4 \\
2 & 1 & \frac{1}{4} & \ddots & \ddots & & \vdots \\
0 & 2 & \ddots & \ddots & \ddots & 4 & \vdots \\
\vdots & \ddots & \ddots & \ddots & \ddots & 2 & 4 \\
0 & \dots & 0 & 2 & 1 & \frac{1}{n} & 2 \\
0 & \dots & \dots & 0 & 2 & 1 & \frac{1}{n+1}
\end{pmatrix}
\]
e
\[
\mathbf{b} = (b_i)_{i\in\{1,\dots,n\}}, \quad b(i) = \sum_{j=1}^n A_{i,j}.
\]
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero massimo di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare il vettore soluzione con il metodo di eliminazione di Gauss e riportarne le prime due componenti con le cifre significative che si possono certamente ritenere corrette.
\item Confrontare la soluzione ottenuta (vettore x) con la soluzione che si ottiene usando la function predefinita del Matlab (vettore y). Di quanto differiscono al massimo?
\item Confrontare la soluzione ottenuta (vettore x) con il vettore $t = {}^t(1,\ldots,1)$ che rappresenta la soluzione esatta del sistema. Di quanto differiscono al massimo?
\end{itemize}
\item Si consideri il sistema di equazioni lineari $A\mathbf{x}=\mathbf{b}$ di ordine $n=100$, con
\[
A = \begin{pmatrix}
1 & 1 & 4 & 0 & \dots & 0 & 0 \\
6 & 3 & 1 & 4 & \ddots & & 0 \\
0 & 6 & 5 & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & \ddots & \ddots & \ddots & 4 & 0 \\
\vdots & \ddots & \ddots & \ddots & \ddots & 1 & 4 \\
0 & \dots & 0 & 0 & 6 & \ddots & 1 \\
0 & \dots & \dots & \dots & 0 & 6 & 2n-1
\end{pmatrix}
\]
e
\[
\mathbf{b} = (b_i)_{i\in\{1,\dots,n\}}, \quad b(i) = \sum_{j=1}^n A_{i,j}.
\]
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero massimo di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare il vettore soluzione con il metodo di eliminazione di Gauss e riportare le prime due componenti del vettore soluzione con le cifre significative che si possono certamente ritenere corrette.
\item Confrontare la soluzione ottenuta (vettore x) con il vettore $t = {}^t(1,\ldots,1)$ che rappresenta la soluzione esatta del sistema. Di quanto differiscono al massimo?
\end{itemize}
\item Scrivere una function Matlab che implementi il metodo di eliminazione di Gauss con la strategia del pivoting parziale.
\item Scrivere una function Matlab che prende in input una matrice $A$ e restituisce in output le matrici $L$ e $U$ tali che $A = LU$.
\item Consideriamo il sistema lineare $A\mathbf{x} = \mathbf{b}$ di ordine $n = 18$, dove
\[
A_{i,j} = \cos \left( (j-1)\frac{2i-1}{2n}\pi \right), \quad i,j \in \{1, \dots, n\},
\]
e
\[
b_i = \sum_{j=1}^n A_{i,j}, \quad i \in \{1, \dots, n\},
\]
la cui soluzione esatta è $\mathbf{x} = {}^t(1,\ldots,1)$.
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare la soluzione approssimata del sistema utilizzando il metodo di Gauss e calcolare l'errore relativo. Quante sono le cifre significative corrette?
\item Calcolare la soluzione approssimata del sistema utilizzando il metodo di Gauss con pivoting parziale e calcolare l'errore relativo. Quante sono le cifre significative corrette?
\item Qual è il metodo più stabile?
\end{itemize}
\item Consideriamo il sistema lineare $A\mathbf{x} = \mathbf{b}$ di ordine $n = 50$, dove
\[
A = \begin{pmatrix}
3 & 2 & 2 & 2 & \dots & 2 & 6 \\
2 & \frac{5}{2} & 2 & 2 & \ddots & & 2 \\
2 & 2 & \frac{7}{3} & 2 & \ddots & \ddots & \vdots \\
2 & 2 & 2 & \ddots & \ddots & 2 & 2 \\
\vdots & \vdots & \ddots & \ddots & \ddots & 2 & 2 \\
2 & 2 & \dots & 2 & 2 & 2+\frac{1}{n-1} & 2 \\
6 & 2 & 2 & \dots & 2 & 2 & 2+\frac{1}{n}
\end{pmatrix}
\]
e
\[
b_i = \sum_{j=1}^n A_{i,j}, \quad i \in \{1, \dots, n\},
\]
la cui soluzione esatta è $\mathbf{x} = {}^t(1,\ldots,1)$.
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare la soluzione approssimata del sistema utilizzando il metodo di Gauss e calcolare l'errore relativo. Quante sono le cifre significative corrette?
\item Calcolare la soluzione approssimata del sistema utilizzando il metodo di Gauss con pivoting parziale e calcolare l'errore relativo. Quante sono le cifre significative corrette?
\item Qual è il metodo più stabile?
\end{itemize}

\item Consideriamo il problema di $n = 80$
\[
AX = B, \quad A \in \mathbb{R}^{n \times n}, X \in \mathbb{R}^{n \times 3}, B \in \mathbb{R}^{n \times 3},
\]
dove
\[
A = \begin{pmatrix}
5 & 0 & \frac{1}{2} & 0 & \dots & 0 \\
0 & 9 & 0 & \frac{1}{3} & \ddots & \vdots \\
\vdots & 0 & 13 & \ddots & \ddots & 0 \\
0 & \ddots & \ddots & \ddots & \ddots & \frac{1}{n-1} \\
\frac{1}{3} & \ddots & 0 & \dots & 4(n-1)+1 & 0 \\
0 & \frac{1}{3} & 0 & \dots & 0 & 4n+1
\end{pmatrix}, \quad
B = \begin{pmatrix}
1 & 2 & 1 \\
1 & 2 & 2 \\
\vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots \\
1 & 2 & n-1 \\
1 & 2 & n
\end{pmatrix}.
\]
(In questa la trascrizione è corretta ma confrontare meglio con pdf esercitazioni per conferma sulla struttura.)
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della matrice soluzione $X$.
\item Calcolare la soluzione approssimata $Y$ utilizzando opportunamente la fattorizzazione $LU$ della matrice $A$. Riportare le componenti della prima riga della matrice $Y$ con le cifre che si possono ritenere corrette.
\item Calcolare il residuo relativo in norma infinito.
\end{itemize}

\item Scrivere una function Matlab che implementi opportunamente il metodo di eliminazione di Gauss per risolvere un sistema a matrice tridiagonale.
\item Scrivere una function Matlab che implementi opportunamente il metodo di eliminazione di Gauss per risolvere un sistema a matrice di Hessemberg superiore.

\item Scrivere una function Matlab che prende in input una matrice $A$ e restituisce in output le matrici $L, U$ e $P$ tali che $PA = LU$ e il numero $s$ degli scambi effettuati.
\item Consideriamo il sistema lineare $A\mathbf{x} = \mathbf{b}$ di ordine $n = 80$, dove
\[
A = \begin{pmatrix}
4 & 2 & 2 & 2 & \dots & 2 & 8 \\
4 & \frac{7}{2} & 2 & 2 & \ddots & & 2 \\
4 & 4 & \frac{10}{3} & 2 & \ddots & \ddots & \vdots \\
4 & 4 & 4 & \ddots & \ddots & 2 & 2 \\
\vdots & \vdots & \ddots & \ddots & \ddots & 2 & 2 \\
4 & 4 & \dots & 4 & 4 & 3+\frac{1}{n-1} & 2 \\
10 & 4 & 4 & \dots & 4 & 4 & 3+\frac{1}{n}
\end{pmatrix}
\]
e
\[
b_i = \sum_{j=1}^n A_{i,j}, \quad i \in \{1, \dots, n\},
\]
la cui soluzione esatta è $\mathbf{x} = {}^t(1,\ldots,1)$.
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare la soluzione approssimata del sistema utilizzando opportunamente il metodo di Gauss e calcolare l'errore relativo. Quante sono le cifre significative corrette?
\item Calcolare il Determinante della matrice $A$.
\item Calcolare l'inversa della matrice $A$.
\end{itemize}
\item Data la matrice di ordine $n = 40$
\[
A = \begin{pmatrix}
\frac{1}{10} & 0 & \frac{1}{2} & 0 & \dots & 0 \\
0 & \frac{1}{10} & 0 & \frac{1}{2} & \ddots & \vdots \\
\vdots & 0 & \frac{1}{10} & \ddots & \ddots & 0 \\
0 & \ddots & \ddots & \ddots & \ddots & \frac{1}{2} \\
\frac{1}{2} & \ddots & 0 & \dots & \frac{1}{10} & 0 \\
0 & \frac{1}{3} & 0 & \dots & 0 & \frac{1}{10}
\end{pmatrix},
\]
Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolarne:
\begin{itemize}
\item il determinante;
\item l'inversa.
\end{itemize}
\item Data la matrice di ordine $n = 40$(?)
\[
A = \begin{pmatrix}
10 & 1 & \frac{1}{2} & 3 & \dots & 3 & 3 \\
0 & 10 & 1 & \frac{1}{3} & \ddots & & 3 \\
2 & 0 & 10 & 1 & \ddots & \ddots & \vdots \\
\vdots & 2 & 0 & \ddots & \ddots & \frac{1}{n-2} & 3 \\
2 & & \ddots & \ddots & \ddots & 1 & \frac{1}{n-1} \\
\frac{1}{2} & \ddots & & 2 & 0 & 10 & 1 \\
1 & \frac{1}{2} & 2 & \dots & 2 & 0 & 10
\end{pmatrix}
\]
e
\[
b_i = \sum_{j=1}^n A_{i,j}, \quad i \in \{1, \dots, n\},
\]
la cui soluzione esatta è $\mathbf{x} = {}^t(1,\ldots,1)$.
\begin{itemize}
\item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
\item Calcolare la soluzione approssimata del sistema utilizzando opportunamente il metodo di Gauss e calcolare l'errore relativo. Quante sono le cifre significative corrette?
\item Calcolare il Determinante della matrice $A$.
\item Calcolare l'inversa della matrice $A$.
\end{itemize}
    \item Scrivere una function Matlab che implementi il metodo di Cholesky.
    
    \item Modificare le functions che implementano le fattorizzazioni $A = LU$ e $PA = LU$ in modo da fornire in output anche il corrispondente fattore di crescita $\rho$ per la valutazione della loro stabilità numerica.
    
    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 100$, dove
    $$
    A = \left(
    \begin{array}{ccccccc}
    9 & 0 & 2 & 0 & \dots & 0 & 1 \\
    4 & 5 & 0 & 2 & 0 & \dots & 0 \\
    4 & 0 & 5 & \ddots & \ddots & \ddots & \vdots \\
    \vdots & 0 & \ddots & \ddots & 0 & 2 & 0 \\
    \vdots & \vdots & \ddots & \ddots & 5 & 0 & 2 \\
    4 & 0 & \dots & 0 & 0 & 5 & 0 \\
    4 & 0 & \dots & 0 & 0 & 0 & 5
    \end{array}
    \right), \quad A = A * A^T
    $$
    e
    $$
    b_i = \sum_{j=1}^{n} A_{i,j}, \quad i=1, \dots, n,
    $$
    la cui soluzione esatta è $x = [1, \dots, 1]^T$.
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Proporre uno o più metodi numerici per calcolare la soluzione del sistema $Ax = b$ con la massima precisione possibile. Calcolare l'errore relativo in norma infinito. Quante sono le cifre significative corrette della soluzione calcolata?
        \item Motivare la scelta del metodo effettuata e commentare i risultati ottenuti.
        \item Calcolare il determinante di $A$.
        \item Calcolare l'inversa di $A$.
        \item Verificare che il fattore di crescita $\rho$ relativo alla fattorizzazione $LU$ è uguale a 1.
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 100$, dove
    $$
    A(i,j) = \begin{cases}
    -7, & i = j \\
    -2, & |i - j| = 1 \\
    1, & |i - j| = 4 \\
    0, & \text{altrimenti}
    \end{cases}
    $$
    e
    $$
    b_i = \sum_{j=1}^{n} A_{i,j}, \quad i=1, \dots, n,
    $$
    la cui soluzione esatta è $x = [1, \dots, 1]^T$.
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Proporre uno o più metodi numerici per calcolare la soluzione del sistema $Ax = b$ con la massima precisione possibile. Calcolare l'errore relativo in norma infinito. Quante sono le cifre significative corrette della soluzione calcolata?
        \item Valutare la stabilità dei metodi proposti mediante il calcolo del fattore di crescita $\rho$.
        \item Motivare la scelta del metodo effettuata e commentare i risultati ottenuti.
        \item Calcolare il determinante di $A$.
        \item Calcolare l'inversa di $A$.
        \item Verificare che il fattore di crescita $\rho$ relativo alla fattorizzazione $LU$ è minore di 2.
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 80$, dove
    $$
    A = \left(
    \begin{array}{cccccc}
    15 & -3 & -3 & -3 & \dots & -3 \\
    2 & 15 & -3 & -3 & \ddots & \vdots \\
    0 & 2 & 15 & \ddots & \ddots & -3 \\
    \vdots & & \ddots & \ddots & \ddots & -3 \\
    \vdots & \ddots & & 2 & 15 & -3 \\
    0 & \dots & \dots & 0 & 2 & 4
    \end{array}
    \right)
    $$
    e
    $$
    b = [1, \dots, 1]^T.
    $$
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Calcolare la soluzione approssimata $y$ del sistema utilizzando opportunamente il metodo di Gauss. Riportare le prime 2 componenti del vettore $y$ con le cifre che si possono ritenere corrette.
        \item Verificare la stabilità del metodo di Gauss usato, valutando il suo fattore di crescita $\rho$.
        \item Calcolare la norma infinito del residuo $Ay - b$.
        \item Calcolare il determinante di $A$.
        \item Calcolare l'inversa di $A$.
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 60$, con $A$ matrice di Wilkinson
    $$
    A = \left(
    \begin{array}{ccccccc}
    1 & 0 & 0 & 0 & \dots & 0 & 1 \\
    -1 & 1 & 0 & 0 & & \vdots & 1 \\
    -1 & -1 & 1 & \ddots & \ddots & \vdots & \vdots \\
    \vdots & -1 & \ddots & \ddots & 0 & 0 & \vdots \\
    \vdots & \vdots & \ddots & \ddots & 1 & 0 & 1 \\
    -1 & -1 & \dots & -1 & -1 & 1 & 1 \\
    -1 & -1 & \dots & -1 & -1 & -1 & 1
    \end{array}
    \right)
    $$
    e $b(i) = \sum_{j=1}^n a_{ij}, \quad i = 1:n$
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Stabilire in base a proprietà eventuali della matrice quale variante del metodo di Gauss o fattorizzazione è più conveniente usare per garantire la massima stabilità al minimo costo.
        \item Calcolare il fattore di crescita $\rho$ se si usa il GE.
        \item Calcolare la soluzione e l'errore relativo commesso.
        \item Calcolare il fattore di crescita $\rho$ se si usa il GEPP.
        \item Calcolare la soluzione e l'errore relativo commesso.
        \item Fare opportune considerazioni sui risultati ottenuti
        \item Calcolare la soluzione con il GETP e l'errore relativo commesso. Utilizzare opportunamente la function predefinita del Matlab lu.
        \item Fare opportune considerazioni sui risultati ottenuti
        \item Ripetere lo stesso esercizio con $n = 50$.
    \end{itemize}
    \item Scrivere una function Matlab che calcoli il fattore $\rho$ di crescita per la valutazione dell'errore algoritmico in GE e GEPP.

    \item Scrivere una function Matlab per la determinazione delle matrici $L, U, P, Q$ tali che $PAQ = LU$ con pivoting totale.

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 60$, con $A$ matrice di Wilkinson
    $$
    A = \left(
    \begin{array}{ccccccc}
    1 & 0 & 0 & 0 & \dots & 0 & 1 \\
    -1 & 1 & 0 & 0 & & \vdots & 1 \\
    -1 & -1 & 1 & \ddots & \ddots & \vdots & \vdots \\
    \vdots & -1 & \ddots & \ddots & 0 & 0 & \vdots \\
    \vdots & \vdots & \ddots & \ddots & 1 & 0 & 1 \\
    -1 & -1 & \dots & -1 & -1 & 1 & 1 \\
    -1 & -1 & \dots & -1 & -1 & -1 & 1
    \end{array}
    \right)
    $$
    e $b(i) = \sum_{j=1}^n a_{ij}, \quad i = 1:n$
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Stabilire in base a proprietà eventuali della matrice quale variante del metodo di Gauss o fattorizzazione è più conveniente usare per garantire la massima stabilità al minimo costo.
        \item Calcolare il fattore di crescita $\rho$ se si usa il GEPP.
        \item Calcolare la soluzione e l'errore relativo commesso.
        \item Fare opportune considerazioni sui risultati ottenuti
        \item Calcolare la soluzione con il GETP e l'errore relativo commesso.
        \item Fare opportune considerazioni sui risultati ottenuti
        \item Ripetere lo stesso esercizio con $n = 50$.
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 60$, con $b(i) = i \quad i=1,2\dots,n$ ed $A = G * G'$, essendo
    \begin{eqnarray*}
    G(i, i+1) &=& \cos(2i+1), \quad i=1,2,..,,n-1 \\
    G(i,i) &=& n+1, \quad i=1,2,\dots,n \\
    G(i,1) &=& 1, \quad i \ge 2 \\
    0 && \text{altrove}
    \end{eqnarray*}
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Stabilire in base a proprietà eventuali della matrice quale variante del metodo di Gauss o fattorizzazione è più conveniente usare per garantire la massima stabilità al minimo costo.
        \item Calcolare il fattore di crescita $\rho$ se si usa il GEPP.
        \item Calcolare la soluzione e l'errore relativo commesso.
        \item Fare opportune considerazioni sui risultati ottenuti
    \end{itemize}

    \item Consideriamo i sistemi lineari
    $$
    Gx = c
    $$
    $$
    Gy = d
    $$
    con $G = (g_{i,j})_{i,j=1,\dots,n}$ matrice dell'esercizio precedente e con
    $$
    c(i) = \sum_{j=1}^n g_{i,j}, \quad i=1,\dots,n,
    $$
    $$
    d(i) = 1, \quad i=1:n
    $$
    \begin{itemize}
        \item Calcolare l'indice di condizionamento e il numero di cifre significative corrette che ci si può aspettare nel calcolo della soluzione approssimata.
        \item Stabilire in base a proprietà eventuali della matrice quale variante del metodo di Gauss o fattorizzazione è più conveniente usare per garantire la massima stabilità al minimo costo.
        \item Calcolare il fattore di crescita $\rho$ se si usa il GEPP.
        \item Calcolare la soluzione e l'errore relativo commesso.
        \item Fare opportune considerazioni sui risultati ottenuti
    \end{itemize}
\item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 80$ con
    $$
    A = \left(
    \begin{array}{ccccccc}
    10 & 1 & 0 & \dots & 0 & 2 & 200 \\
    1 & 10 & 2 & \ddots & & \ddots & 2 \\
    0 & 1 & 10 & \ddots & \ddots & & 0 \\
    \vdots & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\
    0 & & \ddots & \ddots & \ddots & n-2 & 0 \\
    2 & \ddots & & \ddots & 1 & 10 & n-1 \\
    100 & 2 & 0 & \dots & 0 & 1 & 10
    \end{array}
    \right)
    $$
    e
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i=1,\dots,n.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Riportarne il valore e le istruzioni Matlab utilizzate. Quante sono le cifre significative corrette?
        \item Commentare i risultati ottenuti.
    \end{itemize}

    \item Determinare la matrice inversa della matrice data nell'esercizio precedente. Indicare:
    \begin{itemize}
        \item la procedura utilizzata
        \item la fattorizzazione più adeguata per garantire stabilità
        \item il costo computazionale.
    \end{itemize}
\item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 100$ con
    $$
    A = \left(
    \begin{array}{cccccc}
    2 & 0 & \dots & 0 & 1 & 3 \\
    0 & 4 & 0 & \dots & 0 & 1 \\
    2 & 0 & 6 & \ddots & \vdots & 0 \\
    \vdots & 2 & \ddots & \ddots & 0 & \vdots \\
    2 & \vdots & \ddots & \ddots & 2n-2 & 0 \\
    2 & 2 & \dots & 2 & 0 & 2n
    \end{array}
    \right)
    $$
    e
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i = 1,\dots,n.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Riportarne il valore e le istruzioni Matlab utilizzate. Quante sono le cifre significative corrette?
        \item Commentare i risultati ottenuti.
    \end{itemize}

    \item Consideriamo il sistema lineare $AX = B$ di ordine $n = 80$ con
    $$
    A = (a_{i,j})_{i,j=1,\dots,n} = \left(\sqrt{\frac{2}{n+1}}\sin\left(\frac{\pi i j}{n+1}\right)\right)_{i,j=1,\dots,n}
    $$
    e
    $$
    b = \left(
    \begin{array}{ccc}
    \sum_{j=1}^n A_{1,j} & 1 & 2 \\
    \sum_{j=1}^n A_{2,j} & 1 & 2 \\
    \vdots & \vdots & \vdots \\
    \sum_{j=1}^n A_{n,j} & 1 & 2
    \end{array}
    \right).
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e le prime 2 righe della matrice soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Calcolare il residuo relativo. Riportarne il valore e le istruzioni Matlab utilizzate. Quante sono le cifre significative corrette?
        \item Commentare i risultati ottenuti.
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 110$ con
    $$
    A = \left(
    \begin{array}{ccccccc}
    3 & 0 & 0 & 0 & \dots & 0 & 2 \\
    2 & 1 & 0 & 0 & \dots & 0 & 2 \\
    2 & 0 & 1 & \ddots & \ddots & \vdots & \vdots \\
    \vdots & 0 & \ddots & \ddots & 0 & 0 & 2 \\
    \vdots & \vdots & \ddots & \ddots & 1 & 0 & 2 \\
    2 & 0 & \dots & 0 & 0 & 1 & 2 \\
    2 & 0 & \dots & 0 & 0 & 0 & 3
    \end{array}
    \right)
    $$
    e
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i = 1,\dots,n.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Riportarne il valore e le istruzioni Matlab utilizzate. Quante sono le cifre significative corrette?
        \item Commentare i risultati ottenuti.
    \end{itemize}
    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 120$ con
    $$
    A = \left(
    \begin{array}{cccccc}
    2(n+1) & 2 & 0 & 0 & \dots & 0 \\
    -1 & 2(n+1) & 4 & 0 & \dots & 0 \\
    0 & -1 & 2(n+1) & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \ddots & \ddots & 0 \\
    0 & & \ddots & -1 & 2(n+1) & 2(n-1) \\
    0 & 0 & \dots & 0 & -1 & 2(n+1)
    \end{array}
    \right).
    $$
    e
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i = 1,\dots,n.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Riportarne il valore e le istruzioni Matlab utilizzate. Quante sono le cifre significative corrette?
        \item Calcolare l'inversa $B$ della matrice $A$ con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e il valore delle componenti $B(3,4)$ e $B(4,4)$ con le cifre che si possono ritenere corrette.
        \item Commentare i risultati ottenuti.
    \end{itemize}

    \item Data la matrice di ordine $n = 80$
    $$
    A = \left(
    \begin{array}{cccccc}
    \frac{1}{4} & -2 & -2 & \dots & -2 & 5 \\
    2 & \frac{1}{8} & -2 & \ddots & & -2 \\
    2 & 2 & \frac{1}{12} & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \ddots & -2 & -2 \\
    2 & & \ddots & \ddots & \frac{1}{4n-4} & -2 \\
    9 & 2 & \dots & 2 & 2 & \frac{1}{4n}
    \end{array}
    \right).
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la sua costruzione.
        \item Calcolare il determinante della matrice $A$ con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e il valore del determinante con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 80$ con
    $$
    A = (a_{i,j})_{i,j=1,\dots,n} \quad \text{con} \quad a_{i,j} = \min\{i,j\}
    $$
    e
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i = 1,\dots,n.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Riportarne il valore e le istruzioni Matlab utilizzate. Quante sono le cifre significative corrette?
        \item Calcolare il determinante della matrice $A$ con la massima precisione possibile usando una opportuna procedura numerica. Riportare le istruzioni Matlab utilizzate e il valore del determinante con le cifre che si possono ritenere corrette.
        \item Commentare i risultati ottenuti.
    \end{itemize}
\end{enumerate}


\vspace{20pt}

\subsection{Metodi Numerici per la Risoluzione di un Sistema Lineare Rettangolare nel Senso dei Minimi Quadrati}

\vspace{10pt}

\begin{enumerate}
    \item Considerare la matrice test $A$ definita mediante i comandi Matlab
    $$
    B = gallery('kahan', 80);
    $$
    $$
    A = B(:, 1:30).
    $$
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b(i) = \sum_{j=1}^n a_{ij}, \ i=1,\dots,m$ nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta.
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^n$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}

    \item Considerare la matrice test $A$ definita mediante il comando Matlab
    $$
    A = gallery('chebvand', 350, 60).
    $$
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b(i) = \sum_{j=1}^n a_{ij}, \ i=1,\dots,m$ nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta.
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^n$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}

    \item Considerare la matrice test $A$ definita mediante i comandi Matlab
    $$
    B = gallery('kms', 100, .1);
    $$
    $$
    A = B(:, 1:30).
    $$
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b(i) = \sum_{j=1}^n a_{ij}, \ i=1,\dots,m$ nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta.
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^n$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}

    \item Considerare la matrice test $A$ definita mediante i comandi Matlab
    $$
    A = gallery('lauchli', 100) * gallery('lehmer', 100);
    $$
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b(i) = \sum_{j=1}^n a_{ij}, \ i=1,\dots,m$ nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta.
        \item Calcolare l'errore commesso confrontando la soluzione ottenuta con quella che si ottiene utilizzando la function $\backslash$ del MatLab. Confrontare tale errore con la stima a priori.
    \end{itemize}

    \item Data la matrice di ordine $n = 100$
    $$
    B = \left(
    \begin{array}{cccccc}
    1 & -2 & -2 & \dots & \dots & -1 \\
    4 & 2 & -2 & \dots & \dots & -2 \\
    4 & 4 & \ddots & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \ddots & -2 & -2 \\
    \vdots & & \ddots & \ddots & n-1 & -2 \\
    5 & 4 & \dots & 4 & 4 & n
    \end{array}
    \right),
    $$
    considerare la matrice $A = B(:, 1:7)$.
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b = 2 * sum(A, 2)$, nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta.
        \item Commentare i risultati ottenuti.
    \end{itemize}

    \item Considerare la seguente matrice test
    $$
    A = gallery('lauchli', 10).
    $$
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b(i) = \sum_{j=1}^n a_{ij}, \ i=1,\dots,m$ nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta. Qual è il suo costo computazionale?
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^n$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}

    \item Sia $m = 90$. Consideriamo il sistema $Ax = b$ nel senso dei minimi quadrati, dove
    $$
    A = (a_{i,j})_{i=1,\dots,m \atop j=1,\dots,\frac{m}{3}}, \quad a_{i,j} = \begin{cases} 
    \sin(i+2j)+1, & i = j \\
    \cos(2i+j-5), & i \neq j 
    \end{cases}
    $$
    $$
    b = \{b_i\}_{i=1,\dots,m}, \quad b_i = \sum_{j=1}^{\frac{m}{3}} a_{i,j}, \quad i=1,\dots,m.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta. Qual è il suo costo computazionale?
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^{\frac{m}{}}$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}
    \item Considerare la seguente matrice test
    $$
    A = gallery('lauchli', 10).
    $$
    \begin{itemize}
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Considerare il sistema $Ax = b$ con $b(i) = \sum_{j=1}^n a_{ij}, \ i=1,\dots,m$ nel senso dei minimi quadrati. Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta. Qual è il suo costo computazionale?
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^n$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}

    \item Sia $m = 90$. Consideriamo il sistema $Ax = b$ nel senso dei minimi quadrati, dove
    $$
    A = (a_{i,j})_{i=1,\dots,m \atop j=1,\dots,\frac{m}{3}}, \quad a_{i,j} = \begin{cases} 
    \sin(i+2j)+1, & i = j \\
    \cos(2i+j-5), & i \neq j 
    \end{cases}
    $$
    $$
    b = \{b_i\}_{i=1,\dots,m}^T, \quad b_i = \sum_{j=1}^{\frac{m}{3}} a_{i,j}, \quad i=1,\dots,m.
    $$
    \begin{itemize}
        \item Riportare le istruzioni Matlab utilizzate per la costruzione di $A$ e $b$.
        \item Determinare le dimensioni della matrice e stabilire se è o meno a rango massimo.
        \item Studiare l'esistenza e l'unicità della soluzione del sistema, il condizionamento del problema e determinare la stima dell'errore teorico a priori.
        \item Scegliere il metodo che conviene applicare e giustificare la scelta. Qual è il suo costo computazionale?
        \item È noto che in questo caso la soluzione è data da $\bar{x} = (1, 1, \dots, 1)^T \in \mathbb{R}^{\frac{m}{4}}$. Calcolare quindi l'errore commesso e confrontare il risultato ottenuto con la stima a priori.
    \end{itemize}
    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 90$ con
    $$
    A = \left(
    \begin{array}{ccccccc}
    4 & 0 & 0 & \dots & 0 & 3 & 2 \\
    0 & 5 & 0 & & & 3 & 3 \\
    0 & 0 & 6 & \ddots & \vdots & \vdots & \vdots \\
    \vdots & & \ddots & \ddots & 0 & \vdots & \vdots \\
    0 & \dots & \dots & 0 & \ddots & 3 & n-1 \\
    2 & 2 & \dots & 2 & 2 & (n-1)+3 & n \\
    2 & 3 & \dots & \dots & n-1 & n & n+3
    \end{array}
    \right)
    $$
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i = 1,\dots,n.
    $$
    \begin{itemize}
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Quante sono le cifre significative corrette?
        \item Commentare i risultati ottenuti.
    \end{itemize}

    \item Consideriamo il sistema lineare $Ax = b$ di ordine $n = 90$ con
    $$
    A = \left(
    \begin{array}{cccccc}
    1 & 2 & 2 & \dots & 2 & 2 \\
    2 & 3 & 2 & 2 & & 2 \\
    2 & 2 & 5 & \ddots & \ddots & \vdots \\
    \vdots & 2 & \ddots & \ddots & 2 & 2 \\
    2 & & \ddots & 2 & 2n-3 & 2 \\
    2 & 2 & \dots & 2 & 2 & 2n-1
    \end{array}
    \right)
    $$
    $$
    b = \{b_i\}_{i=1,\dots,n}^T, \quad b_i = \sum_{j=1}^{n} A_{i,j}, \quad i = 1,\dots,n.
    $$
    \begin{itemize}
        \item Calcolare la soluzione approssimata del sistema con la massima precisione possibile usando una opportuna procedura numerica. Riportare le prime 2 componenti del vettore soluzione con le cifre che si possono ritenere corrette.
        \item Motivare la scelta del metodo.
        \item Qual è il costo computazionale del metodo numerico utilizzato?
        \item Poichè è noto che la soluzione è $x = (1, 1, \dots, 1)^T$, calcolare l'errore relativo. Quante sono le cifre significative corrette?
        \item Commentare i risultati ottenuti.
    \end{itemize}
\end{enumerate}

\vspace{20pt}

\subsection{Metodi Numerici per la Risoluzione di una Equazione non Lineare}

\vspace{10pt}

\begin{enumerate}
\item Scrivere una function Matlab che implementi il metodo di bisezione.
\item Scrivere una function Matlab che implementi il metodo di Newton.
\item Scrivere una function Matlab che implementi il metodo combinato bisezione-Newton.
\item Data l'equazione
\[
F(x) = \cos(x) - 4x.
\]
\begin{itemize}
\item Individuare un intervallo che contenga lo zero della funzione.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo di Bisezione. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dal metodo.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo di Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dal metodo.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dai metodi.
\end{itemize}
\item Data l'equazione
\[
F(x) = e^x + \frac{x}{10}.
\]
\begin{itemize}
\item Individuare un intervallo che contenga lo zero della funzione.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo di Bisezione. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dal metodo.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo di Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dal metodo.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dai metodi.
\end{itemize}
\item Data l'equazione
\[
F(x) = x + \log(x^3).
\]
\begin{itemize}
\item Individuare un intervallo che contenga lo zero della funzione.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo di Bisezione. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dal metodo.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo di Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dal metodo.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dai metodi.
\end{itemize}
\item Supponiamo che una reazione chimica origini ad un certo istante $t$ una concentrazione di un particolare ione data dalla legge:
\[
c(t) = 7e^{-5t} + 3e^{-2t}.
\]
Se all'istante iniziale la concentrazione iniziale è $c(0)=10$, a quale istante $\bar{t}$ la concentrazione iniziale si sarà dimezzata, ossia
\[
c(\bar{t}) = 5?
\]
\begin{itemize}
\item Tenendo conto che il problema è equivalente a quello di determinare lo zero dell'equazione
\[
F(t) = 7e^{-5t} + 3e^{-2t} - 5 = 0,
\]
individuare un intervallo del semiasse positivo che contenga lo zero della funzione $F$.
\item Approssimare lo zero con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Riportare il valore approssimato dello zero e il numero di iterazioni effettuate dai metodi.
\end{itemize}
\item Calcolare $\sqrt{33}$ con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton.
\item Calcolare $1/43$ con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton.
\item Scrivere una function Matlab che implementi il metodo di bisezione per equazioni algebriche.
\item Scrivere una function Matlab che implementi il metodo di Newton per equazioni algebriche.
\item Scrivere una function Matlab che implementi il metodo combinato bisezione-Newton per equazioni algebriche.
\item Scrivere una function Matlab che implementi l'algoritmo di Horner per il calcolo del valore di un polinomio e della sua derivata in un punto.
\item Scrivere una function Matlab che calcoli gli indici di condizionamento delle radici semplici e multiple.
\item Sia
\[
P(x) = x^6 - x - 1.
\]
\begin{itemize}
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Qual è il numero di iterazioni del metodo di bisezione? Qual è il numero di iterazioni del metodo di Newton?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = x^9 + 2x^8 - 3x^7 + x^6 + x^4 - 2x^2 + x - 1
\]
\begin{itemize}
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Qual è il numero di iterazioni del metodo di bisezione? Qual è il numero di iterazioni del metodo di Newton?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = 2x^9 - 3x^8 + 4x^5 + \frac{1}{2}x^4 - x^3 + x - \frac{1}{2}
\]
\begin{itemize}
\item Individuare l'intervallo che contiene tutte le radici reali.
\item Quante sono le radici reali? Che molteplicità hanno? Trovare per ciasuna radice reale un intervallo che la contiene.
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando il metodo combinato di bisezione-Newton. Qual è il numero di iterazioni del metodo di bisezione? Qual è il numero di iterazioni del metodo di Newton?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = x^7 - 3x^6 + 2.25x^5 - x^3 + 3.5x^2 - 3.75x + 1.125.
\]
\begin{itemize}
\item Il polinomio $P$ ha $x = \frac{3}{2}$ come radice doppia. Calcolarne il condizionamento.
\item Approssimarla con la precisione di macchina utilizzando il metodo Newton. Qual è il numero di iterazioni?
\item Approssimarla con la precisione di macchina utilizzando il metodo Newton opportunamente modificato. Qual è il numero di iterazioni?
\item Approssimarla con la function roots del MatLab.
\end{itemize}
\item Sia
\[
P(x) = x^5 + 0.631x^4 + 0.676387x^3 - 0.325473867x^2 + 0.04352613299999995x - 0.001860867
\]
\begin{itemize}
\item Individuare l'intervallo che contiene tutte le radici reali di $P$.
\item Quante sono le radici reali? Che molteplicità hanno? Trovare per ciasuna radice reale un intervallo che la contiene.
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando opportunamente i metodi studiati. Qual è il numero di iterazioni del metodo utilizzato?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = x^8 - 4.01x^7 + 4.02x^6 + x^3 - 3.01x^2 + 0.01x + 4.02.
\]
\begin{itemize}
\item Individuare l'intervallo che contiene tutte le radici reali di $P$.
\item Quante sono le radici reali? Che molteplicità hanno? Trovare per ciasuna radice reale un intervallo che la contiene.
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando opportunamente i metodi studiati. Qual è il numero di iterazioni del metodo utilizzato?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = 2x^8 - 8.02x^7 + 8.04x^6 + x^3 - 3.01x^2 + 0.001x + 4.02
\]
\begin{itemize}
\item Individuare l'intervallo che contiene tutte le radici reali.
\item Quante sono le radici reali? Che molteplicità hanno? Trovare per ciasuna radice reale un intervallo che la contiene.
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando opportunamente i metodi studiati. Qual è il numero di iterazioni del metodo utilizzato?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = x^{10} - 55x^9 + 1320x^8 - 18150x^7 + 157773x^6 - 902055x^5 + 3416930x^4 - 8409500x^3 + 12753576x^2 - 10628640x + 3628800.
\]
\begin{itemize}
\item Individuare l'intervallo che contiene tutte le radici reali di $P$.
\item Quante sono le radici reali? Che molteplicità hanno? Trovare per ciasuna radice reale un intervallo che la contiene.
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando opportunamente i metodi studiati. Qual è il numero di iterazioni del metodo utilizzato?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\item Sia
\[
P(x) = x^6 - 2x^5 - 4x^4 + 6x^3 + 7x^2 - 4x - 4
\]
\begin{itemize}
\item Individuare l'intervallo che contiene tutte le radici reali di $P$.
\item Quante sono le radici reali? Che molteplicità hanno? Trovare per ciasuna radice reale un intervallo che la contiene.
\item Approssimare le radici reali di $P$ con la precisione di macchina utilizzando opportunamente i metodi studiati. Qual è il numero di iterazioni del metodo utilizzato?
\item Studiare il condizionamento delle radici reali di $P$. Riportare il valore delle radici con le cifre che si possono ritenere corrette.
\end{itemize}
\end{enumerate}
\end{document}
